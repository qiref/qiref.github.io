{
  "version": "hugo-0.127.0",
  "timestamp": "2025-06-12T21:44:01+08:00",
  "pages": [{
      "title": "20250612 English Learning Journal",
      "url": "https://qiref.github.io/note/2025/06/12/20250612-english-learning-journal/",
      "content": "Today, I begin to write my journal of English Learning. And this is the first one.\nI will write my thoughts or some new words, phrase about my learning English every day. In theses days, I have listened POC ENGLISH video in the youtube, that\u0026rsquo;s is high quality video about English learning. I will pay more attention to the video. By the way , the POC ENGLISH has their own website, there is a lot of blogs in that website, all of them are pretty good .\nAI fixed:\n\u0026ldquo;Today, I begin documenting my English learning journey. ​​This is my first journal entry.​​\n​​I\u0026rsquo;ll​​ record my thoughts, new vocabulary, and English phrases daily. Recently, I\u0026rsquo;ve ​​been watching​​ high-quality ​​videos​​ from ​​POC English​​ on YouTube and plan ​​to focus​​ more on their content.\n​​By the way, POC English has its own website featuring​​ numerous excellent ​​blog articles​​.\u0026rdquo;\n原句​​\t​​问题类型​​\t​​修正建议​​\nthe first one\t冠词不当\tmy first journal entry listened POC video\t动词误用 + 无冠词\twatched POC English videos have listened\t时态不自然\thave been watching pay attention\t略生硬\tfocus on / prioritize their own website\t代词不一致\tits own website a lot of blogs\t冗余表达\tmany blog articles ",
      "summary": "Today, I begin to write my journal of English Learning. And this is the first one.\nI will write my thoughts or some new words, phrase about my learning English every day. In theses days, I have listened POC ENGLISH video in the youtube, that\u0026rsquo;s is high quality video about English learning. I will pay more attention to the video. By the way , the POC ENGLISH has their own website, there is a lot of blogs in that website, all of them are pretty good ."
    },{
      "title": "Paxos推导过程",
      "url": "https://qiref.github.io/post/2025/01/01/paxos%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B/",
      "content": "paxos 是一个分布式共识算法, 就是用来解决分布式系统中, 多副本数据如何保证读写一致性的问题.\n不完美的副本数据同步机制 假设我们有个分布式存储系统, 数据在写入时, 需要把数据写入到其他节点: 副本数据从一个节点复制到其他节点, 有几种复制办法:\n同步复制 异步复制 半同步复制 同步复制 # node1, node2, node3 a=x --\u0026gt; node1 --\u0026gt; node2 ------\u0026gt; node3 --\u0026gt; done 数据在一次写入时, 需要同时写入 node1, node2, node3 三个节点, 写入完成才算是一次写入成功;\n同步复制有什么问题:\n性能低下; 写入性能会受制于节点数量; 没有容错, 任何一个节点写入失败, 则系统不可用; 异步复制 a=x --\u0026gt; node1 --\u0026gt; done async( node1 --\u0026gt; node2 ; node1 --\u0026gt; node3 ) 数据在一次写入时, 只要写入一个节点, 则认定写入成功, node1 写入 node2; node1 写入 node3 是异步复制, 不影响整体写入结果;\n异步复制有什么问题:\n数据可能存在不一致, 当 async( node1 \u0026ndash;\u0026gt; node2) 写入失败时, node2 和 node1 上的数据就不一致; 半同步复制 数据在一次写入时, 数据必须写入一定量的副本(不是全部), 这样多副本则提供了较高的可靠性;\na=x --\u0026gt; node1 --\u0026gt; node2 --\u0026gt; done async( node1 --\u0026gt; node3 ) 半同步复制存在什么问题:\n数据可能不一致, 但比纯异步复制要好, 保证尽可能多的节点能达到一致; 多副本同步如何保证读写一致性 基于以上三种复制的思路, 可以发现半同步复制是比纯异步和纯同步方式更优的解决方案, 可以基于这个思路继续推导;\n在半同步复制中, 虽然最终所有节点也可能出现不一致, 但是可以保证大多数的节点达到数据一致, 如果在读取数据时, 也能保证从大多数节点中读, 那基于写入的“大多数一致性” , 数据在读取时, 也能做到读的 “大多数一致性” .\nquorum 对于大多数节点的定义: 超过整个集合半数以上的节点, 可以被称为一个 多数派 Quorum , Quorumsize=(N/2)+1; （下文统称为 Quorum），例如，一个三个节点的系统，可能的 Quorum 有如下情况：\nnode1, node2, node3 1. node1, node2 2. node2, node3 3. node1, node3 在 Quorum 读取时, 可能出现以下情况:\nt1 t2 node1 a=x a=y node2 a=x a=y node3 a=x a=x t1 时刻, node1, node2, node3 数据一致, 此时读取Quorum: a=x t2 时刻, node3 数据有落后, 此时读取Quorum时, 仅 (node1, node2) 这个子集能读取到一致的结果! 增加版本 针对上述这种情况, 考虑 t1 时刻, t2 时刻 node 中的数据存在变化, 其实数据就自然而然带上了版本的概念, 在读取时, 需选取最高版本的数据;\n再来看一下 Quorum 的写入，假设有以下场景:\nt1 t2 t3 node1 a=x1 a=x2 a=x3 node2 a=x1 a=x2 a=x3 node3 a=x1 a=x1 [a=x3, a=x2] x1 表示第一个版本的x; x2 表示第二个版本的x, 以此类推\u0026hellip; 数据加入版本之后, 在 t3 时刻, node3 接收到 a=x3, a=x2 两个请求: 当写入请求 a=x2 后到, 此时写入会导致 a=x3 被覆盖; 由于给数据加上版本后, 还需要保证数据不被历史版本的数据覆盖, 因此 每个 node 在接收写入请求时, 需要拒绝掉比当前版本更小的数据写入. 因此, 每个node 需要记录自己上一次写入数据的版本.\n还有个关键问题是: 版本如何确定?\n由于 node 记录了上一次写入的数据版本, 那么在 client 写入之前, 需要先去查询一下当前最新的版本, 版本号也需要遵循多数派Quorum和最新的原则, 然后 client 生成新的版本之后执行写入操作;\n并发写入问题 假定此时有两个client p1, p2 并发向 node 写入:\nt1 时刻, p1, p2 读取最新的版本, v=2\np1 -----------\u0026gt; node1, node2, node3 \u0026lt;-----------p2 # t1 时刻 a=x1, a=x2, a=x2 t2 时刻, p1,p2 同时写入, 此时 p1 p2 的版本相同：v=3\np1 -----------\u0026gt; node1, node2, node3 \u0026lt;-----------p2 # t2 时刻 (v++;a=x3) a=x1, a=x2, a=x2 (v++;a=y3) t3 时刻, 如果此时 node1 接收了p1, node2 接收了p2, 此时节点的数据完全不一致!\np1 -----------\u0026gt; node1, node2, node3 \u0026lt;-----------p2 # t3 时刻 (v++;a=x3) a=x3, a=y3, a=x2 (v++;a=y3) 如何解决并发写入的问题?\n解决这个问题, node 需要拒绝掉同一个version下的其他 client 的写入, 也就是说 node 需要记录下来最新的verion中, 上一次它同意写入的 client, node 要记住的前提是client 需要提前告知node, 其实就是client 在 t2 时刻之前, 还需要进行一次 Quorum 读, 告诉node, I‘ll write !\n另外, 同一个 version 下, node 记录多个 client 的写前读时, 只能记录最后一次的写前读的 client ;\n改进后的流程如下:\nt1 时刻, p1, p2 读取最新的版本, v=2\np1 -----------\u0026gt; node1, node2, node3 \u0026lt;-----------p2 # t1 时刻 a=x1, a=x2, a=x2 t2 时刻, node 会记录下最后一次的写前读取的 client p2;\np1 -----------\u0026gt; node1, node2, node3 \u0026lt;-----------p2 # t2 时刻, 写前读取 (I'll write) a=x1, a=x2, a=x2 (I'll write) p1 \u0026lt;----------- node1, node2, node3 # t2.1 时刻, node record p1 (ok) a=x1, a=x2, a=x2 node1, node2, node3 -----------\u0026gt;p2 # t2.2 时刻, node record p2 a=x1, a=x2, a=x2 (ok) t3 时刻, node 接受了 p2 的写请求, 对于 p1 的写入请求会拒绝, 最终一个 quorum 完成写入;\np1 -----------\u0026gt; node1, node2, node3 # t3 时刻, p1 写入 (write,a=x3) a=x1, a=x2, a=x2 p1 \u0026lt;----------- node1, node2, node3 # t3.1 时刻, reject p1 (reject) a=x1, a=x2, a=x2 node1, node2, node3 \u0026lt;-----------p2 # t3.2 时刻, p2 写入 a=y3, a=y3, a=x2 (write,a=y3) 推导总结 基于上述推导, 如何保证副本同步的强一致性?\n首先, 基于半同步复制, 通过一个 quorum 的读写, 唯一确认一个值, 当两个客户端同时写入 quorum, 为了防止后到的写入影响quorum的读, 需要给数据加上版本, 一个 quorum 的读写中, 冲突时选择最高版本的值;\n由于数据加上版本, 所以在写入之前, client 需要通过一次 quorum 读确认最新的版本;\n当多个 client 发起 quorum 写时, 存在并发问题, 如果两个 client 写前度拿到的最新的数据版本相同时, 此时需要唯一确认一个下次要写入的值, client 进行 quorum 写之前, 需要先通知 quorum, node 记录下次要写入的 client, 并拒绝掉其他的 client;\npaxos 算法描述 角色划分 Proposer：提案发起者，负责提出提案（Proposal）。 Acceptor：提案接受者，负责对提案进行投票和存储。 Learner：学习最终被接受的提案（通常是被动接收结果的节点，不参与投票）。 注意这里的三个角色里没有客户端的角色，三种角色都是服务端，算法的流程也是侧重于服务端多个node如何交互；\nClient Proposer Acceptor Learner |----写请求---\u0026gt;| | | | |---Prepare(n)----\u0026gt;| | | |\u0026lt;--Promise(n)-----| | | |---Accept(n,v)---\u0026gt;| | | |\u0026lt;--Accepted(n)----| | | | |----Learn(v)---\u0026gt;| |\u0026lt;----OK------| | | 算法流程 Prepare 阶段\nProposer 生成 全局唯一且递增的提案号 n，向所有 Acceptor 发送 Prepare(n)。 Acceptor 收到 Prepare(n) 后： 若 n \u0026gt; max_received_n：\n✅ 承诺 不再接受 \u0026lt;n 的提案，并返回已接受的最高编号提案 (max_accepted_n, value)。\n✅ 更新 max_received_n = n。 若 n ≤ max_received_n：\n❌ 拒绝请求。 Accept 阶段\nProposer 若收到 多数派 Acceptor 的 Prepare 响应： 从所有响应中选择 最高 max_accepted_n 对应的 value 作为提案值。 若所有响应无历史值，可自由决定 value。 向 Acceptor 发送 Accept(n, value)。 Acceptor 收到 Accept(n, value) 后： 若 n ≥ max_received_n：\n✅ 接受 提案，持久化存储 (n, value)。 否则：\n❌ 拒绝请求。 Learner 传播\n当 多数派 Acceptor 接受 (n, value) 后，Learner 将 value 广播为最终值。 max_received_n 是算法的核心机制，首先，其必须为单调递增，保证在多个提案中，总有一个最大的，也就是 Acceptor 只保留最大的提案号，并拒绝比本地记录小的提案，当一个提案被拒绝时，说明此时的提案号已经不是最新的了，需要重新生成提案；\n在Accept阶段，读取提案的数据时，也需要读取 max_accepted_n 最大版本的数据 value ，保证数据是最新的；Acceptor 在此阶段接受到Accept请求时，也判断 n ≥ max_received_n，并且拒绝掉更小编号的提案，这就意味着在两个提案同时发生时，可能会有一个被拒绝：\nProposer A 发起提案n=5，进入Accept阶段。 Proposer B 发起提案n=6： 在Prepare阶段，Acceptor承诺n=6。 Proposer B 发现大多数Acceptor承诺了n=6，进入Accept阶段。 Proposer A 的Accept请求因n=5 \u0026lt; 6被拒绝，需重新发起更高编号的提案（如n=7）。 结果：提案n=6成功取代n=5。 ",
      "summary": "paxos 是一个分布式共识算法, 就是用来解决分布式系统中, 多副本数据如何保证读写一致性的问题.\n不完美的副本数据同步机制 假设我们有个分布式存储系统, 数据在写入时, 需要把数据写入到其他节点: 副本数据从一个节点复制到其他节点, 有几种复制办法:\n同步复制 异步复制 半同步复制 同步复制 # node1, node2, node3 a=x --\u0026gt; node1 --\u0026gt; node2 ------\u0026gt; node3 --\u0026gt; done 数据在一次写入时, 需要同时写入 node1, node2, node3 三个节点, 写入完成才算是一次写入成功;\n同步复制有什么问题:\n性能低下; 写入性能会受制于节点数量; 没有容错, 任何一个节点写入失败, 则系统不可用; 异步复制 a=x --\u0026gt; node1 --\u0026gt; done async( node1 --\u0026gt; node2 ; node1 --\u0026gt; node3 ) 数据在一次写入时, 只要写入一个节点, 则认定写入成功, node1 写入 node2; node1 写入 node3 是异步复制, 不影响整体写入结果;\n异步复制有什么问题:\n数据可能存在不一致, 当 async( node1 \u0026ndash;\u0026gt; node2) 写入失败时, node2 和 node1 上的数据就不一致; 半同步复制 数据在一次写入时, 数据必须写入一定量的副本(不是全部), 这样多副本则提供了较高的可靠性;"
    },{
      "title": "两阶段提交和三阶段提交",
      "url": "https://qiref.github.io/post/2024/07/25/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%92%8C%E4%B8%89%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/",
      "content": "分布式共识问题 分布式共识问题是指在分布式系统中，多个节点或参与者需要就某个共同的结果达成一致意见的问题。例如主从同步问题, 简单的主从同步可以借助两阶段提交和三阶段提交来解决,确保所有节点上的事务操作能够保持一致性，即要么全部提交，要么全部回滚。\n两阶段提交 在主从同步的场景中, backup 需要从 master 同步数据, 从数据写入的场景来说, 假定主从节点是同步复制, 实际上就是一次分布式事务, backup 和 master 的一次写入要么都成功, 要么都失败, 才能保证主从节点的数据一致性;\n借助两阶段提交, 需要引入一个协调者: Coordinator, 如下:\n将提交过程分为两个阶段: PreCommit、commit;\n在 PreCommit 阶段, 执行 WAL 流程, 把事务日志提前写入, 然后 Coordinator 在收到参与者的 PreCommit ack 之后, 开启第二阶段 commit, commit 成功之后, 数据才算是真正的写入. 当 PreCommit 失败时, Coordinator 会发起事务回滚, 所有参与者会基于 WAL 的事务日志, 回滚此次事务;\n当 Coordinator 和 Participant 网络异常时, 如果是在 PreCommit 阶段, 那事务都不会提交, 相当于一次写入就失败了; 如果在 commit 阶段网络异常, Coordinator 会重试执行, 直到 Participant 恢复, 事务重新提交或者回滚. 但在此期间, 客户端的写入都会被拒绝, 此时分布式系统处于不可用状态(相当于保证了数据一致性, 牺牲了可用性)\n三阶段提交 三阶段提交的工作流程： 准备阶段： 协调者向所有参与者发送CanCommit请求。 参与者在准备好后回复Yes，如果有一个参与者回复No或者超时，则协调者会向所有参与者发送Abort请求，中止事务。 预提交阶段： 如果所有参与者回复Yes，协调者向所有参与者发送PreCommit请求。 参与者在收到PreCommit请求后，将事务写入日志，但不提交。如果有一个参与者写入失败，则协调者会向所有参与者发送Abort请求。 提交阶段： 协调者向所有参与者发送DoCommit请求。 参与者在收到DoCommit请求后，提交事务并释放资源。如果有一个参与者提交失败，则协调者会向所有参与者发送Abort请求。 为了缓解两阶段提交过程中的可用性问题, 三阶段做了如下改造:\n在 WAL 之前, 增加一个阶段, CanCommit, 避免在 PreCommit 时, 有参与者失败, 导致后续还需要回滚事务, 让异常请求尽量在最开始就丢弃掉;\n在进入最后 DoCommit 执行阶段的时候，如果参与者等待协调者超时了，那么参与者不会一直在那里死等，而是会把已经答应的事务执行完成;\n由于参与者默认提交事务的机制, 虽然提升了系统的可用性, 不会阻塞客户端的写入, 但是也会引入一些新的问题:\n当参与者在 PreCommit 阶段异常, 协调者会回滚本次事务, 如果在回滚过程中, 有另外一个参与者与协调者网络中断, 导致参与者无法感知到事务回滚就默认提交事务了, 此时系统中的数据就不一致了! 实际上，三阶段提交，就是为了可用性，牺牲了一致性.\n总结 在正常场景下, 三阶段提交会比两阶段提交有更好的性能, 原因是在出现网络异常时, 把PreCommit请求阶段拆分成 CanCommit 和 PreCommit 两个动作，缩短了各个参与者发生同步阻塞的时间。同时, 在 DoCommit 阶段, 参与者默认提交也增强整个分布式系统的可用性;\n两种方案都是在可用性和一致性之间权衡, 在大数据量的场景下, 这种小概率的异常事件也会成为必然, 并且两种方案都存在单点问题: coordinator, 当协调者不可用时, 整个事务无法推进下去!\n",
      "summary": "分布式共识问题 分布式共识问题是指在分布式系统中，多个节点或参与者需要就某个共同的结果达成一致意见的问题。例如主从同步问题, 简单的主从同步可以借助两阶段提交和三阶段提交来解决,确保所有节点上的事务操作能够保持一致性，即要么全部提交，要么全部回滚。\n两阶段提交 在主从同步的场景中, backup 需要从 master 同步数据, 从数据写入的场景来说, 假定主从节点是同步复制, 实际上就是一次分布式事务, backup 和 master 的一次写入要么都成功, 要么都失败, 才能保证主从节点的数据一致性;\n借助两阶段提交, 需要引入一个协调者: Coordinator, 如下:\n将提交过程分为两个阶段: PreCommit、commit;\n在 PreCommit 阶段, 执行 WAL 流程, 把事务日志提前写入, 然后 Coordinator 在收到参与者的 PreCommit ack 之后, 开启第二阶段 commit, commit 成功之后, 数据才算是真正的写入. 当 PreCommit 失败时, Coordinator 会发起事务回滚, 所有参与者会基于 WAL 的事务日志, 回滚此次事务;\n当 Coordinator 和 Participant 网络异常时, 如果是在 PreCommit 阶段, 那事务都不会提交, 相当于一次写入就失败了; 如果在 commit 阶段网络异常, Coordinator 会重试执行, 直到 Participant 恢复, 事务重新提交或者回滚. 但在此期间, 客户端的写入都会被拒绝, 此时分布式系统处于不可用状态(相当于保证了数据一致性, 牺牲了可用性)"
    },{
      "title": "关于锁的思考和总结(二)",
      "url": "https://qiref.github.io/post/2024/05/21/%E5%85%B3%E4%BA%8E%E9%94%81%E7%9A%84%E6%80%9D%E8%80%83%E5%92%8C%E6%80%BB%E7%BB%93%E4%BA%8C/",
      "content": "书接上文, 在单机模式下, 可以借助操作系统能力, 使用原子指令去实现锁, 但是在分布式场景中, 这种方案就会无法实现, 因为要竞争锁的进程在不同的机器上, 分布式锁因此而诞生.\n分布式锁的常见问题 举一个很常见的案例, 如果某个服务为了实现高可用而采用了多副本模式, 当服务中存在定时任务, 如何保证同时只有一个定时任务在运行呢? 从这里, 问题就开始变得复杂.\n很常规的思路就是借助数据库, 操作系统提供了原子指令, 同样, 数据库也提供了事务来保证原子性, 那么案例中的问题可以这么解决:\n可以设计一张表 lock, id, key 两个字段, 把 key 设置为唯一索引; key 的业务意义是定时任务的唯一标识; 每个实例执行定时任务之前, 往表里写入一条数据: (1, tastA), 由于事务机制的存在, 如果此时有其他实例往这个表里写数据时就会失败, 此时跳过当前实例的定时任务; 执行完定时任务之后, 把 (1, taskA) 这条记录删除; 问题解决了吗? 考虑一下异常情况: 当实例A拿到锁之后挂了, 那其他实例永远也拿不到锁了;\n一个很直观的思路就是给锁设置超时时间, 但是设置超时时间就需要权衡了, 如果定时任务本身的耗时跟锁的超时时间还要长, 那就会出现锁超时而导致同时两个实例在执行定时任务, 因此, 这个方案是需要一定的前提的, 这取决于实际的业务场景;\n再更进一步思考, 如果真的定时任务比锁的超时时间还长, 怎么解决呢? 锁的超时时间如果能动态变化, 这个问题就引刃而解了, 这就是锁续期;\nlock 表结构改为: id, key, createTime, expiredTime ; 在执行定时任务时,往表里写一条数据 (1, tastA), 同步开一个线程去给锁续期, expiredTime 时间增加; 定时任务执行结束时, 续期线程退出, 删除记录 (1, tastA); 注意, 续期的前提是加了锁超时的机制, 如果使用数据库的话, 需要定期扫描, 发现已经达到 expiredTime 时, 就删除记录;\n问题真的解决了吗? 有一种场景, 有 A, B 两个实例, A 拿到锁了, 然后 A 开始执行定时任务, 然后 A 开始 full GC, GC 期间, 锁已经过期了, B 中检测锁过期时, 发现过期了, 然后就删除 lock 的记录, 此时 B 可以拿到锁, 如果 B 现在拿到锁了, 并开始执行定时任务, 如果 A 又恢复了, 对于 A 来说, 此时是拿到锁的状态, A 也会开始执行定时任务, 锁失效!\n这个问题可以先放一下, 看完文章可能会有自己的理解和思考.\n总结一下, 实现分布式锁会遇到哪些问题\n问题1: 如何保证获取锁, 释放锁的原子性? 问题2: client 拿到锁之后挂了, 锁如何释放? 问题3: 锁加了超时时间后, 如何续期? 问题4: 是否需要释放其他 client 的锁? 基于 redis 的分布式锁 使用 redis 实现天然就避免了问题2, 因为 redis 支持过期时间;\n在 redis 中, SETNX 命令，它用于设置键值对的值。具体来说，就是这个命令在执行时会判断键值对是否存在，如果不存在，就设置键值对的值，如果存在，就不做任何设置。\nSETNX key value // 加锁 SETNX lock_key 1 // 业务逻辑 DO THINGS // 释放锁 DEL lock_key 那在 redis 中, 如何保证原子性呢?\n# NX 不存在即设置,EX PX 设置过期时间 SET key value [EX seconds | PX milliseconds] [NX] 对于释放锁而言, DEL lock_key 本身不存在原子性的问题; 但是如果要解决问题4, 保证 client 只释放自己的锁, 此时加锁时就需要把 value 设置为 client 的一个标识, 与此同时, 释放锁时, 也需要先判断当前 client 是否能释放锁, 此时的命令为:\n// 加锁, unique_value作为客户端唯一性的标识 SET lock_key unique_value NX PX 10000 //释放锁 比较unique_value是否相等，避免误释放, 使用 lua 脚本保证原子性 if redis.call(\u0026quot;get\u0026quot;,KEYS[1]) == ARGV[1] then return redis.call(\u0026quot;del\u0026quot;,KEYS[1]) else return 0 end KEYS[1]表示 lock_key，ARGV[1]是当前客户端的唯一标识，这两个值都是我们在执行 Lua 脚本时作为参数传入.\nredis-cli --eval unlock.script lock_key , unique_value redlock 上述基于redis实现的分布式锁, redis 是单实例的, 如果要增强锁的可靠性, 可以基于多个redis节点去实现, 问题从这里开始变得复杂, 业界已经出现了基于多个redis实例实现分布式锁的算法, redlock.\nRedlock 算法的基本思路，是让客户端和多个独立的 Redis 实例依次请求加锁，如果客户端能够和半数以上的实例成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败。这样一来，即使有单个 Redis 实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失。\n前置: 选取 N 个 redis 节点, 无需组成哨兵或者 cluster 模式, 这里假设 N = 5 , 对于每个 redis 节点而言, 获取锁/释放锁的方式跟单机版本的方式一致;\n算法步骤:\n加锁\n客户端获取当前时间戳 T1; 客户端依次在每个 redis 节点去获取锁, 此时, 使用相同的 key 和 random value; 客户端获取锁的请求有超时时间, 这个时间小于锁的总的超时时间(示例值: lock 10s 超时, client 请求超时时间 5-50 ms), 这个机制是为了防止某个 redis 实例不可用导致阻塞影响整个流程; 客户端再次获取当前时间戳 T2, 如果此时获取到半数之上节点的锁(N=5 时, 需要获取 N/2+1=3 个节点的锁), 并且 T2-T1 \u0026lt; lock expired time, 则认为获取锁成功; 如果客户端获取到了锁, 锁的有效时间就是 T2 - T1; 如果客户端由于某种原因未能获得锁（要么无法锁定 N/2+1 个实例，要么有效时间为负），客户端将尝试解锁所有实例（甚至是客户端认为没有锁定的实例)。 释放锁\n释放锁很简单, 向所有节点发送释放锁的请求, 使用 lua 脚本保证原子性;\n算法中超过半数节点加锁成功很好理解, 类似于分布式系统中的选举问题, 步骤3 计算 T2-T1 \u0026lt; lock expired time 才算是加锁成功是为什么呢?\n由于要请求多个节点, 网络情况是不可预期的, 请求越多, 响应的延迟、丢包等问题出现的概率就越大, 如果获取锁的时间都已经超过了锁的超时时间, 那最开始加锁成功节点上的锁就会超时失效了, 那本次加锁就没有意义了.\n为什么释放锁要向所有节点发送请求?\n向所有节点获取锁的过程不一定都会成功, 有可能有的节点由于网络原因, 加锁时间相对长甚至是加锁成功, 但是响应客户端的请求失败了, 此时客户端已经拿到锁了, 客户端是无法感知哪些节点加锁成功, 也不需要感知哪些节点加锁成功, 直接向所有节点发送释放锁的请求, 这样处理更为简洁;\n分布式锁的一些业界争论 Martin 对 Redlock 的质疑 在 Redlock 方案提出之后, 分布式领域的另一位大佬《Designing Data-Intensive Applications》作者 Martin 基于 Redlock 提出了一些质疑:\nMartin 认为, 分布式锁的目地是效率和正确性;\n就效率而言, Redlock 如果保证加锁的高效, 例如多个定时任务同时执行一个, 就算是无法做到完全互斥也无伤大雅,而 Redlock 在效率层面表现不太好, 实现过于重了, 保持高效用单体的 redis 就能达到目的, 当然这样会损失一些正确性;\n就正确性而言, Redlock 无法保证正确性, 如下图:\n这里的 GC 只是一种举例, 在分布式系统中, NPC (N：Network Delay，网络延迟; P：Process Pause，进程GC/重启; C：Clock Drift，时钟漂移) 问题随处可见;\nfecing token 方案 Martin 不仅质疑了 Redlock, 还给出了解决方案, 思路是在资源层做隔离, 保证修改共享资源的正确性, 具体思路如下图:\n客户端获取到锁时, 由锁服务提供一个递增的 token; 操作共享资源时带上这个 token; 共享资源层拒绝掉后来者的操作请求, 避免各种 NPC 问题; Martin 的这个思路是端到端的解决问题, 而不仅仅是着眼于算法如何才算拿到安全的锁.\nRedlock 作者的反驳 Antirez 的反驳主要针对 NPC 的各种问题, 以及fecing token 方案;\nRedlock 应对 NPC 的问题 在此之前, 先来回顾一下 Redlock 加锁的流程:\n客户端获取当前时间戳 T1; 客户端依次在每个 redis 节点去获取锁, 此时, 使用相同的 key 和 random value; 客户端获取锁的请求有超时时间, 这个时间小于锁的总的超时时间(示例值: lock 10s 超时, client 请求超时时间 5-50 ms), 这个机制是为了防止某个 redis 实例不可用导致阻塞影响整个流程; 客户端再次获取当前时间戳 T2, 如果此时获取到半数之上节点的锁(N=5 时, 需要获取 N/2+1=3 个节点的锁), 并且 T2-T1 \u0026lt; lock expired time, 则认为获取锁成功; 如果客户端获取到了锁, 锁的有效时间就是 T2 - T1; 如果客户端由于某种原因未能获得锁（要么无法锁定 N/2+1 个实例，要么有效时间为负），客户端将尝试解锁所有实例（甚至是客户端认为没有锁定的实例)。 首先对于 NPC 的时钟问题, 这个确实会对锁产生影响, 但是时钟问题可以通过有效的运维手段解决, 比如: 1. 系统管理员修改了时钟; 2. 从时钟服务器收到一个更大的时钟; 对于这两种情况而言, 是可以通过运维手段保证的, 手动修改时钟这种情况, 类似于有人手动修改 Raft 的日志, 这种情况下 Raft 也无法正常工作; NTP server 是可以通过调整配置来保证时钟不会大幅度跳跃;\n而对于 NPC 的另外两种情况, 进程GC 、网络延迟, 这两种情况都可以在算法的 1-4步发现, 这也就是为什么需要 T2-T1, 当发现 T2-T1 \u0026lt; lock expired time 不成立时, 则获取锁失败了, 那就执行解锁流程就好了. 而在第4步之后, client 拿到锁遇到异常情况, 这并非是 RedLock 独有的问题, 任何一种分布式锁实现都会面临这个问题, 不在讨论范畴;\n值得一提的是, Antirez 这里为什么要单独解释时钟问题, 这其实是 Redlock 算法的前提, 如果计算 T2-T1 \u0026lt; lock expired time T1 到 T2 的时钟出现跳跃, 那其实锁的过期时间就会计算有误, 比如redis 实例中的 value 其实已经过期, 但是客户端认为还拿到锁了, 这个才是算法的关键所在!\n质疑 fencing token 机制 Antirez 针对 fencing token 机制提出两点质疑:\n局限性, 数据面的隔离需要依赖于数据存储的容器, 如果是类似于mysql的数据, 很容易实现带条件的更新, 如果是无状态的 http 服务, 无法隔离数据, 这种方案就不适用了; 数据层面已经做了隔离, 那分布式锁存在还有必要吗? 个人认为, 在分布式领域没有银弹, 随处可以见妥协和折中的设计, 结合业务端到端看问题才是正解, 这里 fencing token 的局限性客观存在, 但其背后的思路却是很值得借鉴: 如果业务对数据正确性要求非常高, 对上层的机制的可靠性需要提出质疑, 数据面的隔离是必要的.\n基于 zookeeper 的分布式锁 在ZooKeeper中，ephemeral节点是一种临时节点，它与创建它的客户端会话相关联。当创建这样的节点的客户端会话结束（例如客户端断开连接或会话超时），这些节点将被自动删除。 一些关于ZooKeeper ephemeral节点的重要特性包括：\n临时性：节点与客户端会话相关联，会话结束时节点自动删除。 顺序性：可以为ephemeral节点设置顺序标志，使节点按照创建顺序进行编号。 通知机制：ZooKeeper允许客户端注册对节点变化的监听器，当ephemeral节点创建或删除时，客户端可以收到通知。 基于 ZooKeeper 以上特性, 可以实现分布式锁:\n客户端尝试创建一个 znode 节点(ephemeral)，比如/lock。那么第一个客户端就创建成功了，相当于拿到了锁；而其它的客户端会创建失败（znode 已存在），获取锁失败。 持有锁的客户端访问共享资源完成后，将 znode 删掉，这样其它客户端接下来就能来获取锁了。 如果创建 znode 的那个客户端崩溃了，那么相应的 znode 会被自动删除。这保证了锁一定会被释放, 因此不需要考虑过期的问题, 也需要设计续期的机制.\n但是基于 ZooKeeper 实现的分布式锁, 依然会面临 NPC 问题, 当 client1 获取到锁之后, 进入长时间 GC, 此时 client1 与 ZooKeeper 的 seesion 超时, ephemeral节点会被删除, 此时其他客户端可能会拿到锁, 锁失效.\n总结 基于上文, 对于分布式锁的实现而言:\n分布式锁并不是 100% 安全, 无关于实现方式, redis、zk、数据库; 一个严谨的分布式锁模型应该考虑锁租期、锁归属、NPC 问题; 工程实践时, 需要根据业务进行取舍; 对于严格要求数据正确性的场景下, 需要端到端的考虑数据的正确性, 不应该强依赖于分布式锁机制, 分布式锁可以在上层拦截大批量请求, 底层数据面需要有相应的兜底策略; Martin: For me, this is the most important point: I don’t care who is right or wrong in this debate — I care about learning from others’ work, so that we can avoid repeating old mistakes, and make things better in future. So much great work has already been done for us: by standing on the shoulders of giants, we can build better software.\n参考 https://redis.io/docs/latest/develop/use/patterns/distributed-locks/ https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html\n",
      "summary": "书接上文, 在单机模式下, 可以借助操作系统能力, 使用原子指令去实现锁, 但是在分布式场景中, 这种方案就会无法实现, 因为要竞争锁的进程在不同的机器上, 分布式锁因此而诞生.\n分布式锁的常见问题 举一个很常见的案例, 如果某个服务为了实现高可用而采用了多副本模式, 当服务中存在定时任务, 如何保证同时只有一个定时任务在运行呢? 从这里, 问题就开始变得复杂.\n很常规的思路就是借助数据库, 操作系统提供了原子指令, 同样, 数据库也提供了事务来保证原子性, 那么案例中的问题可以这么解决:\n可以设计一张表 lock, id, key 两个字段, 把 key 设置为唯一索引; key 的业务意义是定时任务的唯一标识; 每个实例执行定时任务之前, 往表里写入一条数据: (1, tastA), 由于事务机制的存在, 如果此时有其他实例往这个表里写数据时就会失败, 此时跳过当前实例的定时任务; 执行完定时任务之后, 把 (1, taskA) 这条记录删除; 问题解决了吗? 考虑一下异常情况: 当实例A拿到锁之后挂了, 那其他实例永远也拿不到锁了;\n一个很直观的思路就是给锁设置超时时间, 但是设置超时时间就需要权衡了, 如果定时任务本身的耗时跟锁的超时时间还要长, 那就会出现锁超时而导致同时两个实例在执行定时任务, 因此, 这个方案是需要一定的前提的, 这取决于实际的业务场景;\n再更进一步思考, 如果真的定时任务比锁的超时时间还长, 怎么解决呢? 锁的超时时间如果能动态变化, 这个问题就引刃而解了, 这就是锁续期;\nlock 表结构改为: id, key, createTime, expiredTime ; 在执行定时任务时,往表里写一条数据 (1, tastA), 同步开一个线程去给锁续期, expiredTime 时间增加; 定时任务执行结束时, 续期线程退出, 删除记录 (1, tastA); 注意, 续期的前提是加了锁超时的机制, 如果使用数据库的话, 需要定期扫描, 发现已经达到 expiredTime 时, 就删除记录;"
    },{
      "title": "关于锁的思考和总结(一)",
      "url": "https://qiref.github.io/post/2024/05/15/%E5%85%B3%E4%BA%8E%E9%94%81%E7%9A%84%E6%80%9D%E8%80%83%E5%92%8C%E6%80%BB%E7%BB%93%E4%B8%80/",
      "content": "func add (a *int) *int { *a++ // 线程不安全 return a } 这是一段很典型的线程不安全的代码示例, 在并发场景下, a 的结果是不确定的, 大概率会小于 1000, 原因是 a++ 并非原子操作, 会存在同时有两个协程读取到 a 的值是相同的情况, 执行 a++之后再重新回写时, a的值也是相同的, 想要变为线程安全, 就需要在操作临界资源之前加锁;\nMutex 在操作共享资源之前加锁, 然后操作完临界资源之后释放锁, 保证同时只有一个协程操作临界资源;\nvar mu sync.Mutex func addSafe(a *int) *int { mu.Lock() // 加锁 defer mu.Unlock() // 释放锁 *a++ return a } 锁在多线程或多进程环境中实现资源的互斥访问。当一个线程或进程想要访问某个共享资源（如数据结构、文件等）时，它必须首先尝试获取该资源对应的锁。如果锁未被其他线程或进程占用，那么请求的线程或进程将获得锁并继续执行；否则，它将等待，直到锁被释放。\n// If the lock is already in use, the calling goroutine // blocks until the mutex is available. func (m *Mutex) Lock() { // Fast path: grab unlocked mutex. if atomic.CompareAndSwapInt32(\u0026amp;m.state, 0, mutexLocked) { if race.Enabled { race.Acquire(unsafe.Pointer(m)) } return } // Slow path (outlined so that the fast path can be inlined) m.lockSlow() } type Mutex struct { state int32 // state 表示当前互斥锁的状态 sema uint32 // sema 是用于控制锁状态的信号量 } 在 golang 的实现中, 如果通过 CompareAndSwapInt32 也就是 CAS 能获取到锁, 表明协程已经能拿到锁了, 此时直接返回;\n如果通过 CAS 无法拿到锁, 说明有其他协程拿到锁还未释放, 这时候的选择就会很多了, 可以原地重试, 也就是自旋的特性, 也可以进入队列等待, 等待被唤醒;\n一般来说, 编程语言的实现中, 会优先选择自旋, 因为此时还是在用户态, 但是并发量很大时, 自旋反而会降低性能.\n// lockSlow() 代码片段 ... if atomic.CompareAndSwapInt32(\u0026amp;m.state, old, new) { if old\u0026amp;(mutexLocked|mutexStarving) == 0 { break // locked the mutex with CAS } // If we were already waiting before, queue at the front of the queue. queueLifo := waitStartTime != 0 if waitStartTime == 0 { waitStartTime = runtime_nanotime() } runtime_SemacquireMutex(\u0026amp;m.sema, queueLifo, 1) ... } 当 CAS 和自旋之后依然没有获取到锁, 为了防止大批量协程自旋, lockSlow() 中, runtime_SemacquireMutex(\u0026amp;m.sema, queueLifo, 1) 给出了一种新的方式去获取锁, 从入参来看, 传入了一个信号量, 是否需要 LIFO 的队列, 和一个值, 实现思路大概是如果没获取到锁, 就进入等待队列, 等待被唤醒;\n// func runtime_SemacquireMutex(addr *uint32, lifo bool, skipframes int32) TEXT runtime·runtime_SemacquireMutex(SB), NOSPLIT, $0-12 MOVL addr+0(FP), BP // 将互斥锁的地址加载到 BP 寄存器 MOVL lifo+4(FP), AX // 将 lifo 参数加载到 AX 寄存器 MOVL skipframes+8(FP), CX // 将 skipframes 参数加载到 CX 寄存器 // 尝试原子地获取互斥锁 acquire: MOVL $1, SI // 将 SI 寄存器设置为 1 XCHGL SI, 0(BP) // 使用 XCHGL 指令原子地交换 SI 和互斥锁的值 TESTL SI, SI // 检查 SI 寄存器的值（即原互斥锁的值） JZ acquired // 如果 SI 寄存器的值为 0，表示互斥锁已被获取，跳转到 acquired 标签 // 互斥锁已被占用，阻塞当前协程（goroutine） MOVL BP, 0(SP) // 将互斥锁的地址存储到栈上 MOVL AX, 4(SP) // 将 lifo 参数存储到栈上 MOVL CX, 8(SP) // 将 skipframes 参数存储到栈上 CALL runtime·semacquire1(SB) // 调用 semacquire1 函数阻塞当前协程 JMP acquire // 当协程被唤醒时，跳回到 acquire 标签尝试再次获取互斥锁 acquired: // 互斥锁已被成功获取，执行后续代码 // ... RET CAS Mutex 加锁为什么会影响性能呢? 基于Mutex的原理: 当一个线程尝试获取已被其他线程持有的锁时，它将被阻塞。阻塞的线程会进入睡眠状态，等待锁被释放, 当锁被释放时，操作系统会唤醒等待锁的线程, 使其重新尝试获取锁. 一般来说, 等待的线程会放到队列中等待, 以保障后续唤醒的顺序。\n这里耗时的主要原因是线程被阻塞进入睡眠状态, 而后又被唤醒, 这些操作都需要切换到内核态 ,如果说线程在没获取到锁时不进入睡眠状态, 而是原地重试, 那是不是就能提高效率呢? 这就是 CAS 的思路, 先来看看 golang 中对 CAS 的相关实现:\n// CompareAndSwapInt32 executes the compare-and-swap operation for an int32 value. // Consider using the more ergonomic and less error-prone [Int32.CompareAndSwap] instead. func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool) 这种只有函数声明，没有函数实现的，通常意味着函数的实现在golang汇编中, 也就是还是需要借助系统的能力实现 CAS, 从汇编指令上看, 对于 addr 来说, 如果 addr == old , 就把 addr 设置为 new;\n在 X86 架构下:\n// func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool) TEXT runtime·CompareAndSwapInt32(SB), NOSPLIT, $0-13 MOVL addr+0(FP), BP // 将目标地址加载到 BP 寄存器 MOVL old+4(FP), AX // 将 old 参数加载到 AX 寄存器 MOVL new+8(FP), CX // 将 new 参数加载到 CX 寄存器 // 使用 CMPXCHGL 指令原子地比较和交换值 MOVL AX, 0(BP) // 将 AX 寄存器的值存储到目标地址 LOCK CMPXCHGL CX, 0(BP) // 原子地比较和交换目标地址和 CX 寄存器的值 // 设置返回值 SETE AL // 如果比较和交换成功，将 AL 寄存器设置为 1（true），否则设置为 0（false） MOVB AL, swapped+12(FP) // 将 AL 寄存器的值存储到返回值中 RET 这段汇编中, 最重要的一点就是 LOCK CMPXCHGL , LOCK 前缀用于确保指令在执行过程中不会被其他处理器或线程中断。当 LOCK 前缀与 CMPXCHGL 指令结合使用时，确保了在比较和交换操作数的过程中，内存位置的值不会被其他处理器或线程修改，从而实现了原子性。\n从 CompareAndSwapInt32 函数的返回结果来看, 执行函数可能会失败, 也就是没获取到锁, 往往这种情况会进行重试, 直到执行完成, 也就是自旋的特性; 当然这种自旋是在用户态, 无需切换到内核态, 因此性能上相对更好;\n比如在 java 语言中对于 CAS 的使用:\npublic final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5; } ABA 问题 func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool) 在 CAS 中, 对于 addr 来说, 如果 addr == old , 就把 addr 设置为 new, 在此过程中, 依然存在问题;\n当线程 T1 进入 CAS 之前, 读取了一个 old 值, 假设为 A; 线程 T1 时间片耗尽, 线程 T2 开始执行; T2 执行 CAS, 把 addr 设置为 B, 然后又执行 CAS , 把 addr 修改回 A; 当 T1 继续执行时, 发现 addr 依然为 old A, T1 会认为 addr 没有被修改过; 如果要解决 ABA 问题, 一个可行的思路就是给数据加上一个版本号, 不仅是对比数据的值, 还需要对比数据的版本号, 这个思路其实就是从最底层的资源做了隔离, 在并发的场景中, 如果对数据正确性要求很高, 临界资源兜底策略很重要;\n大批量线程自旋问题 基于 CAS 的特性, 没有修改成功可以进行重试, 也就是自旋, 如果在线程数量很多的场景下, 反而会影响性能, 因为会有大量的线程不断自旋, 而且随着线程数量, 自旋的影响会持续放大, 因此在 Java 中, synchronized 会随着并发量的增加, 会有锁升级的机制, 在golang 的 Mutex 实现中, 也是从 CAS 逐渐升级到最后阻塞线程.\n总结 从编程语言的锁的实现上来看, 从用户态很难实现锁的机制, 因为线程调度是由内核完成的, 不确定当前线程何时耗尽时间片, 如果读到临界资源还没来得及写入, 此时中断, 其他线程获取到 CPU 时间片后, 会读取到相同的临界资源, 当临界资源被修改后, 之前线程拿到的临界资源还是历史值, 此时再修改就是基于错误的值进行修改;\n当借助系统调用时, 会进入内核态, 无论是 XCHGL 还是 CMPXCHGL , 这些指令都是原子操作; 从操作系统的角度而言, 锁的本质就是一个内存中的标识, 锁的竞争就是通过原子指令去修改这个标识位, 修改成功则表现为获取到了锁, 修改失败则可以自行决定, 自旋或者阻塞, 表现为不同的锁的形式, 自旋锁, 排他锁;\n参考 https://zh.wikipedia.org/wiki/%E4%BA%92%E6%96%A5%E9%94%81\nhttps://www.infoq.com/presentations/go-locks/\nhttps://time.geekbang.org/column/article/377913\nhttps://draveness.me/golang/docs/part3-runtime/ch06-concurrency/golang-sync-primitives/#mutex\n",
      "summary": "func add (a *int) *int { *a++ // 线程不安全 return a } 这是一段很典型的线程不安全的代码示例, 在并发场景下, a 的结果是不确定的, 大概率会小于 1000, 原因是 a++ 并非原子操作, 会存在同时有两个协程读取到 a 的值是相同的情况, 执行 a++之后再重新回写时, a的值也是相同的, 想要变为线程安全, 就需要在操作临界资源之前加锁;\nMutex 在操作共享资源之前加锁, 然后操作完临界资源之后释放锁, 保证同时只有一个协程操作临界资源;\nvar mu sync.Mutex func addSafe(a *int) *int { mu.Lock() // 加锁 defer mu.Unlock() // 释放锁 *a++ return a } 锁在多线程或多进程环境中实现资源的互斥访问。当一个线程或进程想要访问某个共享资源（如数据结构、文件等）时，它必须首先尝试获取该资源对应的锁。如果锁未被其他线程或进程占用，那么请求的线程或进程将获得锁并继续执行；否则，它将等待，直到锁被释放。\n// If the lock is already in use, the calling goroutine // blocks until the mutex is available."
    },{
      "title": "Go ppfof工具使用",
      "url": "https://qiref.github.io/post/2024/02/23/go-ppfof%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/",
      "content": "pprof pprof 是用于可视化和分析性能分析数据的工具;\nruntime/pprof：采集程序（非 Server）的运行数据进行分析 net/http/pprof：采集 HTTP Server 的运行时数据进行分析 启用方式 在main函数之前使用启动, DoProfile(6060)\nimport ( \u0026quot;net/http\u0026quot; _ \u0026quot;net/http/pprof\u0026quot; \u0026quot;strconv\u0026quot; ) type ProfileServer struct { } func (this *ProfileServer) DoProfile(port int) { go func() { err := http.ListenAndServe(\u0026quot;:\u0026quot;+strconv.FormatInt(int64(port), 10), nil) if err != nil { log.Errorf(\u0026quot;Failed to do profile on port: %d\u0026quot;, port) } else { log.Infof(\u0026quot;pprof start successfully on port %d\u0026quot;, port) } }() } 分析 curl 'http://127.0.0.1:6060/debug/pprof/profile' -o profile.20240223 curl 'http://127.0.0.1:6060/debug/pprof/heap' -o heap.20240223 curl 'http://127.0.0.1:6060/debug/pprof/block' -o block.20240223 curl 'http://127.0.0.1:6060/debug/pprof/mutex' -o mutex.20240223 curl 'http://127.0.0.1:6060/debug/pprof/trace' -o trace.20240223 输出对应的pprof文件之后, 对文件进行分析;\nFlat：函数自身运行耗时 Flat%：函数自身耗时比例 Sum%：指的就是每一行的flat%与上面所有行的flat%总和 Cum：当前函数加上它所有调用栈的运行总耗时 Cum%：当前函数加上它所有调用栈的运行总耗时比例 使用命令行分析 输入 help 查看帮助命令, 常有 top 和 list\ntop 找到占用资源最高的函数, list methodName 找到源码所在;\ngo tool pprof profile.230904 File: xx_cc Type: cpu Time: Feb 23, 2024 at 10:53am (CST) Duration: 30s, Total samples = 230ms ( 0.77%) Entering interactive mode (type \u0026quot;help\u0026quot; for commands, \u0026quot;o\u0026quot; for options) (pprof) top 20 Showing nodes accounting for 300ms, 88.24% of 340ms total Showing top 20 nodes out of 212 flat flat% sum% cum cum% 70ms 20.59% 20.59% 70ms 20.59% runtime.step 40ms 11.76% 32.35% 40ms 11.76% runtime/internal/syscall.Syscall6 20ms 5.88% 38.24% 20ms 5.88% runtime.unlock2 10ms 2.94% 41.18% 10ms 2.94% encoding/json.simpleLetterEqualFold 10ms 2.94% 44.12% 10ms 2.94% fmt.(*pp).doPrintf 10ms 2.94% 47.06% 10ms 2.94% net/http.readRequest 10ms 2.94% 50.00% 10ms 2.94% runtime.(*moduledata).textAddr 10ms 2.94% 52.94% 10ms 2.94% runtime.casgstatus 10ms 2.94% 55.88% 10ms 2.94% runtime.elideWrapperCalling 10ms 2.94% 58.82% 10ms 2.94% runtime.epollwait 10ms 2.94% 61.76% 10ms 2.94% runtime.findfunc 10ms 2.94% 64.71% 10ms 2.94% runtime.getStackMap 10ms 2.94% 67.65% 30ms 8.82% runtime.gwrite 10ms 2.94% 70.59% 10ms 2.94% runtime.heapBitsSetType 10ms 2.94% 73.53% 10ms 2.94% runtime.lock2 10ms 2.94% 76.47% 20ms 5.88% runtime.mallocgc 10ms 2.94% 79.41% 10ms 2.94% runtime.memmove 10ms 2.94% 82.35% 90ms 26.47% runtime.pcvalue 10ms 2.94% 85.29% 20ms 5.88% runtime.printlock 10ms 2.94% 88.24% 10ms 2.94% runtime.scanblock (pprof) list doPrintf Total: 340ms ROUTINE ======================== fmt.(*pp).doPrintf in /usr/lib/golang/src/fmt/print.go 10ms 10ms (flat, cum) 2.94% of Total . . 1009: p.reordered = false . . 1010:formatLoop: . . 1011: for i := 0; i \u0026lt; end; { . . 1012: p.goodArgNum = true . . 1013: lasti := i 10ms 10ms 1014: for i \u0026lt; end \u0026amp;\u0026amp; format[i] != '%' { . . 1015: i++ . . 1016: } . . 1017: if i \u0026gt; lasti { . . 1018: p.buf.writeString(format[lasti:i]) . . 1019: } (pprof) web 界面查看分析结果 需要先安装 graphviz\nbrew install graphviz # for macos apt install graphviz # for ubuntu yum install graphviz # for centos 界面分析更为方便, 还能看 CPU 火焰图, 更容易找到问题所在; 启动web服务查看profile分析结果:\ngo tool pprof -http=127.0.0.1:8001 profile.230904 Serving web UI on http://127.0.0.1:8001 ",
      "summary": "pprof pprof 是用于可视化和分析性能分析数据的工具;\nruntime/pprof：采集程序（非 Server）的运行数据进行分析 net/http/pprof：采集 HTTP Server 的运行时数据进行分析 启用方式 在main函数之前使用启动, DoProfile(6060)\nimport ( \u0026quot;net/http\u0026quot; _ \u0026quot;net/http/pprof\u0026quot; \u0026quot;strconv\u0026quot; ) type ProfileServer struct { } func (this *ProfileServer) DoProfile(port int) { go func() { err := http.ListenAndServe(\u0026quot;:\u0026quot;+strconv.FormatInt(int64(port), 10), nil) if err != nil { log.Errorf(\u0026quot;Failed to do profile on port: %d\u0026quot;, port) } else { log.Infof(\u0026quot;pprof start successfully on port %d\u0026quot;, port) } }() } 分析 curl 'http://127.0.0.1:6060/debug/pprof/profile' -o profile.20240223 curl 'http://127."
    },{
      "title": "B+树",
      "url": "https://qiref.github.io/post/2023/12/05/b-%E6%A0%91/",
      "content": "B树引入 B树（英语：B-tree），是一种在计算机科学自平衡的树，能够保持数据有序。这种数据结构能够让查找数据、顺序访问、插入数据及删除的动作，都在对数时间内完成。B树，概括来说是一个一般化的二叉搜索树（binary search tree）一个节点可以拥有2个以上的子节点。与自平衡二叉查找树不同，B树适用于读写相对大的数据块的存储系统，例如磁盘。B树减少定位记录时所经历的中间过程，从而加快访问速度。B树这种数据结构可以用来描述外部存储。这种数据结构常被应用在数据库和文件系统的实现上。\nwiki 上是这么描述 B 树的, 重点在于 B 树被用作存储系统的实现上, 基于二叉搜索树天然的有序性, 实现 logn 级别的查询; 既然是用作存储系统的实现, 那么可以来推导一下, 为什么B 树会用作存储系统的实现?\n想要实现 logn 级别的查询, binary search tree skiip list 都可以实现, Why B 树?\n二叉搜索树 1\n二叉搜索树天然有序, 也能达到 logn 级别的查询性能, 但是二叉搜索树, 有个很严重的问题, 如果插入的数据本身是有序的, 那二叉搜索树就会退化为链表, 要解决这个问题, 可以用 AVL Tree (Balanced binary search tree) 和 Red-Black Tree.\nAVL 树 AVL Tree (Balanced binary search tree) 在二叉搜索树的基础上, 增加了自平衡的机制, 解决二叉搜索树退化为链表的问题, 但是自平衡也会带来新的问题(平衡条件必须满足所有节点的左右子树高度差不超过1), 插入时可能会触发多次的自平衡, 从而会影响数据插入的效率, 那有没有办法解决频繁的自平衡的问题呢?\n红黑树 Red-Black Tree 就能做到, 红黑树通过制定一系列的规则:\n节点要么是红色，要么是黑色。 根节点是黑色。 每个叶子节点（NIL节点，空节点）是黑色的。 每个红色节点的两个子节点都是黑色（即从每个叶子到根的所有路径上不能有两个连续的红色节点）。 从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。 这些约束强制了红黑树的关键性质: 从根到叶子的最长的可能路径不多于最短的可能路径的两倍长。结果是这个树大致上是平衡的, 平衡条件相对于 AVL 树宽松, 从而减少平衡次数, 获得更好的插入效率; 到这里, 看上去问题好像都解决了, 用红黑树就能满足存储系统的要求, 事实上确实也是这样, 很多语言的 hashmap 就是用红黑树作为底层存储实现的, 前提是这些索引结构都是在内存中, 如果这个存储系统是基于磁盘, 红黑树是否适用?\nB树 文件放到磁盘上, 那么怎么快速找到文件中的数据呢?\n首先需要减少跟磁盘的交互次数; 其次是存储的数据是有序的, 才能实现 logn 级别的查询; 树形结构的存储, 最直接的思路就是一个node存储为一个文件, 基于这个背景, 查询的性能就直接取决于树的高度, 每一次从 parent 到 child, 都是一次磁盘 IO, 查找的文件越多, 性能越差;\n如果能降低树的高度, 对应查询性能就会提高, 基于树的结构, 如何降低树的高度呢? 提高 child 的个数, 改为 m 叉树, 这样做有两个好处:\n可以把多个节点压缩到一起, 从而减少树的高度; 磁盘和内存都是基于 page 的存储, 默认为 4K, 多个节点数量可以凑够4K, 提高空间利用率; 这样, B 树就诞生了,B 树本质是多路查找树, 每个节点都保存多个 key , 同时每个节点可以包含 M 个子节点, 同时也需要满足查找树的顺序性;\n2\nB+ 树 通常来说, B 树的每个节点会包含 key data 以及子节点的指针, 就这种结构而言, 是否还有优化的空间呢?\n基于B 树, 想进一步优化查询效率, 还是一个重要的原则: 减少树的高度, 如果从存储上来说, 每一个节点存储为一个文件的话, 节点数量越少, 存储的文件就越少, 查询时, 跟磁盘的交互次数就越少, 如果只在叶子节点中存储数据, 其余节点只存储 key 信息, 那节点数量就能大幅减少, 从而能存储更多的数据, 树的高度也会降低, 这就是 B+ 树的思路;\n同时, B+ 树数据都在叶子节点, 叶子节点还用链表连接起来, 更方便范围查询; 这个思路跟跳表很相似, 非叶子节点可以认为是叶子节点的索引;\n/** * 这是B+树非叶子节点的定义。 * * 假设keywords=[3, 5, 8, 10] * 4个键值将数据分为5个区间：(-INF,3), [3,5), [5,8), [8,10), [10,INF) * 5个区间分别对应：children[0]...children[4] * * m值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小： * PAGE_SIZE = (m-1)*4[keywordss大小]+m*8[children大小] */ public class BPlusTreeNode { public static int m = 5; // 5叉树 public int[] keywords = new int[m-1]; // 键值，用来划分数据区间 public BPlusTreeNode[] children = new BPlusTreeNode[m];//保存子节点指针 } /** * 这是B+树中叶子节点的定义。 * * B+树中的叶子节点跟内部节点是不一样的, * 叶子节点存储的是值，而非区间。 * 这个定义里，每个叶子节点存储3个数据行的键值及地址信息。 * * k值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小： * PAGE_SIZE = k*4[keyw..大小]+k*8[dataAd..大小]+8[prev大小]+8[next大小] */ public class BPlusTreeLeafNode { public static int k = 3; public int[] keywords = new int[k]; // 数据的键值 public long[] dataAddress = new long[k]; // 数据地址 public BPlusTreeLeafNode prev; // 这个结点在链表中的前驱结点 public BPlusTreeLeafNode next; // 这个结点在链表中的后继结点 } 参考 https://oi-wiki.org/ds/bplus-tree/\nhttps://www.cs.usfca.edu/~galles/visualization/BPlusTree.html\nhttps://zh.m.wikipedia.org/wiki/B%E6%A0%91\n左子节点小于父节点,右子节点大于父节点,中序遍历能得到一个有序结构\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n这是一个三阶B树,每个node有三个子节点, 同时满足左子节点小于父节点, 右子节点大于父节点\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
      "summary": "B树引入 B树（英语：B-tree），是一种在计算机科学自平衡的树，能够保持数据有序。这种数据结构能够让查找数据、顺序访问、插入数据及删除的动作，都在对数时间内完成。B树，概括来说是一个一般化的二叉搜索树（binary search tree）一个节点可以拥有2个以上的子节点。与自平衡二叉查找树不同，B树适用于读写相对大的数据块的存储系统，例如磁盘。B树减少定位记录时所经历的中间过程，从而加快访问速度。B树这种数据结构可以用来描述外部存储。这种数据结构常被应用在数据库和文件系统的实现上。\nwiki 上是这么描述 B 树的, 重点在于 B 树被用作存储系统的实现上, 基于二叉搜索树天然的有序性, 实现 logn 级别的查询; 既然是用作存储系统的实现, 那么可以来推导一下, 为什么B 树会用作存储系统的实现?\n想要实现 logn 级别的查询, binary search tree skiip list 都可以实现, Why B 树?\n二叉搜索树 1\n二叉搜索树天然有序, 也能达到 logn 级别的查询性能, 但是二叉搜索树, 有个很严重的问题, 如果插入的数据本身是有序的, 那二叉搜索树就会退化为链表, 要解决这个问题, 可以用 AVL Tree (Balanced binary search tree) 和 Red-Black Tree.\nAVL 树 AVL Tree (Balanced binary search tree) 在二叉搜索树的基础上, 增加了自平衡的机制, 解决二叉搜索树退化为链表的问题, 但是自平衡也会带来新的问题(平衡条件必须满足所有节点的左右子树高度差不超过1), 插入时可能会触发多次的自平衡, 从而会影响数据插入的效率, 那有没有办法解决频繁的自平衡的问题呢?\n红黑树 Red-Black Tree 就能做到, 红黑树通过制定一系列的规则:"
    },{
      "title": "Log Structured Merge Tree",
      "url": "https://qiref.github.io/post/2023/10/13/log-structured-merge-tree/",
      "content": "基本概念 Log Structured Merge Tree, 其本质上是一种存储数据的方式,通常用于各种存储系统的底层数据结构,通过尽可能减少磁盘随机IO来提升写入性能, 适用于写多读少的场景.\n随机写和顺序写 对于一个存储系统而言, 不可避免地需要写入文件到磁盘, 对于常规的写来说, 每来一条数据写一次文件, 数据可能是 add update delete, 需要频繁操作文件, 每一次写都是一次随机 IO; 为了提高写入速度, LSM Tree 并不是每一次写操作都把文件写到磁盘, 而是将数据在内存中更新，当内存中的数据达到一定的阈值时，才将这部分数据真正刷新到磁盘文件中. 以这种方式尽可能让每次磁盘 IO 都是顺序写;\n思路 基于减少磁盘的随机 IO 来提升整体存储系统的写入性能这一背景, 很自然可以推导出用批量写入的方式, 要想批量写入, 就需要在内存维护最近写入的数据, 达到阈值之后生成一个文件写入到磁盘, 但是这样又会存在新的问题:\n如果某一条数据已经写入到磁盘文件, 后续又有更新, 怎么处理呢? 内存中维护的临时数据, 如果还未来得及写入磁盘, 服务挂了, 重新启动时, 历史写入的数据如何恢复? 每次内存中数据达到阈值,写一个整个文件到磁盘,那么最终会生成大量的文件, 如何解决? 解决问题1, 为了优化这种更新的写入, 可以采用数据版本的做法, 或者给数据增加标志, 然后定期合并, 当然, 这也是以空间换时间, 相同的数据存储了多次, 以提升写入性能; 与此同时, 在数据读取时,由于写入的逻辑改变, 一条数据可能会存在于多个文件中, 因此在读取时, 需要返回最新的数据, 在读取到多条数据时,需要对多条数据进行合取最新;\n解决问题2, 在业界比较标准的做法是 WAL, WAL 的基本原理是在执行数据修改操作之前，先将这些操作记录在日志（log）文件中, 以确保在发生故障或崩溃时，可以借助日志进行恢复并保持数据的一致性;\n解决问题3, 为了避免大量文件, 可以对文件进行定期合并, 当数据还在内存中时, 可以借助跳表或者 B+Tree 等数据结构保证内存中数据的顺序性, 在写文件时, 由于数据是有序的, 在文件合并时,很自然可以借助归并排序保证合并之后的数据的有序性, 而有序性又能天然提高查询效率.\n架构 lsm 基本上就是基于以上的推导思路实现的, 整体的架构如下:\nWAL WAL(Write Ahead Logging)即日志先写原理。 日志记录所有的数据修改操作, 把数据修改预先写入日志文件, 然后在写内存或者磁盘数据区, 当日志记录了写入操作, 后续的写动作过程中, 任意步骤出现问题导致进程崩溃, 都可以借助 log 进行数据恢复;\nWAL的通常的工作流程:\n客户端发起数据修改请求。 服务器将修改操作记录到WAL日志文件。 返回成功响应给客户端。 异步线程将WAL日志内容同步到磁盘数据文件。 当WAL日志同步完毕,数据修改才真正完成。 在 lsm tree 中, 整个 WAL 的流程可以完全融入到写入的过程中, 包括 log 文件的生命周期, 也可以随着 sstable 的落盘而结束.\nSStable SStable sorted string table, 这是一种存储数据的格式, 并且在文件中, 数据都是有序的, 并且在文件中, 保存了元数据信息, 提高数据查询的效率; 具体格式如下:\n从文件的结构基本能看出来数据是如何写入的, 整个文件分为元数据信息和数据本身, 加上元数据信息是为了提高数据查询的效率, 比如元数据信息包含了当前文件中 key 的范围, 并且还可以保存 bloomfilter; 其中一个比较有意思的设计是: footer 数据为什么是在文件末尾? 从读取文件的角度而言, 读取文件的起始位置不是更符合习惯, 从后往前读, 读取文件的后 48 字节不是需要事先知道文件的大小?\n个人认为这个设计主要是基于以下考虑:\n数据写完才知道元数据里的 stat 和 key range, 直接把元数据 append 到文件尾部, 写入效率会更高; 如果元数据在文件的起始位置, 写完数据本身之后, 还需要去更新文件起始位置的元数据信息, 这一过程有可能会导致随机 IO;\ncompaction 从 lsm tree 的写入过程可以得出, L0 层的文件都是来源于 Memtable(immutable) , 为了保证顺序写入, 每次写入到磁盘都会生成一个新的文件, 基于这种背景, 数据在查询的时候又需要去查询 L0 层的文件, 如果文件很多, 查询效率必然受影响, 而 compaction 的设计就是为了提升数据查询效率;\n文件在存储时设置层级, L0 文件大小上限为 10M L1 为 100M, 以此类推, 同时每个层级的文件个数也有限制; 触发 compaction 的条件为:\nLn 层的文件超过预定个数; Ln 层的文件大小超过层级的大小限制; 某个文件的无效读取过多; compaction 的过程就是把 Li 层的若干个文件, 合并到 Li+1层, 在合并过程中,重新计算新文件的 key 范围, 如果有相同 key , 只保留最新的 key, 合并完成之后, 重新计算元数据, 然后清理历史的文件;\n从 sstable 文件存储的形式来分析, compaction 的过程可以抽象为一个多路归并的算法, 可以参考 : leetcode , 稍微有些区别就是合并的内容是链表或者其他数据结构.\n而解决多路归并的算法, 思路也有很多, 最简单的就是直接合并多个数组,然后基于最后的数组排序, 也可以借助大根堆进行排序, 具体实现可以参考: 堆排序.\n并发问题 当在 compaction 过程中, 会有文件的清理, 此时如果用户正在读需要合并的文件, 如何解决这个读写冲突?\n遇到读写冲突, 首先就能想到加锁, 读时给文件加锁, 写操作被阻塞, 虽然这样能解决问题, 但是却违背了 lsm tree 的设计初衷: 高性能的写入;\n更好的解决办法是: MVCC(Multi-Version Concurrency Control)是多版本并发控制;\nsstable 文件设置为只读，每次 compaction 都只是对若干个 sstable 文件进行多路合并后创建新的文件，故不会影响在某个 sstable 文件读操作的正确性； sstable 都是具有版本信息的，即每次 compaction 完成后，都会生成新版本的 sstable，因此可以保障读写操作都可以针对于相应的版本文件进行，解决了读写冲突； compaction 生成的文件只有等合并完成后才会写元数据，在此期间对读操作来说是透明的，不会污染正常的读操作； 采用引用计数来控制删除行为。当 compaction 完成后试图去删除某个 sstable 文件，会根据该文件的引用计数作适当的删除延迟，即引用计数不为0时，需要等待至该文件的计数为0才真正进行删除； 参考 https://leveldb-handbook.readthedocs.io/zh/latest/basic.html\nhttps://github.com/facebook/rocksdb/wiki/RocksDB-Overview\nhttps://zhuanlan.zhihu.com/p/351241814\n",
      "summary": "基本概念 Log Structured Merge Tree, 其本质上是一种存储数据的方式,通常用于各种存储系统的底层数据结构,通过尽可能减少磁盘随机IO来提升写入性能, 适用于写多读少的场景.\n随机写和顺序写 对于一个存储系统而言, 不可避免地需要写入文件到磁盘, 对于常规的写来说, 每来一条数据写一次文件, 数据可能是 add update delete, 需要频繁操作文件, 每一次写都是一次随机 IO; 为了提高写入速度, LSM Tree 并不是每一次写操作都把文件写到磁盘, 而是将数据在内存中更新，当内存中的数据达到一定的阈值时，才将这部分数据真正刷新到磁盘文件中. 以这种方式尽可能让每次磁盘 IO 都是顺序写;\n思路 基于减少磁盘的随机 IO 来提升整体存储系统的写入性能这一背景, 很自然可以推导出用批量写入的方式, 要想批量写入, 就需要在内存维护最近写入的数据, 达到阈值之后生成一个文件写入到磁盘, 但是这样又会存在新的问题:\n如果某一条数据已经写入到磁盘文件, 后续又有更新, 怎么处理呢? 内存中维护的临时数据, 如果还未来得及写入磁盘, 服务挂了, 重新启动时, 历史写入的数据如何恢复? 每次内存中数据达到阈值,写一个整个文件到磁盘,那么最终会生成大量的文件, 如何解决? 解决问题1, 为了优化这种更新的写入, 可以采用数据版本的做法, 或者给数据增加标志, 然后定期合并, 当然, 这也是以空间换时间, 相同的数据存储了多次, 以提升写入性能; 与此同时, 在数据读取时,由于写入的逻辑改变, 一条数据可能会存在于多个文件中, 因此在读取时, 需要返回最新的数据, 在读取到多条数据时,需要对多条数据进行合取最新;\n解决问题2, 在业界比较标准的做法是 WAL, WAL 的基本原理是在执行数据修改操作之前，先将这些操作记录在日志（log）文件中, 以确保在发生故障或崩溃时，可以借助日志进行恢复并保持数据的一致性;\n解决问题3, 为了避免大量文件, 可以对文件进行定期合并, 当数据还在内存中时, 可以借助跳表或者 B+Tree 等数据结构保证内存中数据的顺序性, 在写文件时, 由于数据是有序的, 在文件合并时,很自然可以借助归并排序保证合并之后的数据的有序性, 而有序性又能天然提高查询效率."
    },{
      "title": "Skip Lists 阅读笔记",
      "url": "https://qiref.github.io/post/2023/10/01/skip-lists-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/",
      "content": "算法介绍 《Skip Lists: A Probabilistic Alternative to Balanced Trees》 论文标题翻译就是 跳表: 平衡树的概率性替代方案; 跳表是一种可以用来代替平衡树的数据结构。跳表使用概率平衡而不是严格强制的平衡，因此跳跃列表中的插入和删除算法比平衡树的等效算法要简单得多并且速度明显更快。\n从论文的标题和介绍, 基本上就能知道跳表是一种怎么样的数据结构, 为了解决平衡树实现的复杂性, 提供一种概率性平衡的数据结构,作为平衡树的平替数据结构, 查询和插入时间复杂度是 O(log n).\n算法流程 基本原理 节点结构：跳表由多个层级组成，每个层级都是一个有序链表。每个节点包含一个值和多个指向下一层级节点的指针。\n层级索引：跳表的最底层是一个普通的有序链表，每个节点都连接到下一个节点。而在更高的层级，节点以一定的概率连接到更远的节点，形成了一种“跳跃”的效果。这些连接被称为“跳跃指针”，它们允许我们在查找时可以快速地跳过一些节点。\n查找操作：从跳表的顶层开始，我们沿着每个层级向右移动，直到找到目标值或找到一个大于目标值的节点。然后我们进入下一层级继续查找，直到最底层。这种方式可以在平均情况下实现快速的查找，时间复杂度为 O(log n)。\n插入和删除操作：在插入新节点时，我们首先执行查找操作，找到合适的插入位置。然后我们在每个层级上插入新节点，并根据一定的概率决定是否要为该节点添加跳跃指针。删除操作类似，我们首先找到要删除的节点，然后将其从每个层级中移除。\n查询 level 表示跳表的层级, 而 forward[i] 是每一个层级的链表.\nSearch(list, searchKey) x := list→header // 从跳表的顶层开始,遍历到第一层 for i := list→level downto 1 do while x→forward[i]→key \u0026lt; searchKey do x := x→forward[i] // x→key \u0026lt; searchKey ≤ x→forward[1]→key // 最终的结果从跳表最底层获取 x := x→forward[1] if x→key = searchKey then return x→value else return failure 写入 由跳表的定义得出, 跳表的上一层级相当于下一层级的索引, 如果需要构建多级的索引, 首先需要解决: 当前node是否应该索引到上一层级?\n基于链表的有序性,首先就能联想到每隔 2 个元素往上建立索引, 但是这样也会带来新的问题: 每次更新元素, 有可能会导致索引的全局更新, 效率反而降低了;\n论文这里采用了概率的做法, 只需要在宏观上, 上层的 node 是下层 node 数量的 1/2, 就可以认为上层的索引建立成功, 而不需要保证每隔 n 个 node 向上索引; 这样做有个很明显的优点, 插入 node 时, 不需要去重新更新每一个层级的索引, 效率大大提高; 当然索引的稀疏性得不到保证, 但是在大批量数据背景下, 误差可以忽略不计.\n概率性 这里 randomLevel() 实现了 node 所在层级呈概率分布, 第二层概率: 1/2; 第三层概率: 1/2 * 1/2 = 1/4;\nrandomLevel() lvl := 1 // random() that returns a random value in [0...1) // p 默认是0.5 while random() \u0026lt; p and lvl \u0026lt; MaxLevel do lvl := lvl + 1 return lvl 写入的前半部分是在查找元素, 如果找到相同 key, 直接更新, 如果没找到, 则需要插入 node;\n在插入 node 之前, 需要先明确一下跳表在实现上是个怎么样的结构, 从基础的单链表出发, 每一个 node 的有 val 和 next, val 是存储的具体的值 next 指向下一个 node; 从上图的跳表描述来看, 每一层都是一个单链表, 那怎么能直接从上一层快速跳转到下一层呢?\n可以把 next 定义成一个数组, 这样通过数组下标的变化, 就能实现链表从上一层级跳转到下一层级. 论文里的 forward 就是 next 的含义.\nInsert(list, searchKey, newValue) local update[1..MaxLevel] // 执行查找动作, 并且找到被更新节点的前一个节点 update[n] x := list→header for i := list→level downto 1 do while x→forward[i]→key \u0026lt; searchKey do x := x→forward[i] -- x→key \u0026lt; searchKey ≤ x→forward[i]→key update[i] := x x := x→forward[1] if x→key = searchKey then // 如果找到直接更新 value x→value := newValue else // 没找到的情况, 需要新增节点 lvl := randomLevel() if lvl \u0026gt; list→level then // 如果新生成的层级比当前跳表层级还高的话, 需要更新跳表的层级 level, 并且新的层级update[i]需要指向header for i := list→level + 1 to lvl do update[i] := list→header list→level := lvl // 创建新节点 x := makeNode(lvl, searchKey, value) for i := 1 to level do // update[i] 指向的是新节点的前一个节点 x→forward[i] := update[i]→forward[i] update[i]→forward[i] := x 删除 Delete(list, searchKey) local update[1..MaxLevel] // 执行查找动作, 并且找到被更新节点的前一个节点 update[n] x := list→header for i := list→level downto 1 do while x→forward[i]→key \u0026lt; searchKey do x := x→forward[i] update[i] := x // 找到需要删除的目标节点的最下层级元素 x := x→forward[1] if x→key = searchKey then for i := 1 to list→level do // 判断update[n]的下一个节点是不是 x ; 如果是x,则删除x if update[i]→forward[i] ≠ x then break update[i]→forward[i] := x→forward[i] free(x) // 判断最顶层是不是为空, 为空则跳表降级 while list→level \u0026gt; 1 and list→header→forward[list→level] = NIL do list→level := list→level – 1 时间空间复杂度分析 从论文的思想来看, 很明显, 跳表这种数据结构, 就是以空间换时间的思路解决大数据量查找的问题, 通过多层级索引, 加速查询;\n对于空间复杂度, 额外利用的空间就是所有上层的索引链表, 假设有 n 个元素, 元素索引到上层概率为 1/2; 额外的空间为: n/2 + n/4 + n/8 +...; 根据等比数列求和公式, 额外空间为: n(1-1/2^k) 其中k是项数, 也就是跳表层级; 因此空间复杂度为 O(n).\n对于时间复杂度, 假设有 n 个元素, 元素索引到上层的概率为 1/2; 对于某个层级而言, 每级索引都是两个结点抽出一个结点作为上一级索引的结点时，所以每一层最多遍历3个结点, 所以最后需要查询的时间就取决于跳表的层级; l=log n, 所以查询时间复杂度为 O(log n).\n具体实现 const maxLevel = 32 const pFactor = 0.25 type SkiplistNode struct { val int forward []*SkiplistNode } type Skiplist struct { head *SkiplistNode level int } func Constructor() Skiplist { return Skiplist{\u0026amp;SkiplistNode{-1, make([]*SkiplistNode, maxLevel)}, 0} } func (Skiplist) randomLevel() int { lv := 1 for lv \u0026lt; maxLevel \u0026amp;\u0026amp; rand.Float64() \u0026lt; pFactor { lv++ } return lv } func (s *Skiplist) Search(target int) bool { curr := s.head for i := s.level - 1; i \u0026gt;= 0; i-- { // 找到第 i 层小于且最接近 target 的元素 for curr.forward[i] != nil \u0026amp;\u0026amp; curr.forward[i].val \u0026lt; target { curr = curr.forward[i] } } curr = curr.forward[0] // 检测当前元素的值是否等于 target return curr != nil \u0026amp;\u0026amp; curr.val == target } func (s *Skiplist) Add(num int) { update := make([]*SkiplistNode, maxLevel) for i := range update { update[i] = s.head } curr := s.head for i := s.level - 1; i \u0026gt;= 0; i-- { // 找到第 i 层小于且最接近 num 的元素 for curr.forward[i] != nil \u0026amp;\u0026amp; curr.forward[i].val \u0026lt; num { curr = curr.forward[i] } update[i] = curr } lv := s.randomLevel() s.level = max(s.level, lv) newNode := \u0026amp;SkiplistNode{num, make([]*SkiplistNode, lv)} for i, node := range update[:lv] { // 对第 i 层的状态进行更新，将当前元素的 forward 指向新的节点 newNode.forward[i] = node.forward[i] node.forward[i] = newNode } } func (s *Skiplist) Erase(num int) bool { update := make([]*SkiplistNode, maxLevel) curr := s.head for i := s.level - 1; i \u0026gt;= 0; i-- { // 找到第 i 层小于且最接近 num 的元素 for curr.forward[i] != nil \u0026amp;\u0026amp; curr.forward[i].val \u0026lt; num { curr = curr.forward[i] } update[i] = curr } curr = curr.forward[0] // 如果值不存在则返回 false if curr == nil || curr.val != num { return false } for i := 0; i \u0026lt; s.level \u0026amp;\u0026amp; update[i].forward[i] == curr; i++ { // 对第 i 层的状态进行更新，将 forward 指向被删除节点的下一跳 update[i].forward[i] = curr.forward[i] } // 更新当前的 level for s.level \u0026gt; 1 \u0026amp;\u0026amp; s.head.forward[s.level-1] == nil { s.level-- } return true } func max(a, b int) int { if b \u0026gt; a { return b } return a } 参考 https://15721.courses.cs.cmu.edu/spring2018/papers/08-oltpindexes1/pugh-skiplists-cacm1990.pdf\nhttps://leetcode.cn/problems/design-skiplist/solutions/1696545/she-ji-tiao-biao-by-leetcode-solution-e8yh/\n",
      "summary": "算法介绍 《Skip Lists: A Probabilistic Alternative to Balanced Trees》 论文标题翻译就是 跳表: 平衡树的概率性替代方案; 跳表是一种可以用来代替平衡树的数据结构。跳表使用概率平衡而不是严格强制的平衡，因此跳跃列表中的插入和删除算法比平衡树的等效算法要简单得多并且速度明显更快。\n从论文的标题和介绍, 基本上就能知道跳表是一种怎么样的数据结构, 为了解决平衡树实现的复杂性, 提供一种概率性平衡的数据结构,作为平衡树的平替数据结构, 查询和插入时间复杂度是 O(log n).\n算法流程 基本原理 节点结构：跳表由多个层级组成，每个层级都是一个有序链表。每个节点包含一个值和多个指向下一层级节点的指针。\n层级索引：跳表的最底层是一个普通的有序链表，每个节点都连接到下一个节点。而在更高的层级，节点以一定的概率连接到更远的节点，形成了一种“跳跃”的效果。这些连接被称为“跳跃指针”，它们允许我们在查找时可以快速地跳过一些节点。\n查找操作：从跳表的顶层开始，我们沿着每个层级向右移动，直到找到目标值或找到一个大于目标值的节点。然后我们进入下一层级继续查找，直到最底层。这种方式可以在平均情况下实现快速的查找，时间复杂度为 O(log n)。\n插入和删除操作：在插入新节点时，我们首先执行查找操作，找到合适的插入位置。然后我们在每个层级上插入新节点，并根据一定的概率决定是否要为该节点添加跳跃指针。删除操作类似，我们首先找到要删除的节点，然后将其从每个层级中移除。\n查询 level 表示跳表的层级, 而 forward[i] 是每一个层级的链表.\nSearch(list, searchKey) x := list→header // 从跳表的顶层开始,遍历到第一层 for i := list→level downto 1 do while x→forward[i]→key \u0026lt; searchKey do x := x→forward[i] // x→key \u0026lt; searchKey ≤ x→forward[1]→key // 最终的结果从跳表最底层获取 x := x→forward[1] if x→key = searchKey then return x→value else return failure 写入 由跳表的定义得出, 跳表的上一层级相当于下一层级的索引, 如果需要构建多级的索引, 首先需要解决: 当前node是否应该索引到上一层级?"
    },{
      "title": "DBLog 阅读笔记",
      "url": "https://qiref.github.io/post/2023/08/09/dblog-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/",
      "content": "介绍 论文原名: DBLog: A Watermark Based Change-Data-Capture Framework , 基于 Watermark 的 Change-Data-Capture(数据库实时捕获已提交的变更记录) 框架, 本质上是解决数据库同步(全量+增量)的框架, Watermark 是框架使用的一种手段, 在源表中创建表,生成唯一 uuid 并更新表数据, 在源表中就会生成一条变更记录,记作 Watermark 的变更记录, 通过 High Watermark 和 Low Watermark 将变更记录分割, 保证 select chunk 数据包含了增量的变更记录.\n框架整体架构如下:\n框架特点:\n按顺序处理捕获到的 changelog; 转储可以随时进行，跨所有表，针对一个特定的表或者针对一个表的具体主键; 以块(chunk)的形式获取转储，日志与转储事件交错。通过这种方式，changelog 可以与转储处理一起进行。如果进程终止，它可以在最后一个完成的块之后恢复，而不需要从头开始。这还允许在需要时对转储进行调整和暂停; 不会获取表级锁，这可以防止影响源数据库上的写流量; 支持任何类型的输出，因此，输出可以是流、数据存储甚或是 API; 设计充分考虑了高可用性。因此，下游的消费者可以放心，只要源端发生变化，它们就可以收到变化事件。 注意, 本文并非详细介绍 DBLog 框架本身, 而是分析其框架背后的设计思路.\n算法流程 chunk 划分 对于源表数据, 全量数据使用分块读取, 基于 primary key 顺序排序, 将全量数据划分为 N 个 chunk;\nwatermark 基于 chunk 划分, 然后 chunk 数据全量写入下游之后, 再将源表的变更记录 changelog 增量同步到下游, 整体思路就是这样, 但是划分 chunk 有个问题需要解决, 就是先同步到下游的数据不一定的最终的数据, 例如上图 chunk1 中的数据在同步到下游之后可能会删除, 那chunk1 的数据写到下游之后, 下游就会出现脏数据; 如何解决 chunk 和 changelog 之间不会相互覆盖的问题?\n为了解决这一问题, DBLog 的解决办法是引入 watermark 的机制, 在查询 chunk 期间, 对 changelog 进行标记 , 然后去移除 select chunk 期间, chunk 数据中对应的 changelog 数据, 这样就解决了 select chunk 数据和期间对应的 changelog 数据的顺序问题 , 这也就是论文的精妙之处!\nwatermark 是通过源数据库中的一个表实现的, 表存储在专用的命名空间中，因此不会与应用程序表发生冲突。改表只包含一行,存储 UUID 字段; 通过将这一行更新为特定的 UUID 来生成watermark, 行更新生成一个更改事件。\n算法伪码 算法伪代码如下:\n// step1 暂停处理 changelog pause log event processing lw := uuid() hw := uuid() // step2 通过更新watermark表生成 low watermark update watermark table set value = lw // step3 为下一个块运行 SELECT 语句，并将结果集存储在内存中，按主键索引 chunk := select next chunk from table // step4 通过更新watermark表生成 high watermark update watermark table set value = hw // step5 恢复处理 changelog, 监听 high watermark resume log event processing inwindow := false // other steps of event prosessing loop while true do e := next event from changelog if not inwindow then{ if e is not watermark then append e to outputbuffer else if e is watermark with value lw then inwindow := true }else{ if e is not watermark then{ // step 6 接收到lw事件后，开始从结果集中删除所有在lw之后接收到的changelog主键的条目 if chunk contains e.key then{ remove e.key from chunk } append e to outputbuffer }else if e is watermark with value hw then { // step7 接收到hw事件后，将所有剩余的结果集条目发送到输出 for each row in chunk do{ append row to outputbuffer } } } 算法步骤分析 示例表，k1 到 k6 为主键。每个更改日志条目表示主键的创建、更新或删除事件。在下图展示了watermark的生成和chunk的选择（step 1 到 step 4）。重点看从位于watermark之间的主键结果: 删除选定chunk的行（step 5 到 step 7）。\n如果一个或多个事务在 lw 和 hw 之间提交了大量的行更改，则可能会出现大量的 changelog。这就是为什么论文在 step 2-4 期间会短暂地暂停日志处理，从而保证不会遗漏watermark, 这样changelog处理就可以在以后逐个事件地恢复。日志处理暂停的时间很短，因为 step 2-4 预计会比较快：水印更新是单个的写操作，而 SELECT 操作有一定的范围, 可能耗时较长。\n在第 7 步接收到hw后，非冲突的chunk块将被提交写入，以便按顺序发送到下游。这是一个非阻塞操作，因为写入在单独的线程中运行，允许在step 7 之后快速恢复日志处理。然后，changelog处理将继续处理hw之后发生的事件。\n在下图中，使用上图相同的示例来描述整个同步数据的写顺序, 出现在hw之前的changelog首先被写入, 然后是chunk结果(被修正后), 最后是在hw之后发生的changelog。\n问题 当chunk划分完之后, 进入增量的 binlog offset 应该如何选取? chunk 划分过程中,实际上已经把 lw 到 hw 范围中的增量数据, 合并到 chunk 的全量数据中(只合并chunk范围内的数据), 因此, 增量阶段的 binlog offset 应该要从chuck中最小的 hw 开始, 一直到最大的 hw 为止, 这个范围内的数据, 如果 binlog 已经包含在 chunk 中, 就无需处理, 否则往下游发送;\n增量阶段开始的 offset 为什么不是最小的 lw ? 如果从chuck中最小的 hw 开始, 那从 lw - hw 这一段的 binlog中, 没被合并到 chunk 中的 binlog不就漏掉了?\n如下图, [lw1, hw1] 这个范围中, 如果在 chunk 中, 会合并, 如果不在 chunk 中, 这个范围的数据在下一个 chunk 的全量阶段能查询到, 所以从 min(hw) 的 offset 开启增量阶段即可.\n多个 chunk 读取是并行的, 如果两个 chunk 的 lw 到 hw binlog 数据有交叉, 或者 chunk 乱序, 是否会影响结果? 在增量阶段开始时, 会从 min(hw) 的 offset 开始消费 binlog, 直到 max(hw), 此范围中的数据, 如果是不在对应的 chunk 中(全量阶段已经合并), 则会向下游发送;\n当 watermark 有交叉时, 交叉部分的 binlog 会依次在不同的 chunk 中进行判断, 因此不会重复; 当 watermark 乱序时, 全量阶段,每个 chunk 划分的范围不同, 乱序没有影响, 至于 binlog 消费, 也是顺序的, 不会影响增量阶段的顺序;\n参考 https://arxiv.org/pdf/2010.12597.pdf\nhttps://netflixtechblog.com/dblog-a-generic-change-data-capture-framework-69351fb9099b\n",
      "summary": "介绍 论文原名: DBLog: A Watermark Based Change-Data-Capture Framework , 基于 Watermark 的 Change-Data-Capture(数据库实时捕获已提交的变更记录) 框架, 本质上是解决数据库同步(全量+增量)的框架, Watermark 是框架使用的一种手段, 在源表中创建表,生成唯一 uuid 并更新表数据, 在源表中就会生成一条变更记录,记作 Watermark 的变更记录, 通过 High Watermark 和 Low Watermark 将变更记录分割, 保证 select chunk 数据包含了增量的变更记录.\n框架整体架构如下:\n框架特点:\n按顺序处理捕获到的 changelog; 转储可以随时进行，跨所有表，针对一个特定的表或者针对一个表的具体主键; 以块(chunk)的形式获取转储，日志与转储事件交错。通过这种方式，changelog 可以与转储处理一起进行。如果进程终止，它可以在最后一个完成的块之后恢复，而不需要从头开始。这还允许在需要时对转储进行调整和暂停; 不会获取表级锁，这可以防止影响源数据库上的写流量; 支持任何类型的输出，因此，输出可以是流、数据存储甚或是 API; 设计充分考虑了高可用性。因此，下游的消费者可以放心，只要源端发生变化，它们就可以收到变化事件。 注意, 本文并非详细介绍 DBLog 框架本身, 而是分析其框架背后的设计思路.\n算法流程 chunk 划分 对于源表数据, 全量数据使用分块读取, 基于 primary key 顺序排序, 将全量数据划分为 N 个 chunk;\nwatermark 基于 chunk 划分, 然后 chunk 数据全量写入下游之后, 再将源表的变更记录 changelog 增量同步到下游, 整体思路就是这样, 但是划分 chunk 有个问题需要解决, 就是先同步到下游的数据不一定的最终的数据, 例如上图 chunk1 中的数据在同步到下游之后可能会删除, 那chunk1 的数据写到下游之后, 下游就会出现脏数据; 如何解决 chunk 和 changelog 之间不会相互覆盖的问题?"
    },{
      "title": "Java进程分析工具",
      "url": "https://qiref.github.io/post/2023/06/26/java%E8%BF%9B%E7%A8%8B%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7/",
      "content": "JVM 内存区域 如果要为新生代分配 256m 的内存（NewSize 与 MaxNewSize 设为一致），参数应该这样来写：-Xmn256m;\n还可以通过 -XX:NewRatio=\u0026lt;int\u0026gt; 来设置老年代与新生代内存的比值。比如以下参数就是设置老年代与新生代内存的比值为 1。也就是说老年代和新生代所占比值为 1：1，新生代占整个堆栈的 1/2。\n-XX:NewRatio=1 JDK 1.8 ，方法区（HotSpot 的永久代）被彻底移除了，取而代之是元空间 Metaspace，元空间使用的是本地内存。\nMetaspace 的初始容量并不是 -XX:MetaspaceSize 设置，无论 -XX:MetaspaceSize 配置什么值，对于 64 位 JVM 来说，Metaspace 的初始容量都是 21807104（约 20.8m）。可以参考 Oracle 官方文档 :\nSpecify a higher value for the option MetaspaceSize to avoid early garbage collections induced for class metadata. The amount of class metadata allocated for an application is application-dependent and general guidelines do not exist for the selection of MetaspaceSize. The default size of MetaspaceSize is platform-dependent and ranges from 12 MB to about 20 MB.\nMetaspace 由于使用不断扩容到-XX:MetaspaceSize参数指定的量，就会发生 Full GC，且之后每次 Metaspace 扩容都会发生 Full GC。也就是说，MetaspaceSize 表示 Metaspace 使用过程中触发 Full GC 的阈值，只对触发起作用；如果 MaxMetaspaceSize 设置太小，可能会导致频繁 Full GC ，甚至OOM。\njmap 打印heap信息 命令：jmap -heap pid\n描述：显示Java堆详细信息\n➜ ~ jmap -heap 10439 Attaching to process ID 10439, please wait... Error attaching to process: sun.jvm.hotspot.runtime.VMVersionMismatchException: Supported versions are 25.312-b07. Target VM is 25.272-b1 sun.jvm.hotspot.debugger.DebuggerException: sun.jvm.hotspot.runtime.VMVersionMismatchException: Supported versions are 25.312-b07. Target VM is 25.272-b1 at sun.jvm.hotspot.HotSpotAgent.setupVM(HotSpotAgent.java:435) at sun.jvm.hotspot.HotSpotAgent.go(HotSpotAgent.java:305) at sun.jvm.hotspot.HotSpotAgent.attach(HotSpotAgent.java:140) at sun.jvm.hotspot.tools.Tool.start(Tool.java:185) at sun.jvm.hotspot.tools.Tool.execute(Tool.java:118) at sun.jvm.hotspot.tools.HeapSummary.main(HeapSummary.java:50) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at sun.tools.jmap.JMap.runTool(JMap.java:201) at sun.tools.jmap.JMap.main(JMap.java:130) Caused by: sun.jvm.hotspot.runtime.VMVersionMismatchException: Supported versions are 25.312-b07. Target VM is 25.272-b1 at sun.jvm.hotspot.runtime.VM.checkVMVersion(VM.java:227) at sun.jvm.hotspot.runtime.VM.\u0026lt;init\u0026gt;(VM.java:294) at sun.jvm.hotspot.runtime.VM.initialize(VM.java:370) at sun.jvm.hotspot.HotSpotAgent.setupVM(HotSpotAgent.java:431) 报错原因是系统有多个jdk，可以指定绝对路径。\n➜ ~ echo $JAVA_HOME /usr/local/TencentKona-8.0.4-272 ➜ ~ /usr/local/TencentKona-8.0.4-272/bin/jmap -heap 10439 Attaching to process ID 10439, please wait... Debugger attached successfully. Server compiler detected. JVM version is 25.272-b1 using thread-local object allocation. Garbage-First (G1) GC with 8 thread(s) Heap Configuration: MinHeapFreeRatio = 40 MaxHeapFreeRatio = 70 MaxHeapSize = 536870912 (512.0MB) NewSize = 1363144 (1.2999954223632812MB) MaxNewSize = 321912832 (307.0MB) OldSize = 5452592 (5.1999969482421875MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 260046848 (248.0MB) MaxMetaspaceSize = 268435456 (256.0MB) G1HeapRegionSize = 1048576 (1.0MB) Heap Usage: G1 Heap: regions = 512 capacity = 536870912 (512.0MB) used = 226987352 (216.47200775146484MB) free = 309883560 (295.52799224853516MB) 42.27968901395798% used G1 Young Generation: Eden Space: regions = 207 capacity = 338690048 (323.0MB) used = 217055232 (207.0MB) free = 121634816 (116.0MB) 64.08668730650155% used Survivor Space: regions = 0 capacity = 0 (0.0MB) used = 0 (0.0MB) free = 0 (0.0MB) 0.0% used G1 Old Generation: regions = 10 capacity = 198180864 (189.0MB) used = 9932120 (9.472007751464844MB) free = 188248744 (179.52799224853516MB) 5.0116443129443615% used 8841 interned Strings occupying 891192 bytes. 统计heap对象 命令：jmap -histo:live pid 描述：显示堆中对象的统计信息\n其中包括每个Java类、对象数量、内存大小(单位：字节)、完全限定的类名。打印的虚拟机内部的类名称将会带有一个 * 前缀。如果指定了live子选项，则只计算活动的对象。\n➜ ~ /usr/local/TencentKona-8.0.4-272/bin/jmap -histo:live 10439 |head -n 30 num #instances #bytes class name ---------------------------------------------- 1: 23634 2041848 [C 2: 43 1409712 [Ljava.util.concurrent.ForkJoinTask; 3: 1053 1191208 [B 4: 7197 790496 java.lang.Class 5: 23631 567144 java.lang.String 6: 6322 542264 [Ljava.lang.Object; 7: 8725 279200 java.util.concurrent.ConcurrentHashMap$Node 8: 4123 263872 java.nio.DirectByteBuffer 9: 4096 229376 org.apache.flink.core.memory.MemorySegment 10: 2900 201184 [I 11: 4117 164680 sun.misc.Cleaner 12: 4115 131680 java.nio.DirectByteBuffer$Deallocator 13: 3923 125536 java.util.HashMap$Node 14: 5009 80144 java.lang.Object 15: 900 79200 java.lang.reflect.Method 16: 811 77264 [Ljava.util.HashMap$Node; 17: 1304 73024 java.lang.invoke.MemberName 18: 101 66976 [Ljava.util.concurrent.ConcurrentHashMap$Node; 19: 4160 66560 java.util.concurrent.atomic.AtomicBoolean 20: 80 53760 org.apache.flink.shaded.netty4.io.netty.util.internal.shaded. org.jctools.queues.MpscArrayQueue 21: 1266 50640 java.util.LinkedHashMap$Entry 22: 1160 46400 java.lang.invoke.MethodType 23: 1144 45760 java.lang.ref.SoftReference 24: 1663 41632 [Ljava.lang.Class; 25: 986 39440 com.typesafe.config.impl.SimpleConfigOrigin 26: 1177 37664 java.lang.invoke.MethodType$ConcurrentWeakInternSet$WeakEntry 27: 777 37296 java.util.HashMap 打印等待终结的对象 命令：jmap -finalizerinfo pid\n描述：打印等待终结的对象信息\n➜ ~ /usr/local/TencentKona-8.0.4-272/bin/jmap -finalizerinfo 10439 Attaching to process ID 10439, please wait... Debugger attached successfully. Server compiler detected. JVM version is 25.272-b1 Number of objects pending for finalization: 0 生成dump文件 命令：jmap -dump:format=b,file=heapdump.hprof pid\n描述：生成堆转储快照dump文件。\n➜ ~ /usr/local/TencentKona-8.0.4-272/bin/jmap -dump:format=b,file=heapdump.hprof 10439 Dumping heap to /root/heapdump.hprof ... Heap dump file created 以hprof二进制格式转储Java堆到指定filename的文件中。live子选项是可选的，如果指定了live子选项，堆中只有活动的对象会被转储。想要浏览heap dump，你可以使用jhat(Java堆分析工具)读取生成的文件。\njhat 分析dump文件 命令：jhat -J-mx1024m heapdump.hprof，当dump文件很大时，需要适当调高-mx； 当端口有占用时 jhat -port 7080 heapdump.hprof 可以指定端口；\n➜ ~ jhat -J-mx1024m heapdump.hprof Reading from heapdump.phrof... Dump file created Tue May 24 21:38:43 CST 2022 Snapshot read, resolving... Resolving 4346760 objects... Chasing references, expect 869 dots Eliminating duplicate references. Snapshot resolved. Started HTTP server on port 7000 Server is ready. jhat会启动一个web服务，在web界面上查看dump文件解析的结果。\n使用EMA工具，会比jhat更友好的界面；\njinfo jinfo 获取 JVM 运行参数，包括启动时指定的环境变量；\n$ /usr/local/TencentKona-8.0.10-332/bin/jinfo 1 Attaching to process ID 1, please wait... Debugger attached successfully. Server compiler detected. JVM version is 25.332-b1 Java System Properties: zookeeper.sasl.client = true java.runtime.name = OpenJDK Runtime Environment java.vm.version = 25.332-b1 sun.boot.library.path = /usr/local/TencentKona-8.0.10-332/jre/lib/amd64 java.vm.vendor = Tencent path.separator = : file.encoding.pkg = sun.io java.vm.name = OpenJDK 64-Bit Server VM sun.os.patch.level = unknown sun.java.launcher = SUN_STANDARD user.dir = / java.vm.specification.name = Java Virtual Machine Specification java.runtime.version = 1.8.0_332-b1 java.awt.graphicsenv = sun.awt.X11GraphicsEnvironment os.arch = amd64 java.endorsed.dirs = /usr/local/TencentKona-8.0.10-332/jre/lib/endorsed line.separator = ... VM Flags: Non-default VM flags: -XX:ActiveProcessorCount=8 -XX:CICompilerCount=4 -XX:CompressedClassSpaceSize=260046848 -XX:ErrorFile=null -XX:InitialHeapSize=3103784960 -XX:MaxDirectMemorySize=493921243 -XX:MaxHeapSize=3103784960 -XX:MaxMetaspaceSize=268435456 -XX:MaxNewSize=1034420224 -XX:MinHeapDeltaBytes=524288 -XX:NewSize=1034420224 -XX:OldSize=2069364736 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC -XX:+UseUTF8UTF16Intrinsics Command line: -Duser.timezone=GMT+08 -Dlog.level=INFO -Xmx3103113861 -Xms3103113861 -XX:MaxDirectMemorySize=493921243 -XX:MaxMetaspaceSize=268435456 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:ActiveProcessorCount=8 -Dlog4j2.formatMsgNoLookups=true jstat jstat -gc 监控GC相关信息。\n/usr/local/TencentKona-8.0.10-332/bin/jstat -gc 1 1000 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT 43520.0 43008.0 0.0 27234.4 922112.0 282128.7 2020864.0 35542.2 76928.0 73257.9 10368.0 9767.7 9 0.124 3 0.320 0.444 43520.0 43008.0 0.0 27234.4 922112.0 282128.7 2020864.0 35542.2 76928.0 73257.9 10368.0 9767.7 9 0.124 3 0.320 0.444 43520.0 43008.0 0.0 27234.4 922112.0 282128.7 2020864.0 35542.2 76928.0 73257.9 10368.0 9767.7 9 0.124 3 0.320 0.444 43520.0 43008.0 0.0 27234.4 922112.0 282128.7 2020864.0 35542.2 76928.0 73257.9 10368.0 9767.7 9 0.124 3 0.320 0.444 43520.0 43008.0 0.0 27234.4 922112.0 282128.7 2020864.0 35542.2 76928.0 73257.9 10368.0 9767.7 9 0.124 3 0.320 0.444 43520.0 43008.0 0.0 27234.4 922112.0 285552.9 2020864.0 35542.2 76928.0 73257.9 10368.0 9767.7 9 0.124 3 0.320 0.444 43520.0 43008.0 0.0 27234.4 922112.0 285992.0 2020864.0 35542.2 76928.0 73257.9 10368.0 9767.7 9 0.124 3 0.320 0.444 43520.0 43008.0 0.0 27234.4 922112.0 285992.0 2020864.0 35542.2 76928.0 73257.9 10368.0 9767.7 9 0.124 3 0.320 0.444 S0C: S0区0的容量 (kB); S1C: S1的容量(kB); S0U: S0已用内存 (kB); S1U: S1已用内存 (kB); EC: 伊甸园区容量 (kB); EU: 伊甸园区已用内存 (kB); OC: 老年代容量 (kB); OU: 老年代已用内存 (kB); MC: 元数据区容量 (kB); MU: 元数据区已用内存 (kB); CCSC: 类压缩区容量 (kB); CCSU: 类压缩区已用内存 (kB); YGC：年轻代垃圾回收次数; YGCT：年轻代垃圾回收时间; FGC：老年代垃圾回收次数; FGCT：老年代垃圾回收时间; GCT：总垃圾回收时间; jstat -gcutil 监控JVM GC 的信息。\n/usr/local/TencentKona-8.0.10-332/bin/jstat -gcutil 1 1000 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 0.00 63.32 9.70 1.76 95.23 94.21 9 0.124 3 0.320 0.444 0.00 63.32 9.71 1.76 95.23 94.21 9 0.124 3 0.320 0.444 0.00 63.32 9.71 1.76 95.23 94.21 9 0.124 3 0.320 0.444 0.00 63.32 10.92 1.76 95.23 94.21 9 0.124 3 0.320 0.444 0.00 63.32 12.08 1.76 95.23 94.21 9 0.124 3 0.320 0.444 0.00 63.32 12.08 1.76 95.23 94.21 9 0.124 3 0.320 0.444 0.00 63.32 13.03 1.76 95.23 94.21 9 0.124 3 0.320 0.444 S0：S0区域使用率 S1：S1区域使用率 E：伊甸园区使用率 O：Old Generation使用率，OU/OC M：Matespace区使用率，MU/MC CCS：压缩类空间使用率 YGC：年轻代垃圾回收次数 YGCT：年轻代垃圾回收时间 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收时间 GCT：总垃圾回收时间 jstack jstack -l 获取线程栈信息，可以分析java进程中线程的执行情况，可以协助分析死锁问题；\n/usr/local/TencentKona-8.0.10-332/bin/jstack -l 1 一般情况下，线程栈会很长，可以输出到文件中分析.\n/usr/local/TencentKona-8.0.10-332/bin/jstack -l 1 \u0026gt; /tmp/1.threaddump Native Memory Tracking 默认情况下，NMT是处于关闭状态的，我们可以通过设置 JVM 启动参数来开启：-XX:NativeMemoryTracking=[off | summary | detail]；（启用NMT会导致5% -10%的性能开销）\noff 不跟踪 JVM 本地内存使用情况。如果不指定 -XX:NativeMemoryTracking 选项则默认为off。 summary 仅跟踪 JVM 子系统（如：Java heap、class、code、thread等）的内存使用情况。 detail 除了通过 JVM 子系统跟踪内存使用情况外，还可以通过单独的 CallSite、单独的虚拟内存区域及其提交区域来跟踪内存使用情况。 除了在虚拟机运行时获取 NMT 数据，我们还可以通过两个参数：-XX:+UnlockDiagnosticVMOptions和-XX:+PrintNMTStatistics ，来获取虚拟机退出时内存使用情况的数据（输出数据的详细程度取决于你设定的跟踪级别，如 summary/detail 等）。\n-XX:+UnlockDiagnosticVMOptions：解锁用于诊断 JVM 的选项，默认关闭。 -XX:+PrintNMTStatistics：当启用 NMT 时，在虚拟机退出时打印内存使用情况，默认关闭，需要开启前置参数 -XX:+UnlockDiagnosticVMOptions 才能正常使用。 开启 NMT jcmd \u0026lt;pid\u0026gt; VM.native_memory [summary | detail | baseline | summary.diff | detail.diff | shutdown] [scale= KB | MB | GB] jcmd 33561 VM.native_memory summary scale=MB 33561: Native Memory Tracking: Total: reserved=5593MB, committed=477MB - Java Heap (reserved=4096MB, committed=256MB) (mmap: reserved=4096MB, committed=256MB) - Class (reserved=1049MB, committed=25MB) (classes #2428) (malloc=11MB #2029) (mmap: reserved=1038MB, committed=14MB) - Thread (reserved=25MB, committed=25MB) (thread #26) (stack: reserved=25MB, committed=25MB) - Code (reserved=244MB, committed=5MB) (malloc=1MB #1623) (mmap: reserved=244MB, committed=4MB) - GC (reserved=162MB, committed=150MB) (malloc=12MB #148) (mmap: reserved=150MB, committed=137MB) - Internal (reserved=12MB, committed=12MB) (malloc=12MB #4010) - Symbol (reserved=4MB, committed=4MB) (malloc=3MB #13444) (arena=1MB #1) 各区域说明 主要关注 committed 部分，这部分是已使用的内存；\nJava Heap Java 堆所使用的内存大小；可以使用 -Xms/-Xmx 或 -XX:InitialHeapSize/-XX:MaxHeapSize 等参数来控制初始/最大的大小\nClass Class 主要是类元数据（meta data）所使用的内存空间，即虚拟机规范中规定的方法区。具体到 HotSpot 的实现中，JDK7 之前是实现在 PermGen 永久代中，JDK8 之后则是移除了 PermGen 变成了 MetaSpace 元空间。 基于此，那在启动 JVM 进程的时候设置的 -XX:MaxMetaspaceSize=256M 参数应该可以限制 Class 所使用的内存大小； 但实际情况并非这样，这里 Class 需要设置 -XX:CompressedClassSpaceSize=256M 来控制。\n-XX:MaxMetaspaceSize：Metaspace 总空间的最大允许使用内存，默认是不限制。 -XX:CompressedClassSpaceSize：Metaspace 中的 Compressed Class Space 的最大允许内存，默认值是 1G，这部分会在 JVM 启动的时候向操作系统申请 1G 的虚拟地址映射，但不是真的就用了操作系统的 1G 内存。 Thread 线程所使用的内存；可以使用参数 -Xss/-XX:ThreadStackSize 设置\nCode JVM 自身会生成一些 native code 并将其存储在称为 codecache 的内存区域中。JVM 生成 native code 的原因有很多，包括动态生成的解释器循环、 JNI、即时编译器(JIT)编译 Java 方法生成的本机代码 。其中 JIT 生成的 native code 占据了 codecache 绝大部分的空间。\ncodecache reserve 的最大内存是由 -XX:ReservedCodeCacheSize 参数决定的； codecache commit 的内存是由 -XX:InitialCodeCacheSize 参数决定的。\nGC GC 所使用的内存，就是垃圾收集器使用的数据所占据的内存，例如卡表 card tables、记忆集 remembered sets、标记栈 marking stack、标记位图 marking bitmaps 等等。其实都是一种借助额外的空间，来记录不同内存区域之间引用关系的结构（都是基于空间换时间的思想，否则寻找引用关系就需要诸如遍历这种浪费时间的方式）。GC 这块内存是必须的，也是我们在使用过程中无法压缩的。\nInternal Internal 包含命令行解析器使用的内存、JVMTI、PerfData 以及 Unsafe 分配的内存等等。 直接内存可以由 -XX:MaxDirectMemorySize=128M 参数决定，最后会在 Internal 中体现：\nByteBuffer.allocateDirect(256 * _1M); // 如果设置 -XX:MaxDirectMemorySize=128M，这里分配 DirectMemory 会失败 Symbol Symbol 为 JVM 中的符号表所使用的内存，HotSpot中符号表主要有两种：SymbolTable 与 StringTable。\nJava 的类在编译之后会生成 Constant pool 常量池，常量池中会有很多的字符串常量，HotSpot 出于节省内存的考虑，往往会将这些字符串常量作为一个 Symbol 对象存入一个 HashTable 的表结构中即 SymbolTable，如果该字符串可以在 SymbolTable 中 lookup（SymbolTable::lookup）到，那么就会重用该字符串，如果找不到才会创建新的 Symbol（SymbolTable::new_symbol）。\n除了 SymbolTable，还有 StringTable（StringTable 结构与 SymbolTable 基本是一致的，都是 HashTable 的结构），即我们常说的字符串常量池。HotSpot 也是基于节省内存的考虑为我们提供了 StringTable，我们可以通过 String.intern 的方式将字符串放入 StringTable 中来重用字符串。\n-XX:StringTableSize 参数设置 HashTable 的长度； -XX:SymbolTableSize 参数设置 SymbolTable 的长度。\n如果该值设置的过小的话会导致hash 冲突，即使 HashTable 进行 rehash，hash 冲突也会十分频繁；\n基线分析 jcmd 33561 VM.native_memory baseline 33561: Baseline succeeded jcmd 33561 VM.native_memory summary.diff scale=MB 33561: Native Memory Tracking: Total: reserved=5593MB, committed=479MB - Java Heap (reserved=4096MB, committed=256MB) (mmap: reserved=4096MB, committed=256MB) - Class (reserved=1049MB, committed=26MB) (classes #2428) (malloc=11MB #2270 +99) (mmap: reserved=1038MB, committed=14MB) - Thread (reserved=25MB, committed=25MB) (thread #26) (stack: reserved=25MB, committed=25MB) - Code (reserved=245MB, committed=6MB) (malloc=1MB #1996 +106) (mmap: reserved=244MB, committed=5MB) - GC (reserved=162MB, committed=150MB) (malloc=12MB #148) (mmap: reserved=150MB, committed=137MB) - Internal (reserved=12MB, committed=12MB) (malloc=12MB #4029 +3) - Symbol (reserved=4MB, committed=4MB) (malloc=3MB #13444) (arena=1MB #1) - Native Memory Tracking (reserved=1MB, committed=1MB) (tracking overhead=0MB) 基线分析会输出从 baseline 开始，到现在各区域的大小变化，以及线程数量变化；\n",
      "summary": "JVM 内存区域 如果要为新生代分配 256m 的内存（NewSize 与 MaxNewSize 设为一致），参数应该这样来写：-Xmn256m;\n还可以通过 -XX:NewRatio=\u0026lt;int\u0026gt; 来设置老年代与新生代内存的比值。比如以下参数就是设置老年代与新生代内存的比值为 1。也就是说老年代和新生代所占比值为 1：1，新生代占整个堆栈的 1/2。\n-XX:NewRatio=1 JDK 1.8 ，方法区（HotSpot 的永久代）被彻底移除了，取而代之是元空间 Metaspace，元空间使用的是本地内存。\nMetaspace 的初始容量并不是 -XX:MetaspaceSize 设置，无论 -XX:MetaspaceSize 配置什么值，对于 64 位 JVM 来说，Metaspace 的初始容量都是 21807104（约 20.8m）。可以参考 Oracle 官方文档 :\nSpecify a higher value for the option MetaspaceSize to avoid early garbage collections induced for class metadata. The amount of class metadata allocated for an application is application-dependent and general guidelines do not exist for the selection of MetaspaceSize."
    },{
      "title": "The Dataflow Model 阅读笔记",
      "url": "https://qiref.github.io/post/2023/05/16/the-dataflow-model-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/",
      "content": "Dataflow 计算模型 Dataflow 的核心计算模型非常简单，它只有两个概念，一个叫做 ParDo，就是并行处理的意思；另一个叫做 GroupByKey，也就是按照 Key 进行分组。\nParDo ParDo 用来进行通用的并行化处理。每个输入元素（这个元素本身有可能是一个有限的集合）都会使用一个 UDF 进行处理（在Dataflow中叫做DoFn），输出是0或多个输出元素。这个例子是把键的前缀进行展开，然后把值复制到展开后的键构成新的键值对并输出。\nGroupByKey GroupByKey 用来按 Key 把元素重新分组。\nParDo 操作因为是对每个输入的元素进行处理，因此很自然地就可以适用于无边界的数据。而 GroupByKey 操作，在把数据发送到下游进行汇总前，需要收集到指定的键对应的所有数据。如果输入源是无边界的，那么我们不知道何时才能收集到所有的数据。所以通常的解决方案是对数据使用窗口操作。\n窗口 时间语义 窗口通常基于时间，时间对于窗口来说是必不可少的，在流式计算中，有 processing-time 和 event-time 两种时间语义，具体参考： 时间语义\n窗口分类 固定窗口（Fixed Window）固定区间（互不重叠）的窗口，可以基于时间，也可以基于数量；将事件分配到不同区间的窗口中，在通过窗口边界后，窗口内的所有事件会发送给计算函数进行计算；\n滑动窗口（Sliding Window）固定区间但可以重叠的窗口，需要指定窗口区间以及滑动步长，区间重叠意味着同一个事件会分配到不同窗口参与计算。 窗口区间决定何时触发计算，滑动步长决定何时创建一个新的窗口；\n会话窗口（Session Window）会话窗口通常基于用户的会话，通过定义会话的超时时间，将事件分割到不同的会话中； 例如，有个客服聊天系统，如果用户超过 30 分钟没有互动，则认为一次会话结束，当客户下次进入，就是一个新的会话了。\n窗口分配与合并 Dataflow 模型里，需要的不只是 GroupByKey，实际在统计数据的时候，往往需要的是 GroupByKeyAndWindow。统计一个不考虑任何时间窗口的数据，往往是没有意义的； Dataflow 模型提出：\n从模型简化的角度上，把所有的窗口策略都当做非对齐窗口，而底层实现来负责把对齐窗口作为一个特例进行优化。 窗口操作可以被分隔为两个互相相关的操作： set\u0026lt;Window\u0026gt; AssignWindows(T datum) 即窗口分配操作。这个操作把元素分配到 0 或多个窗口中去。 set\u0026lt;window\u0026gt; MergeWindows(Set\u0026lt;Window\u0026gt; windows) 即窗口合并操作，这个操作在汇总时合并窗口。 而在实际的逻辑实现层面，Dataflow 最重要的两个函数，也就是 AssignWindows 函数和 MergeWindows 函数。 窗口分配 每一个原始的事件，在业务处理函数之前，其实都是（key, value, event_time）这样一个三元组。而 AssignWindows 要做的，就是把这个三元组，根据我们的处理逻辑，变成（key, value, event_time, window）这样的四元组。\n需要注意的是，在窗口分配过程中，滑动窗口中存在区间重合的情况，那么在为事件分配窗口的过程中，按照上文四元组的定义，可能一个事件会变成多个事件；\n窗口合并 Dataflow 里，通过 AssignWindows+MergeWindows 的组合，来进行相应的数据统计。我们还是以前面会话窗口中的案例：客户 30 分钟没有互动就算作超时。\n因为要根据同一个用户的行为进行分析，所以 Key 是用户 ID 。那么对应的 Value 里，可以记录消息发送方，以及对应的消息内容。而 event_time，则是实际消息发送的时间。对于每一个事件，我们进行 AssignWindows 的时候，都是把对应的时间窗口，设置成 [eventt​ime,eventt​ime+30)。也就是事件发生之后的 30 分钟超时时间之内，都是这个事件对应会话的时间窗口。而在同一个 Key 的多个事件，我们可以把这些窗口合并。对于会话窗口，如果两个事件的窗口之间有重合部分，我们就可以把它们合并成一个更大的时间窗口。而如果不同事件之间的窗口没有重合，那么这两个事件就还是两个各自独立的时间窗口。\n例如同一个用户下，有三个事件，发生的时间分别是 13:02、13:14、13:57。那么分配窗口的时候，三个窗口会是 [13:02,13:32)，[13:14,13:44) 以及 [13:57,14:27)。前两个时间窗口是有重叠部分的，但是第三个时间窗口并没有重叠，对应的窗口会合并成 [13:02,13:44) 以及 [13:57,14:27) 这样两个时间窗口。\n窗口的分配和合并功能，就使得 Dataflow 可以处理乱序数据。相同的数据以不同的顺序到达我们的计算节点，计算的结果仍然是相同的。并且在这个过程里，我们可以把上一次计算完的结果作为状态持久化下来，然后每一个新进入的事件，都按照 AssignWindows 和 MergeWindows 的方式不断对数据进行化简。\n触发器和增量处理 基于 watermark 处理迟到数据必然会面临以下两个问题：\n在 watermark 之后的事件如何处理？ 为了数据准确性而设置过长的 watermark ，会导致没有迟到数据也会等待过长时间，从而失去时效性。 在 Dataflow 中给出如下思路：\n基于 lambda 架构的思想，尽快给出一个计算结果，然后在后续的事件处理过程中，去不断修正计算结果，在 Dataflow 中体现为 触发器（Trigger） 机制；\n触发器除了定义数据何时计算，还可以定义触发之后的输出策略：\n抛弃（Discarding）策略，也就是触发之后，对应窗口内的数据就被抛弃掉了。这意味着后续如果有窗口内的数据到达，也没法和上一次触发时候的结果进行合并计算。但这样做的好处是，每个计算节点的存储空间占用不会太大。一旦触发向下游输出计算结果了，现有的数据我们也就不需要了。比如，一个监控系统，根据本地时间去统计错误日志的数量并告警，使用这种策略就会比较合适。 累积（Accumulating）策略，也就是触发之后，对应窗口内的数据，仍然会持久化作为状态保存下来。当有新的日志过来，我们仍然会计算新的计算结果，并且我们可以再次触发，向下游发送新的计算结果，而下游也会用新的计算结果来覆盖掉老的计算结果。 累积并撤回（Accumulating \u0026amp; Retracting）策略，也就是我们除了“修正”计算结果之外，可能还要“撤回”计算结果。在进行累积语义的基础上，计算结果的一份复制也被保留到持久化状态中。当窗口将来再次触发时，上一次的结果值先下发做撤回处理，然后新的结果作为正常数据下发； 以前面的客服会话为例：原本先收到了三个事件，13:02、13:14、13:57，根据 30 分钟的会话窗口，经过合并后，窗口就变成了 [13:02,13:44) 以及 [13:57,14:27) 这样两个时间窗口。并且，这两个会话分别作为两条记录，向下游的不同计算节点下发了。这个时候，我们又接收到了一条姗姗来迟的新日志，日志的时间是 13:40，这个用户其实只有一个会话 [13:02,14:27)。所以，我们不仅要向下游发送一个新会话出去，还需要能够“撤回”之前已经发送的两个错误的会话。\n参考 43864.pdf 论文-Dataflow 模型 https://developer.aliyun.com/article/64911 ",
      "summary": "Dataflow 计算模型 Dataflow 的核心计算模型非常简单，它只有两个概念，一个叫做 ParDo，就是并行处理的意思；另一个叫做 GroupByKey，也就是按照 Key 进行分组。\nParDo ParDo 用来进行通用的并行化处理。每个输入元素（这个元素本身有可能是一个有限的集合）都会使用一个 UDF 进行处理（在Dataflow中叫做DoFn），输出是0或多个输出元素。这个例子是把键的前缀进行展开，然后把值复制到展开后的键构成新的键值对并输出。\nGroupByKey GroupByKey 用来按 Key 把元素重新分组。\nParDo 操作因为是对每个输入的元素进行处理，因此很自然地就可以适用于无边界的数据。而 GroupByKey 操作，在把数据发送到下游进行汇总前，需要收集到指定的键对应的所有数据。如果输入源是无边界的，那么我们不知道何时才能收集到所有的数据。所以通常的解决方案是对数据使用窗口操作。\n窗口 时间语义 窗口通常基于时间，时间对于窗口来说是必不可少的，在流式计算中，有 processing-time 和 event-time 两种时间语义，具体参考： 时间语义\n窗口分类 固定窗口（Fixed Window）固定区间（互不重叠）的窗口，可以基于时间，也可以基于数量；将事件分配到不同区间的窗口中，在通过窗口边界后，窗口内的所有事件会发送给计算函数进行计算；\n滑动窗口（Sliding Window）固定区间但可以重叠的窗口，需要指定窗口区间以及滑动步长，区间重叠意味着同一个事件会分配到不同窗口参与计算。 窗口区间决定何时触发计算，滑动步长决定何时创建一个新的窗口；\n会话窗口（Session Window）会话窗口通常基于用户的会话，通过定义会话的超时时间，将事件分割到不同的会话中； 例如，有个客服聊天系统，如果用户超过 30 分钟没有互动，则认为一次会话结束，当客户下次进入，就是一个新的会话了。\n窗口分配与合并 Dataflow 模型里，需要的不只是 GroupByKey，实际在统计数据的时候，往往需要的是 GroupByKeyAndWindow。统计一个不考虑任何时间窗口的数据，往往是没有意义的； Dataflow 模型提出：\n从模型简化的角度上，把所有的窗口策略都当做非对齐窗口，而底层实现来负责把对齐窗口作为一个特例进行优化。 窗口操作可以被分隔为两个互相相关的操作： set\u0026lt;Window\u0026gt; AssignWindows(T datum) 即窗口分配操作。这个操作把元素分配到 0 或多个窗口中去。 set\u0026lt;window\u0026gt; MergeWindows(Set\u0026lt;Window\u0026gt; windows) 即窗口合并操作，这个操作在汇总时合并窗口。 而在实际的逻辑实现层面，Dataflow 最重要的两个函数，也就是 AssignWindows 函数和 MergeWindows 函数。 窗口分配 每一个原始的事件，在业务处理函数之前，其实都是（key, value, event_time）这样一个三元组。而 AssignWindows 要做的，就是把这个三元组，根据我们的处理逻辑，变成（key, value, event_time, window）这样的四元组。"
    },{
      "title": "堆和堆排序",
      "url": "https://qiref.github.io/post/2023/05/12/%E5%A0%86%E5%92%8C%E5%A0%86%E6%8E%92%E5%BA%8F/",
      "content": "堆 堆的本质是树，用数组表示的完全二叉树；\n定义 一棵深度为k且有 2^k - 1 个结点的二叉树称为满二叉树。\n根据二叉树的性质2, 满二叉树每一层的结点个数都达到了最大值, 即满二叉树的第i层上有 2^(i-1) 个结点 (i≥1) 。\n如果对满二叉树的结点进行编号, 约定编号从根结点起, 自上而下, 自左而右。则深度为k的, 有n个结点的二叉树, 当且仅当其每一个结点都与深度为k的满二叉树中编号从1至n的结点一一对应时, 称之为完全二叉树。\n从满二叉树和完全二叉树的定义可以看出, 满二叉树是完全二叉树的特殊形态, 即如果一棵二叉树是满二叉树, 则它必定是完全二叉树。\n参考： https://baike.baidu.com/item/%E5%AE%8C%E5%85%A8%E4%BA%8C%E5%8F%89%E6%A0%91/7773232\n性质 arr：[2 3 4 52 2 2 1] idx： 0 1 2 3 4 5 6 i 下标和元素之间的映射关系：\n左子节点：2*i+1 右子节点：2*i+2 父节点：(i-1)/2 大根堆 完全二叉树里，每一个子树的最大值是根节点；\n小根堆 完全二叉树里，每一个子树的最小值是根节点；\n堆排序 定义堆 // maxHeap 定义一个大根堆 type maxHeap struct { Data []int Count int } func NewMaxHeap(size int) *maxHeap { return \u0026amp;maxHeap{ Data: make([]int, size), Count: 0, } } 插入数据 插入数据时，是往数组最后增加元素，由于需要保证大根堆的性质，如果新加入的元素比父节点大，则跟父节点交换位置，以此类推，一直到根节点，这个交换流程完成后，新元素插入就完成了。\n父节点下标跟当前下标index的关系：父节点 = (i-1)/2\n// insert 向堆中插入元素 func (heap *maxHeap) Insert(val int) { heap.Data[heap.Count] = val heap.Count++ // heap.shiftUp(heap.Count) heap.shiftUp1(heap.Count - 1) // 最后一个元素的下标 } // shiftUp 向堆中插入元素时，叶子节点可能需要向上移动 func (heap *maxHeap) shiftUp1(index int) { // 父节点 = (i-1)/2 for index \u0026gt; 0 \u0026amp;\u0026amp; heap.Data[index] \u0026gt; heap.Data[(index-1)/2] { heap.Data[index], heap.Data[(index-1)/2] = heap.Data[(index-1)/2], heap.Data[index] index = (index - 1) / 2 } fmt.Printf(\u0026quot;shiftUp heap %v \\n\u0026quot;, heap) } // shiftUp 向堆中插入元素时，叶子节点可能需要向上移动 func (heap *maxHeap) shiftUp(count int) { for count \u0026gt; 1 \u0026amp;\u0026amp; heap.Data[count-1] \u0026gt; heap.Data[(count/2)-1] { heap.Data[count-1], heap.Data[(count/2)-1] = heap.Data[(count/2)-1], heap.Data[count-1] count = (count / 2) } fmt.Printf(\u0026quot;shiftUp heap %v \\n\u0026quot;, heap) } 移除数据 移除数据，就是移除根节点，同时把最后一个元素放到根节点，然后跟子节点比较，此时，大根堆需要找到左右子节点中，较大的一个元素，跟父节点进行互换，以此类推，直到比较完最后一个节点，同时 count-1；\n// ExtraMax 提取大根堆的最大值，也就是根节点，大根堆整体性质不变 func (heap *maxHeap) ExtraMax() int { heap.Count-- result := heap.Data[0] heap.Data[0], heap.Data[heap.Count] = heap.Data[heap.Count], heap.Data[0] heap.shiftDown(0) return result } // shiftDown 移除堆中元素，节点向下移动 func (heap *maxHeap) shiftDown(index int) { for index \u0026lt;= heap.Count-1 { i := heap.getMaxChildNode(index) if i == -1 { break } if heap.Data[index] \u0026lt; heap.Data[i] { heap.Data[index], heap.Data[i] = heap.Data[i], heap.Data[index] } index = i } } // getMaxChildNode 获取最大子节点下标，如果没有子节点，return -1 func (heap *maxHeap) getMaxChildNode(index int) int { if index \u0026lt;= heap.Count-1 { if index*2+2 \u0026gt; heap.Count-1 { // 当右节点下标越界时 if index*2+1 \u0026lt;= heap.Count-1 { // 如果左下标没有越界，则返回左节点下标 return index*2 + 1 } } else { // 当右节点下标没有越界时，此时左下标一定没有越界 if heap.Data[index*2+1] \u0026gt;= heap.Data[index*2+2] { return index*2 + 1 } else { return index*2 + 2 } } } // 没有子节点时，返回-1 return -1 } 堆初始化 堆的内部实际是一个数组，数组可以通过下标表示为树形结构，堆初始化可以把数组转化为大根堆或者小根堆，这里需要借助完全二叉树的性质：\n最后一个非叶子节点是：(n-1)/2 (n 是数组大小)\n// MaxHeapipy 最大堆初始化 func MaxHeapipy(arr []int) *maxHeap { mh := NewMaxHeap(len(arr)) mh.Data = arr mh.Count = len(arr) // 堆是一棵完全二叉树，最后一个非叶子节点是：(n-1)/2，n 是数组大小 // 基于这个性质，可以依次对每一个非叶子节点进行 shiftDown，相当于每一个子树都完成 shiftDown， // 最后完成根节点的 shiftDown // shiftDown 之后，依然保持大根堆的性质 for i := (mh.Count - 1) / 2; i \u0026gt;= 0; i-- { mh.shiftDown(i) } return mh } 堆排序扩展 已知一个几乎有序的数组，几乎有序是指，如果把数组排好序的话，每个元素移动的距离不可以超过k，并且k相对于数组来说比较小。请选择一个合适的算法针对这个数据进行排序。\n假设k=6，准备一个小根堆，遍历数组，把前7个元素构建一个小根堆，则0位置是数组的最小值（因为排好序之后，每个元素移动距离不会超过k），把小根堆的根节点弹出，放到数组的0位置，然后依次把7个元素之后的数据插入到小根堆，弹出根节点的数据，弹出的数据一次有序。\n",
      "summary": "堆 堆的本质是树，用数组表示的完全二叉树；\n定义 一棵深度为k且有 2^k - 1 个结点的二叉树称为满二叉树。\n根据二叉树的性质2, 满二叉树每一层的结点个数都达到了最大值, 即满二叉树的第i层上有 2^(i-1) 个结点 (i≥1) 。\n如果对满二叉树的结点进行编号, 约定编号从根结点起, 自上而下, 自左而右。则深度为k的, 有n个结点的二叉树, 当且仅当其每一个结点都与深度为k的满二叉树中编号从1至n的结点一一对应时, 称之为完全二叉树。\n从满二叉树和完全二叉树的定义可以看出, 满二叉树是完全二叉树的特殊形态, 即如果一棵二叉树是满二叉树, 则它必定是完全二叉树。\n参考： https://baike.baidu.com/item/%E5%AE%8C%E5%85%A8%E4%BA%8C%E5%8F%89%E6%A0%91/7773232\n性质 arr：[2 3 4 52 2 2 1] idx： 0 1 2 3 4 5 6 i 下标和元素之间的映射关系：\n左子节点：2*i+1 右子节点：2*i+2 父节点：(i-1)/2 大根堆 完全二叉树里，每一个子树的最大值是根节点；\n小根堆 完全二叉树里，每一个子树的最小值是根节点；\n堆排序 定义堆 // maxHeap 定义一个大根堆 type maxHeap struct { Data []int Count int } func NewMaxHeap(size int) *maxHeap { return \u0026amp;maxHeap{ Data: make([]int, size), Count: 0, } } 插入数据 插入数据时，是往数组最后增加元素，由于需要保证大根堆的性质，如果新加入的元素比父节点大，则跟父节点交换位置，以此类推，一直到根节点，这个交换流程完成后，新元素插入就完成了。"
    },{
      "title": "Chandy-Lamport 算法笔记",
      "url": "https://qiref.github.io/post/2023/05/08/chandy-lamport-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/",
      "content": "前言 Global Snapshot（Global State）：全局快照，分布式系统在 Failure Recovery 的时候非常有用，也是广泛应用在分布式系统，更多是分布式计算系统中的一种容错处理理论基础。\n在 Chandy-Lamport 算法中，为了定义分布式系统的 Global Snapshot，先将分布式系统简化成有限个进程和进程之间的 channel 组成，也就是一个有向图 （GAG）：节点是进程，边是 channel。因为是分布式系统，也就是说，这些进程是运行在不同的物理机器上的。那么一个分布式系统的 Global Snapshot 就是有进程的状态和 channel 中的 message 组成，这个也是分布式快照算法需要记录的。因此，Chandy-Lamport 算法解决了分布式系统在 Failure Recovery 时，可以从 Global Snapshot 中恢复的问题；\n算法过程 前提条件及定义 process（Pn）：分布式系统中的进程，用 P1，P2，P3 表示； channel：分布式系统中，Pn 与 Pm 通信的管道，C12 表示从 P1 到 P2 的 channel，反之，C32 表示从 P3 到 P2的 channel； message：分布式系统中，Pn 与 Pm 之间发送的业务消息；M23 表示从 P2 到 P3 的 message； marker：在 Chandy-Lamport 算法中，Pn 与 Pm 之间发送的标记消息，不同于业务的 message，marker 是由 Chandy-Lamport 算法定义，用于帮助实现快照算法； snapshot/state：都表示快照，同时包括进程本身的状态和 message；下文中统一全局快照叫 snapshot，process 本地快照叫 state； Chandy-Lamport 算法有一些前提条件：\n进程之间的 message 是有序的，也即FIFO channel； 进程之间的 message 是可靠传递的； 算法步骤 Pn 中任意进程发起 snapshot，例如 P1 此时发起 snapshot； P1 首先保存本地的 state； P1 向 output channel 发送 marker 消息到其他进程（P2，P3）； P1 开始记录所有 input channel 的 message；？？？ P2 收到 marker 消息后； 如果 P2 还未记录本地的 state，也就是第一次收到 marker消息（例如收到 P1 marker 消息）； P2 开始记录本地 state； P2 将 input channel C12 置为空；？？？ P2 向 output channel 发送 marker 消息（P1，P3）； P2 记录除 C12 之外的所有 input channel 的 message； 如果 P2 已经完成本地 state 记录，不是第一次收到 marker消息（例如收到 P3 marker 消息）； 记录 input channel C32 在收到 marker 消息之前的 message； P1 P2 P3 收到 marker 消息并记录自己的 state 和 message；所有 state 都记录完成后，可以由某个服务器收集这些分散的快照，形成全局快照 （Global Snapshot），全局快照由每个进程的 state 和每个通道的 message 组成； 问题 为什么 P1 发起 snapshot 之后，要开始记录所有 input channel 的 message？\n要回答这个问题，首先要明确，每一次的 snapshot 是从什么时候结束的，当最后一个 P 本地的 state 全部完成之后，才算是一次 snapshot；所以，在一次 snapshot 发起之后，到最后一个 P 完成本地 state，进程之间的增量 message 也会记录并保存到 state 中；\n为什么 P2 开始记录本地 state 之后需要将 C12 置为空？\n回答这个问题，需要理解算法中，marker 这个消息有什么作用，其实 marker 是为了分割每一次的 snapshot！ 相当于是 message 之间的分隔符，当 P2 记录本地 state 之后，说明 P2 此时已经从 C12 中得到一个 marker 消息，从 P1 → P2 的消息默认都已经被 P2 接收到，并且处理完成（已经保存到 P2 的本地 state 中），换句话说，从 C12 中 marker 之后的消息，是下一次 snapshot 的 message；\n示例 背景说明，3 个进程 P1 P2 P3；每个进程在运行中会产生自身的 state 例如 P1（a,b）,每个进程之间还会产生message，例如 message (b-\u0026gt;f);\n在 Chandy-Lamport 算法中，可以由任意进程发起 snapshot；假设这里 P1 先发起 Global Snapshot；\nP1 先记录自身的 state(a,b)，然后向 P2 P3 发送 marker，最后记录 input channel C21 C31\n假设 P3 先接收到 marker 消息，此时是 P3 第一次收到 marker 消息，P3 先开始记录自身的 state(i)，然后将 C13 置为空，然后向 P1 和 P2 发送 marker 消息，最后，P3 记录 C23 的消息；\n当 P2 接收到 marker 消息之后，此时 P2 也是第一次收到 marker 消息，P2 开始记录自身的 state(f,g,h)，然后将 C32 置为空（marker 消息来自于 P3），然后向 P1 P3 发送 marker 消息，最后，P2 记录 C12 的消息；\n至此，所有的 marker 消息已经发出，剩余的过程就是处理非首次接受到 marker 消息的流程；\n当 P1 收到 P2 P3 的 marker 消息时，由于 P1 是 snapshot 的发起者，认为 P1 已经接受到 marker 消息，此时：\n在 time1 时刻，接收到 P3 的 marker 消息，只需要记录 C31 的消息，此时 C31 为空； 在 time2 时刻，接收到 P2 的 marker 消息，只需要记录 C21 的消息，此时 channel 中有个消息 message（h-\u0026gt;d）,因此，需要把 message（h-\u0026gt;d）记录到 snapshot 中，P1 的工作完成了; 与 P1 同理，当 P2 P3 再次接收到 marker 消息时，只需要记录 channel 的消息就行，由于 P2 P3 后续过程没有消息传输，这里不再赘述； 当所有进程处理完所有 marker 消息之后，一次 snapshot 流程就结束了；\n从结果上看，一次 snapshot 包括了 state(a,b,f,g,h,i) + message（h-\u0026gt;d） , 由于 g 在 state 中，并且 h-\u0026gt;d 是在一次 snapshot 中发生的，所以，h-\u0026gt;d 也应该包含在这次 snapshot 中，这也就是 Chandy-Lamport 算法的精妙之处！\n参考 分布式系统的全局快照 - Yang Blog\npaper_reading/Chandy-Lamport.md at main · legendtkl/paper_reading · GitHub\nChandy–Lamport algorithm - Wikipedia\n",
      "summary": "前言 Global Snapshot（Global State）：全局快照，分布式系统在 Failure Recovery 的时候非常有用，也是广泛应用在分布式系统，更多是分布式计算系统中的一种容错处理理论基础。\n在 Chandy-Lamport 算法中，为了定义分布式系统的 Global Snapshot，先将分布式系统简化成有限个进程和进程之间的 channel 组成，也就是一个有向图 （GAG）：节点是进程，边是 channel。因为是分布式系统，也就是说，这些进程是运行在不同的物理机器上的。那么一个分布式系统的 Global Snapshot 就是有进程的状态和 channel 中的 message 组成，这个也是分布式快照算法需要记录的。因此，Chandy-Lamport 算法解决了分布式系统在 Failure Recovery 时，可以从 Global Snapshot 中恢复的问题；\n算法过程 前提条件及定义 process（Pn）：分布式系统中的进程，用 P1，P2，P3 表示； channel：分布式系统中，Pn 与 Pm 通信的管道，C12 表示从 P1 到 P2 的 channel，反之，C32 表示从 P3 到 P2的 channel； message：分布式系统中，Pn 与 Pm 之间发送的业务消息；M23 表示从 P2 到 P3 的 message； marker：在 Chandy-Lamport 算法中，Pn 与 Pm 之间发送的标记消息，不同于业务的 message，marker 是由 Chandy-Lamport 算法定义，用于帮助实现快照算法； snapshot/state：都表示快照，同时包括进程本身的状态和 message；下文中统一全局快照叫 snapshot，process 本地快照叫 state； Chandy-Lamport 算法有一些前提条件："
    },{
      "title": "Go语言生产者消费者模型",
      "url": "https://qiref.github.io/post/2023/04/28/go%E8%AF%AD%E8%A8%80%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/",
      "content": " type Request struct { Name string } type XxHandler struct { RequestQueue chan *Request } func (o *XxHandler) String() string { if b, err := json.Marshal(o); err != nil { return \u0026quot;\u0026quot; } else { return string(b) } } // Start 启动队列监听 func (o *XxHandler) Start() { go func() { for request := range o.RequestQueue { go o.Process(request) } }() } // AppendTask 向队列中增加task func (o *XxHandler) AppendTask(request *Request) { if request == nil { return } o.RequestQueue \u0026lt;- request } // Stop 关闭队列 func (o *XxHandler) Stop() { close(o.RequestQueue) } func newIns() *XxHandler { return \u0026amp;XxHandler{ RequestQueue: make(chan *Request, 10), } } var ins *XxHandler // GetXxHandler 获取 实例 func GetXxHandler() *XxHandler { if ins == nil { ins = newIns() } return ins } // Process 处理队列中的任务 func (o *XxHandler) Process(request *Request) { // do something } func TestXxHandler(t *testing.T) { GetXxHandler().Start() GetXxHandler().AppendTask(\u0026amp;Request{Name: \u0026quot;xx\u0026quot;}) } ",
      "summary": "type Request struct { Name string } type XxHandler struct { RequestQueue chan *Request } func (o *XxHandler) String() string { if b, err := json.Marshal(o); err != nil { return \u0026quot;\u0026quot; } else { return string(b) } } // Start 启动队列监听 func (o *XxHandler) Start() { go func() { for request := range o.RequestQueue { go o.Process(request) } }() } // AppendTask 向队列中增加task func (o *XxHandler) AppendTask(request *Request) { if request == nil { return } o."
    },{
      "title": "Go语言实现 bitmap",
      "url": "https://qiref.github.io/post/2023/04/28/go%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0-bitmap/",
      "content": "算法说明 Bitmap算法是一种基于位运算的数据结构，用于解决大规模数据的快速查找和统计问题。其基本原理是将一个大数据集合映射到一个二进制向量中，其中每个元素对应于数据集合中的一个元素，向量中的每一位表示该元素是否存在于集合中。\n具体来说，Bitmap算法通过使用一个位图（bitmap）来表示一个数据集合，其中每个元素对应一个位。如果某个元素在数据集合中出现，则将其对应的位设置为1，否则将其对应的位设置为0。通过这种方式，可以快速地进行集合操作，如并集、交集和差集等。\nBitmap算法的主要优点在于其空间效率高，可以用较小的空间存储大规模数据集合。另外，Bitmap算法的时间复杂度也非常低，可以快速地进行集合操作。\n如何用数组表示一个 bitmap 以 1byte 为例：8位能表示8个元素， 0-7 号对应了 b[0] 下标， 8-15 号对应了 b[1] 下标，以此类推。\n因此，数组下标 n 跟bitmap元素序号 bitmapIdx 的关系为：n = bitmapIdx \u0026gt;\u0026gt; 3\n值如何映射到 bitmap 数组 当找到了 元素序号 n 在数组中的下标之后，如何给 b[n] 赋值呢？\n1 \u0026lt;\u0026lt; (bitmapIdx \u0026amp; 7) 等同于 1 \u0026lt;\u0026lt; (bitmapIdx % 8)\n(bitmapIdx % 8) 找到在了在数组 b[n] 中的第 m 位，然后 1 \u0026lt;\u0026lt; m 之后，就相当于给数组赋值，把第 m 位 置为1。\n验证 同样以 1byte 为例：借用上述结论，第 24 号元素，对应的数组下标 n 为：n = 24 \u0026gt;\u0026gt; 3 结果为3, b[3]；\n1 \u0026lt;\u0026lt; (24 % 8) = 1 ， 说明 24 号元素，在 b[3] 的第1位，b[3] = 1；\n当 25 号元素加入时，此时 n = 25 \u0026gt;\u0026gt; 3 结果为3，b[3]; 1 \u0026lt;\u0026lt; (25 % 8) = 2 , 说明 25 号元素，在 b[3] 的第2位，此时如何赋值呢，b[3] 已经赋值为 1 了; 通过 | 运算就能合并结果：b[3] = 1|2 = 3, 此时就把 24，25 两个元素放到 b[3] 中了；\n代码实现 package bitmap import ( \u0026quot;encoding/json\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;strconv\u0026quot; ) type BitMap struct { data []int64 } func NewBitMap(size int) *BitMap { return \u0026amp;BitMap{ data: make([]int64, size), } } func ParseFromJsonStr(str string) *BitMap { bitMap := NewBitMap(0) err := json.Unmarshal([]byte(str), \u0026amp;bitMap.data) if err != nil { _ = fmt.Errorf(\u0026quot;parse from json str return error %+v\u0026quot;, err) } return bitMap } func (b *BitMap) set(index uint) { dataIdx := index \u0026gt;\u0026gt; 5 // index/32 if dataIdx \u0026gt;= uint(len(b.data)) { b.data = append(b.data, 1\u0026lt;\u0026lt;(index\u0026amp;31)) // index\u0026amp;31 = index%32 } else { b.data[dataIdx] |= 1 \u0026lt;\u0026lt; (index \u0026amp; 31) } } func (b *BitMap) get(index uint) bool { dataIdx := index \u0026gt;\u0026gt; 5 if len(b.data) \u0026lt;= 0 || uint(len(b.data)) \u0026lt; dataIdx { return false } return b.data[dataIdx]\u0026amp;(1\u0026lt;\u0026lt;(index\u0026amp;31)) \u0026gt; 0 } func (b *BitMap) String() string { result := \u0026quot;\u0026quot; for _, v := range b.data { result += strconv.FormatInt(v, 10) + \u0026quot;(\u0026quot; + DecToBin(v) + \u0026quot;),\u0026quot; } return result } func DecToBin(n int64) string { result := \u0026quot;\u0026quot; if n == 0 { return \u0026quot;0\u0026quot; } for ; n \u0026gt; 0; n /= 2 { lsb := n % 2 result = strconv.FormatInt(lsb, 10) + result } return result } 测试结果 func TestNewBitmap(t *testing.T) { bitMap := NewBitMap(1) bitMap.set(1) bitMap.set(2) bitMap.set(3) bitMap.set(4) bitMap.set(0) fmt.Println(bitMap.String()) bitMap.set(32) bitMap.set(33) fmt.Println(bitMap.String()) bitMap.set(89) fmt.Println(bitMap.String()) } === RUN TestNewBitmap 31(11111), 31(11111),3(11), 31(11111),3(11),33554432(10000000000000000000000000), --- PASS: TestNewBitmap (0.00s) PASS ",
      "summary": "算法说明 Bitmap算法是一种基于位运算的数据结构，用于解决大规模数据的快速查找和统计问题。其基本原理是将一个大数据集合映射到一个二进制向量中，其中每个元素对应于数据集合中的一个元素，向量中的每一位表示该元素是否存在于集合中。\n具体来说，Bitmap算法通过使用一个位图（bitmap）来表示一个数据集合，其中每个元素对应一个位。如果某个元素在数据集合中出现，则将其对应的位设置为1，否则将其对应的位设置为0。通过这种方式，可以快速地进行集合操作，如并集、交集和差集等。\nBitmap算法的主要优点在于其空间效率高，可以用较小的空间存储大规模数据集合。另外，Bitmap算法的时间复杂度也非常低，可以快速地进行集合操作。\n如何用数组表示一个 bitmap 以 1byte 为例：8位能表示8个元素， 0-7 号对应了 b[0] 下标， 8-15 号对应了 b[1] 下标，以此类推。\n因此，数组下标 n 跟bitmap元素序号 bitmapIdx 的关系为：n = bitmapIdx \u0026gt;\u0026gt; 3\n值如何映射到 bitmap 数组 当找到了 元素序号 n 在数组中的下标之后，如何给 b[n] 赋值呢？\n1 \u0026lt;\u0026lt; (bitmapIdx \u0026amp; 7) 等同于 1 \u0026lt;\u0026lt; (bitmapIdx % 8)\n(bitmapIdx % 8) 找到在了在数组 b[n] 中的第 m 位，然后 1 \u0026lt;\u0026lt; m 之后，就相当于给数组赋值，把第 m 位 置为1。\n验证 同样以 1byte 为例：借用上述结论，第 24 号元素，对应的数组下标 n 为：n = 24 \u0026gt;\u0026gt; 3 结果为3, b[3]；"
    },{
      "title": "Go语言实现 LRU",
      "url": "https://qiref.github.io/post/2023/04/27/go%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0-lru/",
      "content": "LRU（Least Recently Used）算法，即最近最少使用算法;其基本思想是，如果一个数据最近被访问过，那么它在未来被访问的概率也会很高；反之，如果一个数据很久都没有被访问过，那么它在未来被访问的概率就相对较低。因此，LRU算法选择淘汰最近最少使用的数据，即选择最长时间没有被访问过的数据进行淘汰。\n具体来说，LRU算法通常使用一个双向链表和一个哈希表来实现。双向链表中的节点按照最近访问时间的顺序排列，最近访问的节点排在链表头部，最久未访问的节点排在链表尾部。哈希表中存储每个节点的地址，以便快速查找和删除。\n当需要访问一个数据时，LRU算法首先在哈希表中查找该数据，如果存在，则将对应的节点移动到链表头部；如果不存在，则将该数据添加到链表头部，并在哈希表中创建对应的节点。\n当需要淘汰数据时，LRU算法选择链表尾部的节点进行淘汰，并在哈希表中删除对应的节点。\ngolang 实现 LRU 算法：\npackage lru import ( \u0026quot;container/list\u0026quot; \u0026quot;errors\u0026quot; \u0026quot;sync\u0026quot; ) // LRU implements a non-thread safe fixed size LRU cache type LRU struct { size int evictList *list.List items map[interface{}]*list.Element } // entry is used to hold a value in the evictList type entry struct { key interface{} value interface{} } // NewLRU constructs an LRU of the given size func NewLRU(size int) (*LRU, error) { if size \u0026lt;= 0 { return nil, errors.New(\u0026quot;must provide a positive size\u0026quot;) } c := \u0026amp;LRU{ size: size, evictList: list.New(), items: make(map[interface{}]*list.Element), } return c, nil } // Add adds a value to the cache. Returns true if an eviction occured. func (c *LRU) Add(key, value interface{}) bool { // Check for existing item if ent, ok := c.items[key]; ok { c.evictList.MoveToFront(ent) ent.Value.(*entry).value = value return false } // Add new item ent := \u0026amp;entry{key, value} entry := c.evictList.PushFront(ent) c.items[key] = entry evict := c.evictList.Len() \u0026gt; c.size // Verify size not exceeded if evict { c.removeOldest() } return evict } // Get looks up a key's value from the cache. func (c *LRU) Get(key interface{}) (value interface{}, ok bool) { if ent, ok := c.items[key]; ok { c.evictList.MoveToFront(ent) return ent.Value.(*entry).value, true } return } // Remove removes the provided key from the cache, returning if the // key was contained. func (c *LRU) Remove(key interface{}) bool { if ent, ok := c.items[key]; ok { c.removeElement(ent) return true } return false } // removeOldest removes the oldest item from the cache. func (c *LRU) removeOldest() { ent := c.evictList.Back() if ent != nil { c.removeElement(ent) } } // removeElement is used to remove a given list element from the cache func (c *LRU) removeElement(e *list.Element) { c.evictList.Remove(e) kv := e.Value.(*entry) delete(c.items, kv.key) if c.onEvict != nil { c.onEvict(kv.key, kv.value) } } 需要注意的是这个 LRU 实现并不是线程安全的，如果需要线程安全，需要在外层方法加锁，同时，由于 golang 的 lock 并不是可重入的，需要注意避免死锁问题。 以下实现中基于 LRU 做了一次封装，实现了线程安全的内存级 LRU-Cache：\n完整代码： https://github.com/ArchieYao/leet-code-training/blob/main/src/lru/lru.go\n",
      "summary": "LRU（Least Recently Used）算法，即最近最少使用算法;其基本思想是，如果一个数据最近被访问过，那么它在未来被访问的概率也会很高；反之，如果一个数据很久都没有被访问过，那么它在未来被访问的概率就相对较低。因此，LRU算法选择淘汰最近最少使用的数据，即选择最长时间没有被访问过的数据进行淘汰。\n具体来说，LRU算法通常使用一个双向链表和一个哈希表来实现。双向链表中的节点按照最近访问时间的顺序排列，最近访问的节点排在链表头部，最久未访问的节点排在链表尾部。哈希表中存储每个节点的地址，以便快速查找和删除。\n当需要访问一个数据时，LRU算法首先在哈希表中查找该数据，如果存在，则将对应的节点移动到链表头部；如果不存在，则将该数据添加到链表头部，并在哈希表中创建对应的节点。\n当需要淘汰数据时，LRU算法选择链表尾部的节点进行淘汰，并在哈希表中删除对应的节点。\ngolang 实现 LRU 算法：\npackage lru import ( \u0026quot;container/list\u0026quot; \u0026quot;errors\u0026quot; \u0026quot;sync\u0026quot; ) // LRU implements a non-thread safe fixed size LRU cache type LRU struct { size int evictList *list.List items map[interface{}]*list.Element } // entry is used to hold a value in the evictList type entry struct { key interface{} value interface{} } // NewLRU constructs an LRU of the given size func NewLRU(size int) (*LRU, error) { if size \u0026lt;= 0 { return nil, errors."
    },{
      "title": "Flink内存模型",
      "url": "https://qiref.github.io/post/2023/03/28/flink%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/",
      "content": "Java 堆外内存 import sun.nio.ch.DirectBuffer; import java.nio.ByteBuffer; import java.util.concurrent.TimeUnit; public class OutHeapMem { public static void main(String[] args) throws Exception { // 分配 1G 直接内存 ByteBuffer byteBuffer = ByteBuffer.allocateDirect(1024 * 1024 * 1024); TimeUnit.SECONDS.sleep(30); System.out.println(\u0026quot;clean start\u0026quot;); // 清除直接内存 ((DirectBuffer) byteBuffer).cleaner().clean(); System.out.println(\u0026quot;clean finished\u0026quot;); TimeUnit.SECONDS.sleep(30); } } # 分配内存 Memory used total max usage heap 21M 165M 3641M 0.59% ps_eden_space 3M 64M 1344M 0.29% ps_survivor_space 0K 10752K 10752K 0.00% ps_old_gen 17M 91M 2731M 0.64% nonheap 28M 28M -1 96.89% code_cache 5M 5M 240M 2.11% metaspace 20M 21M -1 97.00% compressed_class_space 2M 2M 1024M 0.25% direct 1024M 1024M - 100.00% mapped 0K 0K - 0.00% # 释放内存 Memory used total max usage heap 21M 165M 3641M 0.60% ps_eden_space 4M 64M 1344M 0.32% ps_survivor_space 0K 10752K 10752K 0.00% ps_old_gen 17M 91M 2731M 0.64% nonheap 27M 28M -1 96.79% code_cache 5M 5M 240M 2.09% metaspace 20M 21M -1 97.03% compressed_class_space 2M 2M 1024M 0.25% direct 0K 0K - 0.00% mapped 0K 0K - 0.00% 通过 arthas 分析，分配直接内存会在 direct 开辟内存空间，表明是在堆外分配的内存空间；虽然 byteBuffer 指向了 direct memory，但是这个对象引用还在 heap 中，当 byteBuffer 对象引用 被 GC 算法回收掉之后，byteBuffer 指向的内存空间也会被释放；\nflink 内存模型 flink 内存模型大体上可以分为，Heap 内存 和 Off-Heap 内存；\nHeap 内存包括，Task 和 Framework 两部分，分别是框架需要的内存以及用户代码需要的内存； Off-Heap 内存包括三部分，Managed Memory 和 Direct Memory 以及 JVM 相关的消耗，JVM Metaspace 和 JVM Overhead 很好理解，用于 JVM 的开销，那为什么堆外内存又会分成 Managed memory 和 Direct memory 呢？是因为 Managed Memory 是由 Flink 管理的，而且这部分内存是在堆外；而另外的堆外内存并非由 Flink 管理，所以叫做 Direct Memory： Managed Memory 由 Flink 管理（MemoryManager）的内存，以 Flink 抽象的 MemorySegments 的形式分配给使用者； Direct Memory 为了区别于 Flink 管理的堆外内存， Direct Memory 分为 Framework Task Network 三个部分； Framework Off-Heap 用于 Flink 框架的堆外内存； Task Off-Heap 用于 Flink 的算子及其用户代码的堆外内存； task 之间网络传输的堆外内存； JVM 开销 JVM Metaspace Taskmanager JVM 的元空间内存； JVM Overhead 用于 JVM 垃圾回收的开销； JVM Heap 组成部分 配置参数 描述 框架堆内存（Framework Heap Memory） taskmanager.memory.framework.heap.size 用于 Flink 框架的 JVM 堆内存（进阶配置）。TaskExecutors 的框架堆内存大小。这是为 TaskExecutor 框架保留的 JVM 堆内存大小，不会分配给任务槽。 任务堆内存（Task Heap Memory） taskmanager.memory.task.heap.size 用于 Flink 应用的算子及用户代码的 JVM 堆内存。\n任务执行器的任务堆内存大小。这是为任务保留的 JVM 堆内存的大小。如果未指定，它的大小为：总 Flink 内存减去框架堆内存、框架堆外内存、任务堆外内存、托管内存和网络内存。 Off-JVM Memory Managed memory 组成部分 配置参数 描述 托管内存（Managed memory） taskmanager.memory.managed.size taskmanager.memory.managed.fraction 由 Flink 管理的流处理和批处理作业中用于排序、哈希表及缓存中间结果 、 流处理作业中用于 RocksDB State Backend、流处理和批处理作业中用于在 Python 进程中执行用户自定义函数。内存使用者可以从 MemoryManager 以 MemorySegments 的形式分配内存，所以名称叫 Managed Momory。 Direct Memory 组成部分 配置参数 描述 框架堆外内存（Framework Off-heap Memory） taskmanager.memory.framework.off-heap.size 这是为 Flink 框架保留的堆外内存的大小，不会分配给任务槽。 Flink 在计算 JVM max direct memory size 参数时，会把配置的值全部统计进去。 任务堆外内存（Task Off-heap Memory） taskmanager.memory.task.off-heap.size TaskExecutors 的任务堆外内存大小，用于 Flink 算子及用户代码的堆外内存。 Flink 在计算 JVM max direct memory size 参数时，会把配置的值全部统计进去。 网络内存（Network Memory） taskmanager.memory.network.min taskmanager.memory.network.max taskmanager.memory.network.fraction 用于task 之间数据传输的堆外内存（网络传输缓冲区） JVM Metaspace 、JVM Overhead 组成部分 配置参数 描述 JVM Metaspace taskmanager.memory.jvm-metaspace.size Flink JVM 进程的 Metaspace。从 JDK 8 开始，JVM 把永久代拿掉了。类的一些元数据放在叫做 Metaspace 的 Native Memory 中。在 Flink 中的 JVM Metaspace Memory 也一样，配置的是 Taskmanager JVM 的元空间内存大小。 JVM Overhead taskmanager.memory.jvm-overhead.min taskmanager.memory.jvm-overhead.max taskmanager.memory.jvm-overhead.fraction 用于 JVM 开销的本地内存，例如栈空间、垃圾回收空间等。Flink 计算 JVM max direct memory size 参数时不会计算在内。 参考： https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/memory/mem_setup_tm/\n",
      "summary": "Java 堆外内存 import sun.nio.ch.DirectBuffer; import java.nio.ByteBuffer; import java.util.concurrent.TimeUnit; public class OutHeapMem { public static void main(String[] args) throws Exception { // 分配 1G 直接内存 ByteBuffer byteBuffer = ByteBuffer.allocateDirect(1024 * 1024 * 1024); TimeUnit.SECONDS.sleep(30); System.out.println(\u0026quot;clean start\u0026quot;); // 清除直接内存 ((DirectBuffer) byteBuffer).cleaner().clean(); System.out.println(\u0026quot;clean finished\u0026quot;); TimeUnit.SECONDS.sleep(30); } } # 分配内存 Memory used total max usage heap 21M 165M 3641M 0.59% ps_eden_space 3M 64M 1344M 0.29% ps_survivor_space 0K 10752K 10752K 0.00% ps_old_gen 17M 91M 2731M 0."
    },{
      "title": "Flink类加载机制",
      "url": "https://qiref.github.io/post/2023/03/24/flink%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/",
      "content": "flink 类加载配置说明 Flink 作为基于 JVM 的框架，在 flink-conf.yaml 中提供了控制类加载策略的参数 classloader.resolve-order，可选项有 child-first（默认）和 parent-first。\nKey Default Type Description classloader.resolve-order \u0026ldquo;child-first\u0026rdquo; String Defines the class resolution strategy when loading classes from user code, meaning whether to first check the user code jar (\u0026ldquo;child-first\u0026rdquo;) or the application classpath (\u0026ldquo;parent-first\u0026rdquo;). The default settings indicate to load classes first from the user code jar, which means that user code jars can include and load different dependencies than Flink uses (transitively). classloader.parent-first-patterns.default \u0026ldquo;java.\u0026rdquo;;\n\u0026ldquo;scala.\u0026rdquo;;\n\u0026ldquo;org.apache.flink.\u0026rdquo;;\n\u0026ldquo;com.esotericsoftware.kryo\u0026rdquo;;\n\u0026ldquo;org.apache.hadoop.\u0026rdquo;;\n\u0026ldquo;javax.annotation.\u0026rdquo;;\n\u0026ldquo;org.xml\u0026rdquo;;\n\u0026ldquo;javax.xml\u0026rdquo;;\n\u0026ldquo;org.apache.xerces\u0026rdquo;;\n\u0026ldquo;org.w3c\u0026rdquo;;\n\u0026ldquo;org.rocksdb.\u0026rdquo;;\n\u0026ldquo;org.slf4j\u0026rdquo;;\n\u0026ldquo;org.apache.log4j\u0026rdquo;;\n\u0026ldquo;org.apache.logging\u0026rdquo;;\n\u0026ldquo;org.apache.commons.logging\u0026rdquo;;\n\u0026ldquo;ch.qos.logback\u0026rdquo; List A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. This setting should generally not be modified. To add another pattern we recommend to use \u0026ldquo;classloader.parent-first-patterns.additional\u0026rdquo; instead. classloader.parent-first-patterns.additional List A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. These patterns are appended to \u0026ldquo;classloader.parent-first-patterns.default\u0026rdquo;. classloader.fail-on-metaspace-oom-error true Boolean Fail Flink JVM processes if \u0026lsquo;OutOfMemoryError: Metaspace\u0026rsquo; is thrown while trying to load a user code class. classloader.check-leaked-classloader true Boolean Fails attempts at loading classes if the user classloader of a job is used after it has terminated. This is usually caused by the classloader being leaked by lingering threads or misbehaving libraries, which may also result in the classloader being used by other jobs. This check should only be disabled if such a leak prevents further jobs from running. parent-first 类加载策略 ParentFirstClassLoader 和 ChildFirstClassLoader 类的父类均为 FlinkUserCodeClassLoader 抽象类：\n/** * This class loader accepts a custom handler if an exception occurs in {@link #loadClass(String, * boolean)}. */ public abstract class FlinkUserCodeClassLoader extends URLClassLoader { public static final Consumer\u0026lt;Throwable\u0026gt; NOOP_EXCEPTION_HANDLER = classLoadingException -\u0026gt; {}; private final Consumer\u0026lt;Throwable\u0026gt; classLoadingExceptionHandler; protected FlinkUserCodeClassLoader(URL[] urls, ClassLoader parent) { this(urls, parent, NOOP_EXCEPTION_HANDLER); } protected FlinkUserCodeClassLoader( URL[] urls, ClassLoader parent, Consumer\u0026lt;Throwable\u0026gt; classLoadingExceptionHandler) { super(urls, parent); this.classLoadingExceptionHandler = classLoadingExceptionHandler; } @Override public final Class\u0026lt;?\u0026gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { try { synchronized (getClassLoadingLock(name)) { return loadClassWithoutExceptionHandling(name, resolve); } } catch (Throwable classLoadingException) { classLoadingExceptionHandler.accept(classLoadingException); throw classLoadingException; } } /** * Same as {@link #loadClass(String, boolean)} but without exception handling. * * \u0026lt;p\u0026gt;Extending concrete class loaders should implement this instead of {@link * #loadClass(String, boolean)}. */ protected Class\u0026lt;?\u0026gt; loadClassWithoutExceptionHandling(String name, boolean resolve) throws ClassNotFoundException { return super.loadClass(name, resolve); } } FlinkUserCodeClassLoader 继承自 URLClassLoader。因为 Flink App 的用户代码在运行期才能确定，所以通过 URL 在 JAR 包内寻找全限定名对应的类是比较合适的。而 ParentFirstClassLoader 仅仅是一个继承 FlinkUserCodeClassLoader 的空类而已。\n/** * Regular URLClassLoader that first loads from the parent and only after that from the URLs. */ public static class ParentFirstClassLoader extends FlinkUserCodeClassLoader { ParentFirstClassLoader( URL[] urls, ClassLoader parent, Consumer\u0026lt;Throwable\u0026gt; classLoadingExceptionHandler) { super(urls, parent, classLoadingExceptionHandler); } static { ClassLoader.registerAsParallelCapable(); } } 这样就相当于 ParentFirstClassLoader 直接调用了父加载器的 loadClass() 方法。之前已经讲过，JVM 中类加载器的层次关系和默认 loadClass() 方法的逻辑由双亲委派模型（parents delegation model）来体现：\n如果一个类加载器要加载一个类，它首先不会自己尝试加载这个类，而是把加载的请求委托给父加载器完成，所有的类加载请求最终都应该传递给最顶层的启动类加载器。只有当父加载器无法加载到这个类时，子加载器才会尝试自己加载。 双亲委派模型\n可见，Flink 的 parent-first 类加载策略就是照搬双亲委派模型的。也就是说，用户代码的类加载器是 Custom ClassLoader，Flink 框架本身的类加载器是 Application ClassLoader。用户代码中的类先由 Flink 框架的类加载器加载，再由用户代码的类加载器加载。但是，Flink 默认并不采用 parent-first 策略，而是采用 child-first 策略。\nchild-first 类加载策略 双亲委派模型的好处就是类加载器的层次关系保证了被加载类的层次关系，从而保证了 Java 运行环境的安全性。但是在 Flink App 这种依赖纷繁复杂的环境中，双亲委派模型可能并不适用。例如，程序中引入的 Flink-Cassandra Connector 总是依赖于固定的 Cassandra 版本，用户代码中为了兼容实际使用的 Cassandra 版本，会引入一个更低或更高的依赖。而同一个组件不同版本的类定义有可能会不同（即使类的全限定名是相同的），如果仍然用双亲委派模型，就会因为 Flink 框架指定版本的类先加载，而出现莫名其妙的兼容性问题，如NoSuchMethodError、IllegalAccessError 等。\n鉴于此，Flink 实现了 ChildFirstClassLoader 类加载器并作为默认策略。它打破了双亲委派模型，使得用户代码的类先加载，官方文档中将这个操作称为\u0026quot;Inverted Class Loading\u0026quot;。\npublic final class ChildFirstClassLoader extends FlinkUserCodeClassLoader { /** * The classes that should always go through the parent ClassLoader. This is relevant for Flink * classes, for example, to avoid loading Flink classes that cross the user-code/system-code * barrier in the user-code ClassLoader. */ private final String[] alwaysParentFirstPatterns; public ChildFirstClassLoader( URL[] urls, ClassLoader parent, String[] alwaysParentFirstPatterns, Consumer\u0026lt;Throwable\u0026gt; classLoadingExceptionHandler) { super(urls, parent, classLoadingExceptionHandler); this.alwaysParentFirstPatterns = alwaysParentFirstPatterns; } @Override protected Class\u0026lt;?\u0026gt; loadClassWithoutExceptionHandling(String name, boolean resolve) throws ClassNotFoundException { // First, check if the class has already been loaded Class\u0026lt;?\u0026gt; c = findLoadedClass(name); if (c == null) { // check whether the class should go parent-first for (String alwaysParentFirstPattern : alwaysParentFirstPatterns) { if (name.startsWith(alwaysParentFirstPattern)) { return super.loadClassWithoutExceptionHandling(name, resolve); } } try { // check the URLs c = findClass(name); } catch (ClassNotFoundException e) { // let URLClassLoader do it, which will eventually call the parent c = super.loadClassWithoutExceptionHandling(name, resolve); } } else if (resolve) { resolveClass(c); } return c; } 核心逻辑在 loadClassWithoutExceptionHandling() 方法中：\n调用 findLoadedClass() 方法检查全限定名 name 对应的类是否已经加载过，若没有加载过，再继续往下执行。 检查要加载的类是否以 alwaysParentFirstPatterns 集合中的前缀开头。如果是，则调用父类的对应方法，以 parent-first 的方式来加载它。alwaysParentFirstPatterns 集合中的这些类都是 Java、Flink 等组件的基础，不能被用户代码覆盖。它由以下两个参数来指定： classloader.parent-first-patterns.default，不建议修改，固定为以下这些值：\njava.; scala.; org.apache.flink.; com.esotericsoftware.kryo; org.apache.hadoop.; javax.annotation.; org.slf4j; org.apache.log4j; org.apache.logging; org.apache.commons.logging; ch.qos.logback; org.xml; javax.xml; org.apache.xerces; org.w3c classloader.parent-first-patterns.additional：除了上一个参数指定的类之外，用户如果有其他类以 child-first 模式会发生冲突，而希望以双亲委派模型来加载的话，可以额外指定（分号分隔）。\n如果类不符合 alwaysParentFirstPatterns 集合的条件，就调用 findClass() 方法在用户代码中查找并获取该类的定义（该方法在 URLClassLoader 中有默认实现）。如果找不到，再 fallback 到父加载器来加载。 最后，若 resolve 参数为 true，就调用 resolveClass() 方法链接该类，最后返回对应的 Class 对象。 参考：\n双亲委派模型与 Flink 的类加载策略-阿里云开发者社区 (aliyun.com)\n调试类加载 | Apache Flink\n",
      "summary": "flink 类加载配置说明 Flink 作为基于 JVM 的框架，在 flink-conf.yaml 中提供了控制类加载策略的参数 classloader.resolve-order，可选项有 child-first（默认）和 parent-first。\nKey Default Type Description classloader.resolve-order \u0026ldquo;child-first\u0026rdquo; String Defines the class resolution strategy when loading classes from user code, meaning whether to first check the user code jar (\u0026ldquo;child-first\u0026rdquo;) or the application classpath (\u0026ldquo;parent-first\u0026rdquo;). The default settings indicate to load classes first from the user code jar, which means that user code jars can include and load different dependencies than Flink uses (transitively)."
    },{
      "title": "Java双亲委派",
      "url": "https://qiref.github.io/post/2023/03/24/java%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE/",
      "content": "类加载器 Java语言系统中支持以下4种类加载器：\nBootstrap ClassLoader 启动类加载器，主要负责加载Java核心类库，%JRE_HOME%\\lib下的rt.jar、resources.jar、charsets.jar和class等； Extension ClassLoader 标准扩展类加载器，主要负责加载目录%JRE_HOME%\\lib\\ext目录下的jar包和class文件； Application ClassLoader 应用类加载器，主要负责加载当前应用的classpath下的所有类； User ClassLoader 用户自定义类加载器，用户自定义的类加载器,可加载指定路径的class文件； 双亲委派 类加载器采用了双亲委派模式，其工作原理是，如果一个类加载器收到了类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行，如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将到达顶层的启动类加载器，如果父类加载器可以完成类加载任务，就成功返回，倘若父类加载器无法完成此加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式。\n双亲委派模式的好处是什么？\nJava 类随着它的类加载器一起具备了一种带有优先级的层次关系，通过这种层次关系可以避免类的重复加载，当父类加载器已经加载过一次时，没有必要子类再去加载一次。 考虑到安全因素，Java 核心 Api 类不会被随意替换，核心类永远是被上层的类加载器加载。如果我们自己定义了一个 java.lang.String 类，它会优先委派给 BootStrapClassLoader 去加载，加载完了就直接返回了。 如果我们定义了一个 java.lang.ExtString，能被加载吗？答案也是不能的，因为 java.lang 包是有权限控制的，自定义了这个包，会报一个错如下：\njava.lang.SecurityException: Prohibited package name: java.lang 源码分析 protected Class\u0026lt;?\u0026gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // 首先，检查这个类是否已经被加载了，最终实现是一个 native 本地实现 Class\u0026lt;?\u0026gt; c = findLoadedClass(name); // 如果还没有被加载，则开始加载 if (c == null) { long t0 = System.nanoTime(); try { // 首先如果父加载器不为空，则使用父类加载器加载。Launcher 类里提到的 parent 就在这里使用的。 if (parent != null) { c = parent.loadClass(name, false); } else { // 如果父加载器为空（比如 ExtClassLoader），就使用 BootStrapClassloader 来加载 c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { } // 如果还没有找到，则使用 findClass 类来加载。也就是说如果我们自定义类加载器，就重写这个方法 if (c == null) { long t1 = System.nanoTime(); c = findClass(name); sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; } } 如何主动破坏双亲委派机制 双亲委派过程都是在loadClass方法中实现的，那么想要破坏这种机制，那么就自定义一个类加载器，重写其中的loadClass方法，使其不进行双亲委派即可。\nloadClass（）、findClass（）、defineClass（）区别\nClassLoader中和类加载有关的方法有很多，前面提到了loadClass，除此之外，还有findClass和defineClass等，那么这几个方法有什么区别呢？\nloadClass() 就是主要进行类加载的方法，默认的双亲委派机制就实现在这个方法中 findClass() 根据名称或位置加载.class字节码 definclass() 把字节码转化为Class 这里面需要展开讲一下loadClass和findClass，我们前面说过，当我们想要自定义一个类加载器的时候，并且像破坏双亲委派原则时，我们会重写loadClass方法。\n自定义类加载器，并遵循双亲委派 如果你想定义一个自己的类加载器，并且要遵守双亲委派模型，那么可以继承ClassLoader，并且在findClass中实现你自己的加载逻辑即可。\n",
      "summary": "类加载器 Java语言系统中支持以下4种类加载器：\nBootstrap ClassLoader 启动类加载器，主要负责加载Java核心类库，%JRE_HOME%\\lib下的rt.jar、resources.jar、charsets.jar和class等； Extension ClassLoader 标准扩展类加载器，主要负责加载目录%JRE_HOME%\\lib\\ext目录下的jar包和class文件； Application ClassLoader 应用类加载器，主要负责加载当前应用的classpath下的所有类； User ClassLoader 用户自定义类加载器，用户自定义的类加载器,可加载指定路径的class文件； 双亲委派 类加载器采用了双亲委派模式，其工作原理是，如果一个类加载器收到了类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行，如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将到达顶层的启动类加载器，如果父类加载器可以完成类加载任务，就成功返回，倘若父类加载器无法完成此加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式。\n双亲委派模式的好处是什么？\nJava 类随着它的类加载器一起具备了一种带有优先级的层次关系，通过这种层次关系可以避免类的重复加载，当父类加载器已经加载过一次时，没有必要子类再去加载一次。 考虑到安全因素，Java 核心 Api 类不会被随意替换，核心类永远是被上层的类加载器加载。如果我们自己定义了一个 java.lang.String 类，它会优先委派给 BootStrapClassLoader 去加载，加载完了就直接返回了。 如果我们定义了一个 java.lang.ExtString，能被加载吗？答案也是不能的，因为 java.lang 包是有权限控制的，自定义了这个包，会报一个错如下：\njava.lang.SecurityException: Prohibited package name: java.lang 源码分析 protected Class\u0026lt;?\u0026gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // 首先，检查这个类是否已经被加载了，最终实现是一个 native 本地实现 Class\u0026lt;?\u0026gt; c = findLoadedClass(name); // 如果还没有被加载，则开始加载 if (c == null) { long t0 = System.nanoTime(); try { // 首先如果父加载器不为空，则使用父类加载器加载。Launcher 类里提到的 parent 就在这里使用的。 if (parent !"
    },{
      "title": "Go语言单例模式",
      "url": "https://qiref.github.io/post/2023/03/24/go%E8%AF%AD%E8%A8%80%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/",
      "content": "错误示例 type singleton struct {} var instance *singleton func GetInstance() *singleton { if instance == nil { instance = \u0026amp;singleton{} // 不是并发安全的 } return instance } 不优雅示例 func GetInstance() *singleton { mu.Lock() // 如果实例存在没有必要加锁 defer mu.Unlock() if instance == nil { instance = \u0026amp;singleton{} } return instance } 加锁可以保证每次拿到相同实例，但是如果已经实例化，再调用函数，依然有锁存在，浪费性能，不够优雅；\n优雅示例 import ( \u0026quot;sync\u0026quot; ) type singleton struct {} var instance *singleton var once sync.Once func GetInstance() *singleton { once.Do(func() { instance = \u0026amp;singleton{} }) return instance } 该实现利用sync.Once类型去同步对GetInstance()的访问，并确保我们的类型仅被初始化一次。\n",
      "summary": "错误示例 type singleton struct {} var instance *singleton func GetInstance() *singleton { if instance == nil { instance = \u0026amp;singleton{} // 不是并发安全的 } return instance } 不优雅示例 func GetInstance() *singleton { mu.Lock() // 如果实例存在没有必要加锁 defer mu.Unlock() if instance == nil { instance = \u0026amp;singleton{} } return instance } 加锁可以保证每次拿到相同实例，但是如果已经实例化，再调用函数，依然有锁存在，浪费性能，不够优雅；\n优雅示例 import ( \u0026quot;sync\u0026quot; ) type singleton struct {} var instance *singleton var once sync.Once func GetInstance() *singleton { once.Do(func() { instance = \u0026amp;singleton{} }) return instance } 该实现利用sync."
    },{
      "title": "Flink反压",
      "url": "https://qiref.github.io/post/2023/03/23/flink%E5%8F%8D%E5%8E%8B/",
      "content": "什么是反压 如果你看到一个 Task 发生 反压警告（例如： High），意味着它生产数据的速率比下游 Task 消费数据的速率要快。 在工作流中数据记录是从上游向下游流动的（例如：从 Source 到 Sink）。反压沿着相反的方向传播，沿着数据流向上游传播。\n以一个简单的 Source -\u0026gt; Sink Job 为例。如果看到 Source 发生了警告，意味着 Sink 消费数据的速率比 Source 生产数据的速率要慢。 Sink 正在向上游的 Source 算子产生反压。\nTask（SubTask）的每个并行实例都可以用三个一组的指标评价：\nbackPressureTimeMsPerSecond，subtask 被反压的时间 idleTimeMsPerSecond，subtask 等待某类处理的时间 busyTimeMsPerSecond，subtask 实际工作时间 在任何时间点，这三个指标相加都约等于1000ms。 指标值说明：\nOK: 0 \u0026lt;= 比例 \u0026lt;= 0.10 LOW: 0.10 \u0026lt; 比例 \u0026lt;= 0.5 HIGH: 0.5 \u0026lt; 比例 \u0026lt;= 1 反压问题定位 可以看各个operator的metrics的指标，比如：buffers.outPoolUsage、buffers.inPoolUsage、buffers.inputFloatingBuffersUsage、buffers.inputExclusiveBuffersUsage；\n接收端共用一个LocalBufferPool，接收端每个Channel在初始化阶段都会分配固定数量的Buffer(Exclusive Buffer)。如果某一时刻接收端接受到的数量太多，Exclusive Buffer就会耗尽，此时就会向BufferPool申请剩余的Floating Buffer（除了Exclusive Buffer，其他的都是Floating Buffer,备用Buffer）；inPoolUsage = floatingBuffersUsage + exclusiveBuffersUsage\n若 inPoolUsage 低，而 outPoolUsage 低，则说明完全没有背压现象。 若 inPoolUsage 低，而 outPoolUsage 高，则说明处于临时状态，可能是背压刚开始，也可能是刚结束，需要再观察。 若 inPoolUsage 高，而 outPoolUsage 低，那么通常情况下这个算子就是背压的根源了。 若 inPoolUsage 高，而 outPoolUsage 高，则说明这个算子是被其他下游算子反压而来的，并不是元凶。 在反压定位过程中，建议关闭 Operator Chaining 优化，这样所有的算子可以单独拆分出来，不会相互干扰：\npipeline.operator-chaining: false 参考 监控反压 | Apache Flink\n",
      "summary": "什么是反压 如果你看到一个 Task 发生 反压警告（例如： High），意味着它生产数据的速率比下游 Task 消费数据的速率要快。 在工作流中数据记录是从上游向下游流动的（例如：从 Source 到 Sink）。反压沿着相反的方向传播，沿着数据流向上游传播。\n以一个简单的 Source -\u0026gt; Sink Job 为例。如果看到 Source 发生了警告，意味着 Sink 消费数据的速率比 Source 生产数据的速率要慢。 Sink 正在向上游的 Source 算子产生反压。\nTask（SubTask）的每个并行实例都可以用三个一组的指标评价：\nbackPressureTimeMsPerSecond，subtask 被反压的时间 idleTimeMsPerSecond，subtask 等待某类处理的时间 busyTimeMsPerSecond，subtask 实际工作时间 在任何时间点，这三个指标相加都约等于1000ms。 指标值说明：\nOK: 0 \u0026lt;= 比例 \u0026lt;= 0.10 LOW: 0.10 \u0026lt; 比例 \u0026lt;= 0.5 HIGH: 0.5 \u0026lt; 比例 \u0026lt;= 1 反压问题定位 可以看各个operator的metrics的指标，比如：buffers.outPoolUsage、buffers.inPoolUsage、buffers.inputFloatingBuffersUsage、buffers.inputExclusiveBuffersUsage；\n接收端共用一个LocalBufferPool，接收端每个Channel在初始化阶段都会分配固定数量的Buffer(Exclusive Buffer)。如果某一时刻接收端接受到的数量太多，Exclusive Buffer就会耗尽，此时就会向BufferPool申请剩余的Floating Buffer（除了Exclusive Buffer，其他的都是Floating Buffer,备用Buffer）；inPoolUsage = floatingBuffersUsage + exclusiveBuffersUsage\n若 inPoolUsage 低，而 outPoolUsage 低，则说明完全没有背压现象。 若 inPoolUsage 低，而 outPoolUsage 高，则说明处于临时状态，可能是背压刚开始，也可能是刚结束，需要再观察。 若 inPoolUsage 高，而 outPoolUsage 低，那么通常情况下这个算子就是背压的根源了。 若 inPoolUsage 高，而 outPoolUsage 高，则说明这个算子是被其他下游算子反压而来的，并不是元凶。 在反压定位过程中，建议关闭 Operator Chaining 优化，这样所有的算子可以单独拆分出来，不会相互干扰："
    },{
      "title": "Arthas火焰图使用",
      "url": "https://qiref.github.io/post/2023/03/22/arthas%E7%81%AB%E7%84%B0%E5%9B%BE%E4%BD%BF%E7%94%A8/",
      "content": "arthas 火焰图相关命令 arthas 中 profiler 命令支持生成应用热点的火焰图。本质上是通过不断的采样，然后把收集到的采样结果生成火焰图。\n启动arthas：\njava -jar arthas-boot.jar 开始收集火焰图：\n[arthas@1]$ profiler start Profiling started [arthas@1]$ profiler status Profiling is running for 6 seconds [arthas@1]$ profiler status Profiling is running for 27 seconds [arthas@1]$ profiler getSamples 2 [arthas@1]$ profiler getSamples 4 [arthas@1]$ profiler stop --file /tmp/cpu-result-1.html OK profiler output file: /tmp/cpu-result-1.html 命令说明：\nprofiler start 开启火焰图收集； profiler status 查看火焰图收集的状态，会显示当前已经采集多长时间； profiler getSamples 获取已采集的 sample 的数量，理论上，sample 越多，结果越准确； profiler stop --file /tmp/cpu-result-1.html 停止当前火焰图收集，会输出到文件中去，生成的文件就是火焰图； 关键参数说明 event -e, \u0026ndash;event 默认采集 CPU 信息，可设：cpu, alloc, lock, cache-misses etc\n如果想采集内存信息，可以用以下命令：\nprofiler start --event alloc file -f, --file \u0026lt;value\u0026gt; 输出的文件，主要支持 html, jfr 两种格式；\nprofiler stop --file /tmp/cpu-result-1.html duration -d, --duration \u0026lt;value\u0026gt; 指定结束时间；\nprofiler 执行 30 秒自动结束：\nprofiler start --duration 30 火焰图分析方法 arthas 火焰图颜色说明：\n绿色： java 代码 黄色： jvm c++ 代码 红色： 用户态 c 代码 橙色： 内核态 c 代码 这里分析java进程，主要看绿色部分；\n纵轴表示函数调用栈，每一层是一个函数；\n横轴表示抽样数，每一格的宽度越宽，表示它被抽样到的次数越多，侧面反应执行时间越长；火焰图的每一格，鼠标悬浮时会显示完整的函数名、占据总抽样次数的百分比，以此来对比不同函数的耗时差别；\nCPU 火焰图，能找出cpu占用高的问题函数，分析代码热路径； 内存火焰图，能找到内存占用高的对象，申请内存多的函数； 参考：\nhttps://arthas.aliyun.com/doc/profiler.html\n",
      "summary": "arthas 火焰图相关命令 arthas 中 profiler 命令支持生成应用热点的火焰图。本质上是通过不断的采样，然后把收集到的采样结果生成火焰图。\n启动arthas：\njava -jar arthas-boot.jar 开始收集火焰图：\n[arthas@1]$ profiler start Profiling started [arthas@1]$ profiler status Profiling is running for 6 seconds [arthas@1]$ profiler status Profiling is running for 27 seconds [arthas@1]$ profiler getSamples 2 [arthas@1]$ profiler getSamples 4 [arthas@1]$ profiler stop --file /tmp/cpu-result-1.html OK profiler output file: /tmp/cpu-result-1.html 命令说明：\nprofiler start 开启火焰图收集； profiler status 查看火焰图收集的状态，会显示当前已经采集多长时间； profiler getSamples 获取已采集的 sample 的数量，理论上，sample 越多，结果越准确； profiler stop --file /tmp/cpu-result-1.html 停止当前火焰图收集，会输出到文件中去，生成的文件就是火焰图； 关键参数说明 event -e, \u0026ndash;event 默认采集 CPU 信息，可设：cpu, alloc, lock, cache-misses etc"
    },{
      "title": "Flink Append流、Retract流、Upsert流",
      "url": "https://qiref.github.io/post/2022/03/13/flink-append%E6%B5%81retract%E6%B5%81upsert%E6%B5%81/",
      "content": "摘要： 介绍 Flink 中 Append流、Retract流、Upsert流的含义。\nAppend流 Retract流 Upsert流 Append流 在 Append 流中，仅通过 INSERT 操作修改的动态表，可以通过输出插入的行转换为流。\nRetract流 retract 流包含两种类型的 message： add messages 和 retract messages 。\n通过将INSERT 操作编码为 add message、将 DELETE 操作编码为 retract message、将 UPDATE 操作编码为更新(先前)行的 retract message 和更新(新)行的 add message，将动态表转换为 retract 流。\nOPERATOR ENCODE insert add update retract -\u0026gt; add delete retract Upsert流 upsert 流包含两种类型的 message： upsert messages 和delete messages。\n转换为 upsert 流的动态表需要(可能是组合的)唯一键。通过将 INSERT 和 UPDATE 操作编码为 upsert message，将 DELETE 操作编码为 delete message ，将具有唯一键的动态表转换为流。消费流的算子需要知道唯一键的属性，以便正确地应用 message。与 retract 流的主要区别在于 UPDATE 操作是用单个 message 编码的，因此效率更高。\nOPERATOR ENCODE insert upsert update upsert delete delete 将动态表转为datastream时，仅支持append 流与retract流。\n将动态表输出到外部系统时，支持Append、Retract以及Upsert模式\nhttps://nightlies.apache.org/flink/flink-docs-release-1.12/zh/dev/table/streaming/dynamic_tables.html\n",
      "summary": "摘要： 介绍 Flink 中 Append流、Retract流、Upsert流的含义。\nAppend流 Retract流 Upsert流 Append流 在 Append 流中，仅通过 INSERT 操作修改的动态表，可以通过输出插入的行转换为流。\nRetract流 retract 流包含两种类型的 message： add messages 和 retract messages 。\n通过将INSERT 操作编码为 add message、将 DELETE 操作编码为 retract message、将 UPDATE 操作编码为更新(先前)行的 retract message 和更新(新)行的 add message，将动态表转换为 retract 流。\nOPERATOR ENCODE insert add update retract -\u0026gt; add delete retract Upsert流 upsert 流包含两种类型的 message： upsert messages 和delete messages。\n转换为 upsert 流的动态表需要(可能是组合的)唯一键。通过将 INSERT 和 UPDATE 操作编码为 upsert message，将 DELETE 操作编码为 delete message ，将具有唯一键的动态表转换为流。消费流的算子需要知道唯一键的属性，以便正确地应用 message。与 retract 流的主要区别在于 UPDATE 操作是用单个 message 编码的，因此效率更高。"
    },{
      "title": "Flink Checkpoint机制",
      "url": "https://qiref.github.io/post/2022/03/04/flink-checkpoint%E6%9C%BA%E5%88%B6/",
      "content": "摘要： 如果把运行中的 Flink 程序比做一条河流，Checkpoint 就是一个相机，定期地对河流进行拍照，记录河水的状态。本文以自顶向下的视角，从理论到实现，分析 Flink 中的 Checkpoint 机制；\n理论基础 asynchronous barrier snapshotting 算法步骤 算法在 Flink 中的实现 Flink Checkpoint 整体流程 Flink Checkpoint Barrier Alignment Flink Checkpoint 使用 Flink Job 重启策略 Flink Job 开启 Checkpoint 理论基础 asynchronous barrier snapshotting Flink Checkpoint 机制是异步屏障快照（asynchronous barrier snapshotting, ABS）算法的一种实现，而 ABS 算法基于 Chandy-Lamport 的变种，但数据模型是还是基于 Chandy-Lamport；\n在 flink 中，作业算子被抽象为 DAG，节点为 operator，边是每一个 operator 的 stream（channel），与 Chandy-Lamport 的数据模型正好吻合；\nABS 算法把 Chandy-Lamport 中的 marker 消息换成了 barrier，作用是一致的，都是切分 snapshot；\nABS 算法 中 asynchronous 是异步的意思，当算子收齐 barrier 并触发快照之后，不会等待快照数据全部写入状态后端，而是一边后台写入，一边立刻继续处理数据流，并将 barrier 发送到下游，实现了最小化延迟。当然，引入异步性之后，所有有状态的算子都需要上报 ack，否则 JobManager 就无法确认一次 snapshot 是否完成。\n算法步骤 Source算子接收到JobManager产生的屏障，生成自己状态的快照（其中包含数据源对应的offset/position信息），并将屏障广播给下游所有数据流； 下游非 Source 的算子从它的某个输入数据流接收到屏障后，会阻塞这个输入流，继续接收其他输入流，直到所有输入流的屏障都到达。一旦算子收齐了所有屏障，它就会生成自己状态的快照，并继续将屏障广播给下游所有数据流； Sink算子接收到屏障之后会向 JobManager 确认，所有Sink都确认收到屏障标记着这一周期checkpoint过程结束，快照成功。 如果算子只有一个输入流的话，问题就比较简单，只需要在收到屏障之后立即做快照。但是如果有多个输入流，就必须要等待收到所有屏障才能做快照，以避免将检查点 n 与检查点 n + 1 的数据混淆。这个等待的过程就叫做对齐（alignment），见下图；\n算法在 Flink 中的实现 在 Flink 的 stream 中，每一次的 Checkpoint 被 barrier 分割：\n当算子接收到不止一个 steam 时，barrier 到达算子的顺序会不一致，此时，算子会停止处理新的数据，等到剩余的 barrier 到达算子后，才开始进行 Checkpoint，这就是 Barrier Alignment 。\nFlink Checkpoint 整体流程 Chekcpoint 是由 jobmanager 中的 CheckpointCoordinator 发起的，CheckpointCoordinator 是一个类，Flink 中具体描述如下：\n// The checkpoint coordinator coordinates the distributed snapshots of operators and state. // It triggers the checkpoint by sending the messages to the relevant tasks and // collects the checkpoint acknowledgements. CheckpointCoordinator 会调度task 进行 checkpoint，并接收来自 tasks 的 ack；\n用于保存 Checkpoint state 和 meta 的容器称为 state backend，Flink 中自带几种 state backend：\nMemoryStateBackend 内存型（默认） FsStateBackend 文件系统，本地、hdfs、cos RocksDBStateBackend 数据库RocksDB，存储在TaskManager的data目录下，也可以同步到远程 Flink Checkpoint Barrier Alignment 基于 Barrier Alignment，Checkpoint 产生两种模式：EXACTLY_ONCE AT_LEAST_ONCE ；\n当 Barrier Alignment 时，先到来的数据进行 buffer，就是 EXACTLY_ONCE，当先到来的数据先进行处理时，就是 AT_LEAST_ONCE 。\n从Flink 1.11开始，Checkpoint 可以是非对齐的。 Unaligned checkpoints 包含 In-flight 数据(例如，存储在缓冲区中的数据)作为 Checkpoint State的一部分，允许 Checkpoint Barrier 跨越这些缓冲区。因此，Checkpoint 时长变得与当前吞吐量无关，因为 Checkpoint Barrier 实际上已经不再嵌入到数据流当中。 使用 Unaligned checkpoints 也会带来新的代价，State 中会保存数据，这样会增加 State 的负担；\n如果到算子背压是由于 Checkpoint 周期过长（Barrier Alignment）时，此时建议使用 Unaligned checkpoint；\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 启用非对齐 Checkpoint env.getCheckpointConfig().enableUnalignedCheckpoints(); Flink Checkpoint 使用 Flink Job 重启策略 在作业运行时，经常会由于各种不确定因素导致作业停止，网络，磁盘，外部依赖等等，Flink 内部提供多种作业重启的策略来减少运维成本，Flink 程序设置重启策略后，当作业停止时，Flink 可以重新拉起作业，常用的重启策略如下：\nStreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment(); // 默认重启策略，没有重启策略 executionEnvironment.setRestartStrategy(RestartStrategies.noRestart()); // 采用集群的重启策略，如果没有定义其他重启策略，默认选择固定延时重启策略。 executionEnvironment.setRestartStrategy(RestartStrategies.fallBackRestart()); // 采用集群的重启策略，如果没有定义其他重启策略，默认选择固定延时重启策略。 executionEnvironment.setRestartStrategy(RestartStrategies.fallBackRestart()); // 如果20 S 内，有1次错误，则终止作业，不满足则会重启作业，重启时间间隔为 5 S executionEnvironment.setRestartStrategy( RestartStrategies.failureRateRestart(1, Time.seconds(20), Time.seconds(5))); source 定义了一个 Tuple3 的数据源，index 一直自增，在 map 过程中，遇到 100 时抛出一个异常； 作业运行的结果是，当 index=100 时，作业失败，然后重启，重新开始运行：\npublic class MapFunc { public static MapFunction\u0026lt;Tuple3\u0026lt;String, Integer, Long\u0026gt;, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; getMapFunc( Logger logger) { return new MapFunction\u0026lt;Tuple3\u0026lt;String, Integer, Long\u0026gt;, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;() { @Override public Tuple2\u0026lt;String, Integer\u0026gt; map(Tuple3\u0026lt;String, Integer, Long\u0026gt; event) throws Exception { if (event.f1 % 100 == 0) { logger.error(\u0026quot;event.f1 more than 100. value : \u0026quot; + event.f1); throw new RuntimeException(\u0026quot;event.f1 more than 100.\u0026quot;); } return new Tuple2\u0026lt;\u0026gt;(event.f0, event.f1); } }; } } // 运行作业 public class FixedDelayRestartJob { public static void main(String[] args) throws Exception { Logger logger = LoggerFactory.getLogger(FixedDelayRestartJob.class); StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment(); executionEnvironment.setParallelism(1); // 重启尝试次数 2，每次重启间隔 5 S executionEnvironment.setRestartStrategy( RestartStrategies.fixedDelayRestart(2, Time.seconds(5))); // source DataStreamSource\u0026lt;Tuple3\u0026lt;String, Integer, Long\u0026gt;\u0026gt; source = executionEnvironment.addSource(SourceFunc.getSourceFunc(logger)); // map operator SingleOutputStreamOperator\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; operator = source.map(MapFunc.getMapFunc(logger)); // sink operator.print(); executionEnvironment.execute(\u0026quot;not-restart\u0026quot;); } } Flink Job 开启 Checkpoint 在给 Flink 作业设置重启策略后，作业失败时会重新运行，此时，作业是从最开始的位置开始运行，每次重新启动后，上述示例中的 sum 都会从 0 开始；\n在真实场景下，通常不需要作业又重新把历史数据都计算一次，因为可能会造成数据重复，Flink 中因此引入了 Checkpoint 机制，作业可以利用 Checkpoint 保存 sum 的值，在失败的地方重新启动，此时 sum 的值会从 Checkpoint 中读取，并持续累加。\n// 重启尝试次数 2，每次重启间隔 5 S executionEnvironment.setRestartStrategy(RestartStrategies.fixedDelayRestart(2, Time.seconds(5))); // 每10ms进行一次 Checkpoint executionEnvironment.enableCheckpointing(10); 完整代码地址： https://github.com/ArchieYao/flink-learning/tree/main/hello-world\n参考：\nhttps://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/ops/state/checkpointing_under_backpressure/\n",
      "summary": "摘要： 如果把运行中的 Flink 程序比做一条河流，Checkpoint 就是一个相机，定期地对河流进行拍照，记录河水的状态。本文以自顶向下的视角，从理论到实现，分析 Flink 中的 Checkpoint 机制；\n理论基础 asynchronous barrier snapshotting 算法步骤 算法在 Flink 中的实现 Flink Checkpoint 整体流程 Flink Checkpoint Barrier Alignment Flink Checkpoint 使用 Flink Job 重启策略 Flink Job 开启 Checkpoint 理论基础 asynchronous barrier snapshotting Flink Checkpoint 机制是异步屏障快照（asynchronous barrier snapshotting, ABS）算法的一种实现，而 ABS 算法基于 Chandy-Lamport 的变种，但数据模型是还是基于 Chandy-Lamport；\n在 flink 中，作业算子被抽象为 DAG，节点为 operator，边是每一个 operator 的 stream（channel），与 Chandy-Lamport 的数据模型正好吻合；\nABS 算法把 Chandy-Lamport 中的 marker 消息换成了 barrier，作用是一致的，都是切分 snapshot；\nABS 算法 中 asynchronous 是异步的意思，当算子收齐 barrier 并触发快照之后，不会等待快照数据全部写入状态后端，而是一边后台写入，一边立刻继续处理数据流，并将 barrier 发送到下游，实现了最小化延迟。当然，引入异步性之后，所有有状态的算子都需要上报 ack，否则 JobManager 就无法确认一次 snapshot 是否完成。"
    },{
      "title": "Flink时间语义",
      "url": "https://qiref.github.io/post/2022/02/25/flink%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89/",
      "content": "摘要： 理解流处理中的时间语义，处理时间和事件时间。\n如图，在无界数据中，随着时间推移，数据一直产生，但真实情况中，往往在一段时间内的数据都是不均匀的，往往会出现意外的情况，比如在地铁无信号的情况下，数据虽然产生，但是会有一段时间延迟才会到达消息队列，例如虚线框中的数据。\n处理时间 处理时间就是流计算处理程序的机器本地时间，按照这种时间语义，在流计算的时间窗口中，上述例子中的数据会按这样分布：\n基于本地时间，在第一分钟，流处理程序只收到了 15、18 两个数据，后续数据由于网络原因，在 8:01:00 之后才到达流计算程序，所以后续数据在下一个时间窗口内。\n事件时间 事件时间就是事件的发生时间，这个时间通常会在数据中，按照这种时间语义，在流计算的时间窗口中，上述例子中的数据会按这样分布：\n基于事件时间，在第一分钟，数据应该是：15 18 9 10 ，在第二分钟，数据应该是：11 。\nwatermark 由于事件时间的窗口和事件相关，那么如果下一个事件还未到达，流计算程序是否就无限等待呢？\n为了解决这个问题，flink 引入 watermark 的概念，假如定义 watermark 为 T，那么在每一个时间窗口中，T 都会单调递增 T \u0026lt; T1，并且下一个时间窗口中的事件时间必须大于 T1，那么每一个时间窗口的数据就是介于 T-T1。\n",
      "summary": "摘要： 理解流处理中的时间语义，处理时间和事件时间。\n如图，在无界数据中，随着时间推移，数据一直产生，但真实情况中，往往在一段时间内的数据都是不均匀的，往往会出现意外的情况，比如在地铁无信号的情况下，数据虽然产生，但是会有一段时间延迟才会到达消息队列，例如虚线框中的数据。\n处理时间 处理时间就是流计算处理程序的机器本地时间，按照这种时间语义，在流计算的时间窗口中，上述例子中的数据会按这样分布：\n基于本地时间，在第一分钟，流处理程序只收到了 15、18 两个数据，后续数据由于网络原因，在 8:01:00 之后才到达流计算程序，所以后续数据在下一个时间窗口内。\n事件时间 事件时间就是事件的发生时间，这个时间通常会在数据中，按照这种时间语义，在流计算的时间窗口中，上述例子中的数据会按这样分布：\n基于事件时间，在第一分钟，数据应该是：15 18 9 10 ，在第二分钟，数据应该是：11 。\nwatermark 由于事件时间的窗口和事件相关，那么如果下一个事件还未到达，流计算程序是否就无限等待呢？\n为了解决这个问题，flink 引入 watermark 的概念，假如定义 watermark 为 T，那么在每一个时间窗口中，T 都会单调递增 T \u0026lt; T1，并且下一个时间窗口中的事件时间必须大于 T1，那么每一个时间窗口的数据就是介于 T-T1。"
    },{
      "title": "Flink基本架构",
      "url": "https://qiref.github.io/post/2022/02/23/flink%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84/",
      "content": "摘要： 鸟瞰 Flink 架构，分析 Flink 内部组件工作机制。\nFlink 架构图 提交作业流程 Flink 集群模式 JobManager Taskmanager 算子链 Slot task 数据交换策略 Flink 架构图 一个完整的 Flink 集群由一个 Jobmanager 和若干个 Taskmanager 组成，Jobmanager 主要负责调度 task 以及 协调 Checkpoint。Taskmanager 则负责具体的 task 执行，以及数据流的交换。\n可以通过多种方式启动 JobManager 和 TaskManager：直接在机器上作为standalone 集群启动、在容器中启动、或者通过YARN等资源框架管理并启动。TaskManager 连接到 JobManagers，宣布自己可用，并被分配工作。\n提交作业流程 以一个作业提交的流程来说明 Flink 各个组件是如何交互和工作的：\nFlink 集群模式 Flink 集群类型一般有以下几种：\nFlink Session 集群\n这种模式下，集群自创建开始，最后到集群生命周期结束，不受作业因素影响； 集群下的多个作业共享 内存、网络、磁盘等资源，如果集群出现异常，该集群下的所有作业都会收到影响。\n优点：提交作业速度很快，无需提前申请资源； 并且资源利用率较高。\n缺点：作业之间隔离性较差，横向扩展不太方便。\nFlink job 集群\n这种模式也称 pre-job 模式，集群交由 资源管理器托管，例如 Yarn ，需要运行作业，第一步申请资源，启动一个 Flink 集群，第二步提交作业，这种模式下，每个作业会独享一个 Flink 集群。\n优点：作业之间相互隔离，稳定性高，并且不同作业可以根据资源情况灵活调整。\n缺点：资源浪费，并且作业启动较慢。\nFlink Application 集群\n这种模式也称 Application 模式，这种模式的集群和作业相关，每一个作业独享一个集群，无需事先启动一个集群，而且直接从作业 jar 中提取 JobGraph 执行。这种模式下，每一个作业就是一个 Application ，同时也是一个 Flink 集群。\n优点：集群直接相互隔离，可以很好地利用资源；\n缺点：暂无。\nJobManager 在 Flink 集群中，JobManager 负责协调、调度 Task ，以及作业快照、从快照中恢复等功能。\nJobmanager 核心组件：\nResourceManager\nResourceManager 负责 Flink 集群中的资源分配、回收，并管理 task slot。\nDispatcher\nDispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。\nJobMaster\nJobMaster 负责管理单个JobGraph的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。\n一个 Flink 集群中，至少有一个 Jobmanager ，可以设定多个 Jobmanager ，leader 只有一个，其他 Jobmanager 为 standby。\nTaskmanager Taskmanager 负责具体 task 的执行，以及数据流的交换。 同时， Taskmanager 需要将资源状态向 Jobmanager 汇报。\n算子链 假设 Flink 算子并行度为2，该算子的 subTask 有两个，Flink 会将算子的 subtasks 链接成 tasks。每个 tasks 由一个线程执行。将算子链接成 task 是个有用的优化：它减少线程间切换、缓冲的开销，并且减少延迟的同时增加整体吞吐量，此时一个线程将会执行多个 subTask。\n链行为是可以配置的，如果想对整个作业禁用算子链，可以调用 StreamExecutionEnvironment.disableOperatorChaining()。\nSlot task slot 代表 Taskmanager 中的可用资源（不包括CPU）的集合，例如，如果一个 Taskmanager 有3个 slot，那每个 slot 将会平分 Taskmanager 的内存资源，当第四个 task 提交过来时，task 将进入 SCHEDULE 状态，需要等待其他 task 执行完成，才能执行下一个 task。\nslot共享：\n默认情况下，Flink 允许 subtask 共享 slot，即便它们是不同的 task 的 subtask，只要是来自于同一作业即可。算子最大并行度和 slot 数量一致，算子之间的数据交换会根据不同的策略进行。\n每一个 subtasks 算子链由一个线程执行，在 slot 中的执行情况如下：\ntask 数据交换策略 数据交换策略定义了在物理执行流图中如何将数据分配给任务，\nForward\n将数据从一个任务发送到接收任务。如果两个任务都位于同一台物理计算机上（这通常由任务调度器确保），这种交换策略可以避免网络通信。\nBroadCast\n将所有数据发送到算子的所有的并行任务上面去。因为这种策略会复制数据和涉及网络通信，所以代价相当昂贵。\nKey-based\n基于键控的策略通过Key值(键)对数据进行分区保证具有相同Key的数据将由同一任务处理。\nRandom\n随机策略统一将数据分配到算子的任务中去，以便均匀地将负载分配到不同的计算任务。\n参考：\nhttps://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/concepts/flink-architecture/\nhttps://www.jianshu.com/p/3898dd13f079\nhttps://confucianzuoyuan.github.io/flink-tutorial/book/chapter02-01-03-%E6%95%B0%E6%8D%AE%E4%BA%A4%E6%8D%A2%E7%AD%96%E7%95%A5.html\n",
      "summary": "摘要： 鸟瞰 Flink 架构，分析 Flink 内部组件工作机制。\nFlink 架构图 提交作业流程 Flink 集群模式 JobManager Taskmanager 算子链 Slot task 数据交换策略 Flink 架构图 一个完整的 Flink 集群由一个 Jobmanager 和若干个 Taskmanager 组成，Jobmanager 主要负责调度 task 以及 协调 Checkpoint。Taskmanager 则负责具体的 task 执行，以及数据流的交换。\n可以通过多种方式启动 JobManager 和 TaskManager：直接在机器上作为standalone 集群启动、在容器中启动、或者通过YARN等资源框架管理并启动。TaskManager 连接到 JobManagers，宣布自己可用，并被分配工作。\n提交作业流程 以一个作业提交的流程来说明 Flink 各个组件是如何交互和工作的：\nFlink 集群模式 Flink 集群类型一般有以下几种：\nFlink Session 集群\n这种模式下，集群自创建开始，最后到集群生命周期结束，不受作业因素影响； 集群下的多个作业共享 内存、网络、磁盘等资源，如果集群出现异常，该集群下的所有作业都会收到影响。\n优点：提交作业速度很快，无需提前申请资源； 并且资源利用率较高。\n缺点：作业之间隔离性较差，横向扩展不太方便。\nFlink job 集群\n这种模式也称 pre-job 模式，集群交由 资源管理器托管，例如 Yarn ，需要运行作业，第一步申请资源，启动一个 Flink 集群，第二步提交作业，这种模式下，每个作业会独享一个 Flink 集群。"
    },{
      "title": "Flink WordCount",
      "url": "https://qiref.github.io/post/2022/02/22/flink-wordcount/",
      "content": "摘要：Flink 从零开始，下载集群并运行 WordCount Job。 完整代码地址： https://github.com/ArchieYao/flink-learning/tree/main/hello-world\nFlink 本地模式集群安装 运行Flink，需提前安装好 Java 8 或者 Java 11。\nwget https://dlcdn.apache.org/flink/flink-1.14.3/flink-1.14.3-bin-scala_2.12.tgz tar -zxvf flink-1.14.3-bin-scala_2.12.tgz cd flink-1.14.3 ./bin/start-cluster.sh 运行成功后，可以在 IP:8081 访问 Flink-UI\nFlink Word Count job source 是多段文本，类型： DataSource ，经过 flatMap，切分为每个单词，然后转换为：(val,n) 的数据，然后根据 val 分组统计，得出 sum(n) 的值。\npublic static void main(String[] args) throws Exception { // 创建Flink任务运行环境 final ExecutionEnvironment executionEnvironment = ExecutionEnvironment.getExecutionEnvironment(); // 创建DataSet，数据是一行一行文本 DataSource\u0026lt;String\u0026gt; text = executionEnvironment.fromElements( \u0026quot;Licensed to the Apache Software Foundation (ASF) under one\u0026quot;, \u0026quot;or more contributor license agreements. See the NOTICE file\u0026quot;, \u0026quot;distributed with this work for additional information\u0026quot;, \u0026quot;regarding copyright ownership. The ASF licenses this file\u0026quot;, \u0026quot;to you under the Apache License, Version 2.0 (the\u0026quot;); // 通过Flink内置转换函数进行计算 AggregateOperator\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; sum = text.flatMap( new FlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;() { @Override public void flatMap( String value, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; collector) throws Exception { String[] split = value.split(\u0026quot;\\\\W+\u0026quot;); for (String s : split) { if (s.length() \u0026gt; 0) { collector.collect(new Tuple2\u0026lt;\u0026gt;(s, 1)); // TimeUnit.SECONDS.sleep(5); } } } }) .groupBy(0) .sum(1); // 打印结果 sum.print(); } Job 可以直接运行，也可以提交到 Flink 集群中运行。\nmvn clean package -DskipTests -Dcheckstyle.skip=true -Drat.skip=true # 值得注意的是，在 pom 中，应该指定 Job 的 main class。 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-jar-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;archive\u0026gt; \u0026lt;manifest\u0026gt; \u0026lt;mainClass\u0026gt;archieyao.github.io.WordCount\u0026lt;/mainClass\u0026gt; \u0026lt;/manifest\u0026gt; \u0026lt;/archive\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; ./bin/flink run -j hello-world-1.0-SNAPSHOT.jar ",
      "summary": "摘要：Flink 从零开始，下载集群并运行 WordCount Job。 完整代码地址： https://github.com/ArchieYao/flink-learning/tree/main/hello-world\nFlink 本地模式集群安装 运行Flink，需提前安装好 Java 8 或者 Java 11。\nwget https://dlcdn.apache.org/flink/flink-1.14.3/flink-1.14.3-bin-scala_2.12.tgz tar -zxvf flink-1.14.3-bin-scala_2.12.tgz cd flink-1.14.3 ./bin/start-cluster.sh 运行成功后，可以在 IP:8081 访问 Flink-UI\nFlink Word Count job source 是多段文本，类型： DataSource ，经过 flatMap，切分为每个单词，然后转换为：(val,n) 的数据，然后根据 val 分组统计，得出 sum(n) 的值。\npublic static void main(String[] args) throws Exception { // 创建Flink任务运行环境 final ExecutionEnvironment executionEnvironment = ExecutionEnvironment.getExecutionEnvironment(); // 创建DataSet，数据是一行一行文本 DataSource\u0026lt;String\u0026gt; text = executionEnvironment.fromElements( \u0026quot;Licensed to the Apache Software Foundation (ASF) under one\u0026quot;, \u0026quot;or more contributor license agreements."
    },{
      "title": "Go语言cobra",
      "url": "https://qiref.github.io/post/2022/02/19/go%E8%AF%AD%E8%A8%80cobra/",
      "content": "摘要：Go语言 cobra 框架使用说明，文中代码地址： https://github.com/ArchieYao/clid\ncobra 简介 cobra是 Go 语言的一个命令行程序库，可以用来编写命令行程序。同时，它也提供了一个脚手架， 用于生成基于 cobra 的应用程序框架。非常多知名的开源项目使用了 cobra 库构建命令行，如Kubernetes、Hugo、etcd等。\n安装 cobra cobra 是由大名鼎鼎的 spf13（golang 开发者） 开发的，GitHub 地址：https://github.com/spf13/cobra\n// 安装 go get -u github.com/spf13/cobra # 检查是否安装成功 cobra -h Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application. Usage: cobra [command] Available Commands: add Add a command to a Cobra Application completion Generate the autocompletion script for the specified shell help Help about any command init Initialize a Cobra Application Flags: -a, --author string author name for copyright attribution (default \u0026quot;YOUR NAME\u0026quot;) --config string config file (default is $HOME/.cobra.yaml) -h, --help help for cobra -l, --license string name of license for the project --viper use Viper for configuration Use \u0026quot;cobra [command] --help\u0026quot; for more information about a command. 创建 cobra 工程 # 创建 Go 工程 mkdir clid cd clid go mod init clid # 创建 cobra 工程 cobra init # 可以得到如下工程目录 D:\\WORKSPACE\\GOLANG\\TRAINNING\\CLID │ go.mod │ go.sum │ LICENSE │ main.go │ └───cmd root.go # 正常情况下，这个工程可以正常编译并且运行 go build -o ./bin/clid 增加自定义命令 cobra add version tree /f D:. │ go.mod │ go.sum │ LICENSE │ main.go │ └───cmd root.go version.go # 在 `cmd` 目录下新生成一个 `version.go` 的文件 从新生成的文件可以看出，核心方法就2个：\nvar versionCmd = \u0026amp;cobra.Command{ Use: \u0026quot;version\u0026quot;, Short: \u0026quot;版本信息\u0026quot;, // 命令提示 Long: `版本信息描述`, // 命令说明 Run: func(cmd *cobra.Command, args []string) { fmt.Println(\u0026quot;0.0.1-Alpha\u0026quot;) }, } func init() { rootCmd.AddCommand(versionCmd) // version 命令是 rootCmd 的一个子命令 } #./clid A longer description that spans multiple lines and likely contains examples and usage of using your application. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application. Usage: 1 [command] Available Commands: completion Generate the autocompletion script for the specified shell help Help about any command version 版本信息 # ./clid version 0.0.1-Alpha # ./clid version -h 版本信息描述 Usage: clid version [flags] Flags: -h, --help help for version 给子命令增加参数 重新增加命令测试\ncobra add foo var fooCmd = \u0026amp;cobra.Command{ Use: \u0026quot;foo\u0026quot;, Short: \u0026quot;A brief description of your command\u0026quot;, Long: `A longer description `, Run: func(cmd *cobra.Command, args []string) { fmt.Println(\u0026quot;foo called\u0026quot;) bar, err := cmd.Flags().GetString(\u0026quot;bar\u0026quot;) if err != nil { fmt.Println(\u0026quot;请输入正确的bar\u0026quot;) } fmt.Printf(\u0026quot;输入值bar[%s]\u0026quot;, bar) }, } func init() { rootCmd.AddCommand(fooCmd) fooCmd.Flags().StringP(\u0026quot;bar\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;默认值\u0026quot;, \u0026quot;命令说明\u0026quot;) } 执行结果：\n#./clid foo -h A longer description Usage: clid foo [flags] Flags: -a, --bar string 命令说明 (default \u0026quot;默认值\u0026quot;) -h, --help help for foo # ./clid foo -a test # ./clid foo --bar test foo called 输入值bar[test] 命令其他参数 以上是指定了 foo 命令的参数，cobra 还可以识别命令后的附加参数。\n以下代码定义了一个多数求和的命令，执行命令后，在命令后带上需要求和的参数，即可完成求和。\nvar sumCmd = \u0026amp;cobra.Command{ Use: \u0026quot;sum\u0026quot;, Short: \u0026quot;A brief description of your command\u0026quot;, Long: `A longer description .`, Run: func(cmd *cobra.Command, args []string) { fmt.Println(\u0026quot;sum called\u0026quot;) result := 0 for _, v := range args { arg, err := strconv.Atoi(v) if err != nil { fmt.Printf(\u0026quot;args: %s , %s cannot convert to int \u0026quot;, args, v) return } result += arg } fmt.Printf(\u0026quot;result [%d]\u0026quot;, result) }, } 测试结果：\n# ./clid sum 1 2 3 4 5 sum called result [15] ",
      "summary": "摘要：Go语言 cobra 框架使用说明，文中代码地址： https://github.com/ArchieYao/clid\ncobra 简介 cobra是 Go 语言的一个命令行程序库，可以用来编写命令行程序。同时，它也提供了一个脚手架， 用于生成基于 cobra 的应用程序框架。非常多知名的开源项目使用了 cobra 库构建命令行，如Kubernetes、Hugo、etcd等。\n安装 cobra cobra 是由大名鼎鼎的 spf13（golang 开发者） 开发的，GitHub 地址：https://github.com/spf13/cobra\n// 安装 go get -u github.com/spf13/cobra # 检查是否安装成功 cobra -h Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application. Usage: cobra [command] Available Commands: add Add a command to a Cobra Application completion Generate the autocompletion script for the specified shell help Help about any command init Initialize a Cobra Application Flags: -a, --author string author name for copyright attribution (default \u0026quot;YOUR NAME\u0026quot;) --config string config file (default is $HOME/."
    },{
      "title": "ProtoBuf数据协议",
      "url": "https://qiref.github.io/post/2022/01/11/protobuf%E6%95%B0%E6%8D%AE%E5%8D%8F%E8%AE%AE/",
      "content": "摘要：ProtoBuf(Protocol Buffers)是一种跨平台、语言无关、可扩展的序列化结构数据的方法，可用于网络数据交换及数据存储。\nProtocol Buffers介绍 不同于 XML 、JSON 这种文本格式数据，Protocol Buffers 是一种二进制格式数据。在Protocol Buffers 诞生之初，就被赋予两个特点：\n向前兼容，很容易引入新字段，应对字段的频繁变更 数据格式具备描述性，并且支持多语言处理 传输效率高 基于以上这些特性，Protocol Buffers 被广泛应用于各种 RPC 框架中，并且是 Google 的数据通用语言。\nProtocol Buffers协议文件 Protocol Buffers 在使用前需要先定义好协议文件，以 .proto 为后缀的文件就是Protocol Buffers 的协议文件。\nExample:\n// 指定protobuf的版本，proto3是最新的语法版本 syntax = \u0026quot;proto3\u0026quot;; // 定义数据结构，message 你可以想象成java的class，c语言中的struct message Response { string data = 1; // 定义一个string类型的字段，字段名字为data, 序号为1 int32 status = 2; // 定义一个int32类型的字段，字段名字为status, 序号为2 } 如果 A 和 B 要基于 Protocol Buffers 协议进行通信，那么在通信前，A 和 B 都需要有同一份协议文件，所以在 Protocol Buffers 数据传输过程中，不需要数据的 Schema 信息；\nProtocol Buffers软件 值得一提的是，Protocol Buffers 可以安装软件，将指定的 proto 协议文件转换为多种编程语言代码，并且提供了序列化和反序列化等一系列方法库。\n下载地址： https://github.com/protocolbuffers/protobuf/releases\nMAC 下安装：\nbrew install protobuf # 安装完成后验证 protoc --version 转换为代码:\n# 在当前目录输出 response.proto 协议生成的Java代码 protoc --java_out=. response.proto \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.protobuf\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;protobuf-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.9.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ResponseOuterClass.Response.Builder builder = ResponseOuterClass.Response.newBuilder(); // 设置字段值 builder.setData(\u0026quot;hello www.tizi365.com\u0026quot;); builder.setStatus(200); ResponseOuterClass.Response response = builder.build(); // 将数据根据protobuf格式，转化为字节数组 byte[] byteArray = response.toByteArray(); // 反序列化,二进制数据 ResponseOuterClass.Response newResponse = ResponseOuterClass.Response.parseFrom(byteArray); System.out.println(newResponse.getData()); System.out.println(newResponse.getStatus()); Protocol Buffers 编码 Protocol Buffers 编码主要借助 Varint 和 ZigZag 算法实现，这也使得 Protocol Buffers 有更优秀的传输效率。\n在理解这两个算法前，首先需要回顾一下计算机二进制表示的几种方式：\n原码，第一位表示符号位（0为非负数，1为负数），剩余表示值。 +8 = 0000 1000「原码」 -8 = 1000 1000「原码」\n反码，第一位表示符号位（0为非负数，1为负数），剩余位，负数按位取反，非负数不变，正数的反码是原码本身。 +8 = 0000 1000「反码」 -8 = 1111 0111「反码」\n补码，由于原码和反码无法直接参与位运算（符号位的存在），所以引入补码，补码用第一位表示符号（0为非负数，1为负数），剩下的位非负数保持不变，负数按位求反末位加1.（补码的补码是原码，正数的补码是本身） +8 = 0000 1000「补码」 -8 = 1000 1000「原码」-\u0026gt; 1111 0111 +1 -\u0026gt; 1111 1000「补码」\n计算机中的数值是用补码来表示和存储的。\nVarint Varint 编码是将数字转换为字节数组的编码方式，使用前提是数字较小，这样才能更高效地压缩。\n编码过程：\n300 的二进制表示，int 类型（4个字节） 1 0010 1100 由于前2个字节都是空位，浪费了2个字节的空间。\n使用 Varint 编码则会变为 1010 1100 0000 0010。 变为了两个字节。\n1 0010 1100 // 每7位为一个单元，从后往前编码，再加首位 msb(most significant bit) ，表示后续8位是否连续 010 1100 → 1010 1100 000 0010 → 0000 0010 → 1010 1100 0000 0010 解码过程：\n000 0010 010 1100 // 解码需去除msb → 000 0010 ++ 010 1100 → 100101100 → 256 + 32 + 8 + 4 = 300 ZigZag ZigZag 编码是为了解决 Varint 对负数编码效率低的问题（最高位是符号位 1 ），将有符号整数映射为无符号整数，在实现上，映射通过移位即可实现。\n在 Varint 中，负数有个什么问题呢？ 符号位在第一位！导致高位一直需要编码。\n// -1 10000000 00000000 00000000 00000001「原」 11111111 11111111 11111111 11111111「补」 如果对 -1 进行 Varint 编码，就显得不那么高效了，因为所有位都是1，那有没有办法把这个符号位换一下呢？ZigZag 就完成了这件事：\n// -1 经过zigzag 编码变成 1 11111111 11111111 11111111 11111111「补」 00000000 00000000 00000000 00000001「zigzag」 // 1 经过zigzag 编码变成 2 00000000 00000000 00000000 00000001「补码」 00000000 00000000 00000000 00000010「zigzag」 移位运算：\n\u0026lt;\u0026lt; : 左移运算，左移几位低位就补几个0 \u0026gt;\u0026gt; : 右移运算，如果数字为正数时，移位后在高位补0；如果数字为负数时，移位后在高位补1 \u0026gt;\u0026gt;\u0026gt;: 无符号右移位，不管正数还是负数，高位都用0补齐（忽略符号位） 编码过程：\n(n \u0026lt;\u0026lt; 1) ^ (n \u0026gt;\u0026gt; 31) // 异或（相同为0，不同为1） // 推演过程 -1 11111111 11111111 11111111 11111111「补」 → n \u0026lt;\u0026lt; 1 → 1111111 11111111 11111111 111111110「补」 → n \u0026gt;\u0026gt; 31 → 11111111 11111111 11111111 11111111「补」 → 异或 → 00000000 00000000 00000000 00000001「补」 // 推演过程 1 00000000 00000000 00000000 00000001「补」 → n \u0026lt;\u0026lt; 1 → 00000000 00000000 00000000 00000010「补」 → n \u0026gt;\u0026gt; 31 → 00000000 00000000 00000000 00000000「补」 → 异或 → 00000000 00000000 00000000 00000010「补」 解码过程：\n(n \u0026gt;\u0026gt;\u0026gt; 1) ^ -(n \u0026amp; 1) // 推演过程 -1 00000000 00000000 00000000 00000001「补」 → n \u0026gt;\u0026gt;\u0026gt; 1 → 00000000 00000000 00000000 00000000「补」 → -(n \u0026amp; 1) → 00000000 00000000 00000000 00000001「补」 → 11111111 11111111 11111111 11111111「补」 → 异或 → 11111111 11111111 11111111 11111111「补」 参考：\nhttps://colobu.com/2019/10/03/protobuf-ultimate-tutorial-in-go/\nhttps://github.com/halfrost/Halfrost-Field/blob/master/contents/Protocol/Protocol-buffers-encode.md#%E5%85%AD-protocol-buffer-%E7%BC%96%E7%A0%81%E5%8E%9F%E7%90%86\nhttps://developers.google.com/protocol-buffers\nhttps://www.tizi365.com/archives/367.html\n",
      "summary": "摘要：ProtoBuf(Protocol Buffers)是一种跨平台、语言无关、可扩展的序列化结构数据的方法，可用于网络数据交换及数据存储。\nProtocol Buffers介绍 不同于 XML 、JSON 这种文本格式数据，Protocol Buffers 是一种二进制格式数据。在Protocol Buffers 诞生之初，就被赋予两个特点：\n向前兼容，很容易引入新字段，应对字段的频繁变更 数据格式具备描述性，并且支持多语言处理 传输效率高 基于以上这些特性，Protocol Buffers 被广泛应用于各种 RPC 框架中，并且是 Google 的数据通用语言。\nProtocol Buffers协议文件 Protocol Buffers 在使用前需要先定义好协议文件，以 .proto 为后缀的文件就是Protocol Buffers 的协议文件。\nExample:\n// 指定protobuf的版本，proto3是最新的语法版本 syntax = \u0026quot;proto3\u0026quot;; // 定义数据结构，message 你可以想象成java的class，c语言中的struct message Response { string data = 1; // 定义一个string类型的字段，字段名字为data, 序号为1 int32 status = 2; // 定义一个int32类型的字段，字段名字为status, 序号为2 } 如果 A 和 B 要基于 Protocol Buffers 协议进行通信，那么在通信前，A 和 B 都需要有同一份协议文件，所以在 Protocol Buffers 数据传输过程中，不需要数据的 Schema 信息；"
    },{
      "title": "Log4j2按照时间和日志大小滚动",
      "url": "https://qiref.github.io/post/2021/08/25/log4j2%E6%8C%89%E7%85%A7%E6%97%B6%E9%97%B4%E5%92%8C%E6%97%A5%E5%BF%97%E5%A4%A7%E5%B0%8F%E6%BB%9A%E5%8A%A8/",
      "content": "摘要：Log4j2 按照时间和日志大小滚动。\nLog4j2按照时间和日志大小滚 status = error name = PropertiesConfig #Make sure to change log file path as per your need property.filename = C:\\\\logs\\\\debug.log filters = threshold filter.threshold.type = ThresholdFilter filter.threshold.level = debug appenders = rolling appender.rolling.type = RollingFile appender.rolling.name = RollingFile appender.rolling.fileName = ${filename} appender.rolling.filePattern = debug-backup-%d{MM-dd-yy-HH-mm-ss}-%i.log.gz appender.rolling.layout.type = PatternLayout appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n appender.rolling.policies.type = Policies appender.rolling.policies.time.type = TimeBasedTriggeringPolicy appender.rolling.policies.time.interval = 1 appender.rolling.policies.time.modulate = true appender.rolling.policies.size.type = SizeBasedTriggeringPolicy appender.rolling.policies.size.size=10MB appender.rolling.strategy.type = DefaultRolloverStrategy appender.rolling.strategy.max = 20 loggers = rolling #Make sure to change the package structure as per your application logger.rolling.name = com.howtodoinjava logger.rolling.level = debug logger.rolling.additivity = false logger.rolling.appenderRef.rolling.ref = RollingFile 日志配置动态更新 monitorInterval 是用来设置配置文件的动态加载时间的，单位是秒。\nmonitorInterval = 10 ",
      "summary": "摘要：Log4j2 按照时间和日志大小滚动。\nLog4j2按照时间和日志大小滚 status = error name = PropertiesConfig #Make sure to change log file path as per your need property.filename = C:\\\\logs\\\\debug.log filters = threshold filter.threshold.type = ThresholdFilter filter.threshold.level = debug appenders = rolling appender.rolling.type = RollingFile appender.rolling.name = RollingFile appender.rolling.fileName = ${filename} appender.rolling.filePattern = debug-backup-%d{MM-dd-yy-HH-mm-ss}-%i.log.gz appender.rolling.layout.type = PatternLayout appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n appender.rolling.policies.type = Policies appender.rolling.policies.time.type = TimeBasedTriggeringPolicy appender.rolling.policies.time.interval = 1 appender.rolling.policies.time.modulate = true appender."
    },{
      "title": "Git-rebase用法.md",
      "url": "https://qiref.github.io/post/2021/08/09/git-rebase%E7%94%A8%E6%B3%95.md/",
      "content": "摘要：Git rebase 的使用方法。\ncommit 合并 当多个commit存在时，提交MR会出现很多的commit，review会很困难，这时可以将多个commit合并为一个commit。\n命令说明：\ngit rebase -i [startpoint] [endpoint] 其中-i的意思是\u0026ndash;interactive，即弹出交互式的界面让用户编辑完成合并操作，[startpoint] [endpoint] 则指定了一个编辑区间，如果不指定[endpoint]，则该区间的终点默认是当前分支HEAD所指向的commit(注：该区间指定的是一个前开后闭的区间)。 在查看到了log日志后，我们运行以下命令：\ngit rebase -i 36224db or git rebase -i HEAD~3 # 合并最近三次commit 每一个commit id 前面的pick表示指令类型，git 为我们提供了以下几个命令: pick：保留该commit（缩写:p） reword：保留该commit，但我需要修改该commit的注释（缩写:r） edit：保留该commit, 但我要停下来修改该提交(不仅仅修改注释)（缩写:e） squash：将该commit和前一个commit合并（缩写:s） fixup：将该commit和前一个commit合并，但我不要保留该提交的注释信息（缩写:f） exec：执行shell命令（缩写:x） drop：我要丢弃该commit（缩写:d）\n示例 git log commit 4ab2734f3380fbdace8620f461cd04c7993b6b0b (HEAD -\u0026gt; master) Author: archieyao \u0026lt;archieyao@tencent.com\u0026gt; Date: Mon Aug 9 16:38:25 2021 +0800 add something 2 commit 60d0bbbe094c0b93903ab995879d30246bbf331e Author: archieyao \u0026lt;archieyao@tencent.com\u0026gt; Date: Mon Aug 9 16:38:02 2021 +0800 add something 1 commit 1c3c12316449cf4f340c68e22c70caa60178ba5c Author: archieyao \u0026lt;archieyao@tencent.com\u0026gt; Date: Mon Aug 9 16:37:43 2021 +0800 add something commit 7a9ab6f445ce0c7525a5dce3ca15fe600282553b Author: archieyao \u0026lt;archieyao@tencent.com\u0026gt; Date: Mon Aug 9 09:31:11 2021 +0800 [update] readme 现在合并最近三次的commit。\ngit rebase -i 7a9ab6f445ce0c7525a5dce3ca15fe600282553b pick 1c3c123 add something s 60d0bbb add something 1 s 4ab2734 add something 2 # Rebase 7a9ab6f..4ab2734 onto 7a9ab6f (3 commands) # # Commands: # p, pick \u0026lt;commit\u0026gt; = use commit # r, reword \u0026lt;commit\u0026gt; = use commit, but edit the commit message # e, edit \u0026lt;commit\u0026gt; = use commit, but stop for amending # s, squash \u0026lt;commit\u0026gt; = use commit, but meld into previous commit # f, fixup \u0026lt;commit\u0026gt; = like \u0026quot;squash\u0026quot;, but discard this commit's log message # x, exec \u0026lt;command\u0026gt; = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with 'git rebase --continue') # d, drop \u0026lt;commit\u0026gt; = remove commit # l, label \u0026lt;label\u0026gt; = label current HEAD with a name # t, reset \u0026lt;label\u0026gt; = reset HEAD to a label # m, merge [-C \u0026lt;commit\u0026gt; | -c \u0026lt;commit\u0026gt;] \u0026lt;label\u0026gt; [# \u0026lt;oneline\u0026gt;] # . create a merge commit using the original merge commit's # . message (or the oneline, if no original merge commit was # . specified). Use -c \u0026lt;commit\u0026gt; to reword the commit message. # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. 然后继续编辑，选择commit message，删除后两次的commit message。\n# This is a combination of 3 commits. # This is the 1st commit message: add something # This is the commit message #2: add something 1 # 删除这行 # This is the commit message #3: add something 2 # 删除这行 # Please enter the commit message for your changes. Lines starting # with '#' will be ignored, and an empty message aborts the commit. # # Date: Mon Aug 9 16:37:43 2021 +0800 # # interactive rebase in progress; onto 7a9ab6f # Last commands done (3 commands done): # squash 60d0bbb add something 1 # squash 4ab2734 add something 2 # No commands remaining. # You are currently rebasing branch 'master' on '7a9ab6f'. # # Changes to be committed: # modified: README.md 保存退出后，可以看到输出信息：\n[detached HEAD 98eef0d] add something Date: Mon Aug 9 16:37:43 2021 +0800 1 file changed, 8 insertions(+) Successfully rebased and updated refs/heads/master. 如果后续推送到远程，需要 git push origin --force xxx ，强制覆盖远程代码。\n同步master代码 开发时，从master上checkout一个dev分支，开发一段时间后，master上的代码有更新，这时从master上拉取更新。\ngit rebase origin master # 等价于 git pull origin master --rebase git rebase --continue git rebase --abort 在rebase的过程中，也许会出现冲突(conflict)，在这种情况，Git会停止rebase并会让你去解决冲突；\ngit add conflict file git rebase --continue 在解决完冲突后，用git-add去更新这些内容的索引(index)， 然后，你无需执行 git-commit,只要执行: git rebase --continue这样git会继续应用(apply)余下的补丁。\n在任何时候，你可以用 git rebase --abort来终止rebase的行动，并且\u0026quot;mywork\u0026quot; 分支会回到rebase开始前的状态。\ngit merge 与 git rebase 的最终效果是一致的，但git merge会产生合并记录，使用git rebase 会让分支看起来没有合并一样。\n",
      "summary": "摘要：Git rebase 的使用方法。\ncommit 合并 当多个commit存在时，提交MR会出现很多的commit，review会很困难，这时可以将多个commit合并为一个commit。\n命令说明：\ngit rebase -i [startpoint] [endpoint] 其中-i的意思是\u0026ndash;interactive，即弹出交互式的界面让用户编辑完成合并操作，[startpoint] [endpoint] 则指定了一个编辑区间，如果不指定[endpoint]，则该区间的终点默认是当前分支HEAD所指向的commit(注：该区间指定的是一个前开后闭的区间)。 在查看到了log日志后，我们运行以下命令：\ngit rebase -i 36224db or git rebase -i HEAD~3 # 合并最近三次commit 每一个commit id 前面的pick表示指令类型，git 为我们提供了以下几个命令: pick：保留该commit（缩写:p） reword：保留该commit，但我需要修改该commit的注释（缩写:r） edit：保留该commit, 但我要停下来修改该提交(不仅仅修改注释)（缩写:e） squash：将该commit和前一个commit合并（缩写:s） fixup：将该commit和前一个commit合并，但我不要保留该提交的注释信息（缩写:f） exec：执行shell命令（缩写:x） drop：我要丢弃该commit（缩写:d）\n示例 git log commit 4ab2734f3380fbdace8620f461cd04c7993b6b0b (HEAD -\u0026gt; master) Author: archieyao \u0026lt;archieyao@tencent.com\u0026gt; Date: Mon Aug 9 16:38:25 2021 +0800 add something 2 commit 60d0bbbe094c0b93903ab995879d30246bbf331e Author: archieyao \u0026lt;archieyao@tencent.com\u0026gt; Date: Mon Aug 9 16:38:02 2021 +0800 add something 1 commit 1c3c12316449cf4f340c68e22c70caa60178ba5c Author: archieyao \u0026lt;archieyao@tencent."
    },{
      "title": "Git工程拆分",
      "url": "https://qiref.github.io/post/2021/07/08/git%E5%B7%A5%E7%A8%8B%E6%8B%86%E5%88%86/",
      "content": "摘要：Git将工程按目录拆分。\n# /project/ # ----/test/ # 将test目录抽成单独的工程 cd /project/test # 拆分子目录 $ git subtree split -P test -b test-new-br $ mkdir ../test-new-br $ cd ../test-new-br # 创建子工程 $ git init $ git pull ../project test-new-br # git log 可以看到历史commit \u0026#x1f635;\n",
      "summary": "摘要：Git将工程按目录拆分。\n# /project/ # ----/test/ # 将test目录抽成单独的工程 cd /project/test # 拆分子目录 $ git subtree split -P test -b test-new-br $ mkdir ../test-new-br $ cd ../test-new-br # 创建子工程 $ git init $ git pull ../project test-new-br # git log 可以看到历史commit \u0026#x1f635;"
    },{
      "title": "Kubernates中的pod",
      "url": "https://qiref.github.io/post/2021/06/29/kubernates%E4%B8%AD%E7%9A%84pod/",
      "content": "摘要：运行在Kubernates中的容器：pod。由于不能将多个进程聚集在一个单独的容器中，我们需要一种更高级的结构将容器绑定在一起，将它们作为一个单元进行管理，这就是pod诞生的原因。\npod基本概念 pod是Kubernetes中最重要的概念，pod是Kubernetes中部署的最小单元，一个pod中可以有一个或多个容器，pod有自己独立的私有IP和主机名。\nKubernetes 集群中的 Pod 主要有两种用法：\n运行单个容器的 Pod。\u0026ldquo;每个 Pod 一个容器\u0026quot;模型是最常见的 Kubernetes 用例； 在这种情况下，可以将 Pod 看作单个容器的包装器，并且 Kubernetes 直接管理 Pod，而不是容器。 运行多个协同工作的容器的 Pod。 Pod 可能封装由多个紧密耦合且需要共享资源的共处容器组成的应用程序。 这些位于同一位置的容器可能形成单个内聚的服务单元，一个pod中的容器共享网络和volume，并且pod中的容器共享相同的命名空间。（由于一个pod中的容器共享网络，因此也就共享端口和IP。） 一般情况下，如果不是多个容器需要共用net或volume，尽可能地把不同的容器放到不同的pod中，新建一个pod不需要耗费很多资源，Kubernetes可以很方便地对pod进行管理和扩容、缩容等操作，这种方式可以更好地利用基础资源。\n总结来说，pod就是逻辑上的主机。\npod生命周期 Pending：表示pod已经被同意创建，正在等待kube-scheduler选择合适的节点创建，一般是在准备镜像； Running：表示pod中所有的容器已经被创建，并且至少有一个容器正在运行或者是正在启动或者是正在重启； Succeeded：表示所有容器已经成功终止，并且不会再启动； Failed：表示pod中所有容器都是非0（不正常）状态退出； Unknown：表示无法读取Pod状态，通常是kube-controller-manager无法与Pod通信。 创建pod流程 客户端提交Pod的配置信息（可以是yaml文件定义好的信息）到kube-apiserver； Apiserver收到指令后，通知给controller-manager创建一个资源对象； Controller-manager通过api-server将pod的配置信息存储到ETCD数据中心中； Kube-scheduler检测到pod信息会开始调度预选，会先过滤掉不符合Pod资源配置要求的节点，然后开始调度调优，主要是挑选出更适合运行pod的节点，然后将pod的资源配置单发送到node节点上的kubelet组件上。 Kubelet根据scheduler发来的资源配置单运行pod，运行成功后，将pod的运行信息返回给scheduler，scheduler将返回的pod运行状况的信息存储到etcd数据中心。 删除pod流程 pod从service的endpoint列表中被移除； 如果该pod定义了一个停止前的钩子，其会在pod内部被调用，停止钩子一般定义了如何优雅的结束进程； 进程被发送TERM信号（kill -14）； 当超过优雅退出的时间后，Pod中的所有进程都会被发送SIGKILL信号（kill -9）。 pod常用命令 kubectl describe po cluster-admin-0 -n default # 获取指定namespaces下的pod详情，可以看出container信息 kubectl get pods --all-namespaces # 获取所有namespaces下的pod kubectl get pods -n default # -n 获取指定namaspaces下的pod kubectl get pod podname -nkube-system -oyaml # 获取pod的详情，-oyaml 以yaml格式输出，也可以 -ojson kubectl exec -it taskcenter-0 -c loglistener -noceanus /bin/bash # 进入某个pod下的cotainer kubectl logs tke-log-agent-2687c -c loglistener # 获取某个pod下cotainer的log，也可以加 -f 参数，类似于 tail -f 创建pod的方式 pod本身不具备故障重启以及副本等功能，一般使用其他的资源创建pod。\nReplicaSet 替代 ReplicationController，可以始终保持所需数量的pod副本正在运行，ReplicaSet具备更强的selector expression。 DaemonSet可以确保每个节点都运行一个pod实例，而ReplicaSet 和 ReplicationController 会将pod随机安排到集群节点。 Job可以创建批处理任务，可以在Job中运行一个或多个pod，周期性运行或在某个时间运行的Job可以通过CronJob实现。 参考：\nhttps://kubernetes.io/zh/docs/concepts/workloads/pods/\n《Kubernetes in Action》\n",
      "summary": "摘要：运行在Kubernates中的容器：pod。由于不能将多个进程聚集在一个单独的容器中，我们需要一种更高级的结构将容器绑定在一起，将它们作为一个单元进行管理，这就是pod诞生的原因。\npod基本概念 pod是Kubernetes中最重要的概念，pod是Kubernetes中部署的最小单元，一个pod中可以有一个或多个容器，pod有自己独立的私有IP和主机名。\nKubernetes 集群中的 Pod 主要有两种用法：\n运行单个容器的 Pod。\u0026ldquo;每个 Pod 一个容器\u0026quot;模型是最常见的 Kubernetes 用例； 在这种情况下，可以将 Pod 看作单个容器的包装器，并且 Kubernetes 直接管理 Pod，而不是容器。 运行多个协同工作的容器的 Pod。 Pod 可能封装由多个紧密耦合且需要共享资源的共处容器组成的应用程序。 这些位于同一位置的容器可能形成单个内聚的服务单元，一个pod中的容器共享网络和volume，并且pod中的容器共享相同的命名空间。（由于一个pod中的容器共享网络，因此也就共享端口和IP。） 一般情况下，如果不是多个容器需要共用net或volume，尽可能地把不同的容器放到不同的pod中，新建一个pod不需要耗费很多资源，Kubernetes可以很方便地对pod进行管理和扩容、缩容等操作，这种方式可以更好地利用基础资源。\n总结来说，pod就是逻辑上的主机。\npod生命周期 Pending：表示pod已经被同意创建，正在等待kube-scheduler选择合适的节点创建，一般是在准备镜像； Running：表示pod中所有的容器已经被创建，并且至少有一个容器正在运行或者是正在启动或者是正在重启； Succeeded：表示所有容器已经成功终止，并且不会再启动； Failed：表示pod中所有容器都是非0（不正常）状态退出； Unknown：表示无法读取Pod状态，通常是kube-controller-manager无法与Pod通信。 创建pod流程 客户端提交Pod的配置信息（可以是yaml文件定义好的信息）到kube-apiserver； Apiserver收到指令后，通知给controller-manager创建一个资源对象； Controller-manager通过api-server将pod的配置信息存储到ETCD数据中心中； Kube-scheduler检测到pod信息会开始调度预选，会先过滤掉不符合Pod资源配置要求的节点，然后开始调度调优，主要是挑选出更适合运行pod的节点，然后将pod的资源配置单发送到node节点上的kubelet组件上。 Kubelet根据scheduler发来的资源配置单运行pod，运行成功后，将pod的运行信息返回给scheduler，scheduler将返回的pod运行状况的信息存储到etcd数据中心。 删除pod流程 pod从service的endpoint列表中被移除； 如果该pod定义了一个停止前的钩子，其会在pod内部被调用，停止钩子一般定义了如何优雅的结束进程； 进程被发送TERM信号（kill -14）； 当超过优雅退出的时间后，Pod中的所有进程都会被发送SIGKILL信号（kill -9）。 pod常用命令 kubectl describe po cluster-admin-0 -n default # 获取指定namespaces下的pod详情，可以看出container信息 kubectl get pods --all-namespaces # 获取所有namespaces下的pod kubectl get pods -n default # -n 获取指定namaspaces下的pod kubectl get pod podname -nkube-system -oyaml # 获取pod的详情，-oyaml 以yaml格式输出，也可以 -ojson kubectl exec -it taskcenter-0 -c loglistener -noceanus /bin/bash # 进入某个pod下的cotainer kubectl logs tke-log-agent-2687c -c loglistener # 获取某个pod下cotainer的log，也可以加 -f 参数，类似于 tail -f 创建pod的方式 pod本身不具备故障重启以及副本等功能，一般使用其他的资源创建pod。"
    },{
      "title": "Docker构建Go工程镜像",
      "url": "https://qiref.github.io/post/2021/06/23/docker%E6%9E%84%E5%BB%BAgo%E5%B7%A5%E7%A8%8B%E9%95%9C%E5%83%8F/",
      "content": "摘要：Docker构建Go工程镜像。\n工程 工程是一个比较简单的Http server的demo，现在将这个工程构建为docker镜像。\nimport ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net/http\u0026quot; ) func Init() { log.Println(\u0026quot;start server\u0026quot;) http.HandleFunc(\u0026quot;/hello_world\u0026quot;, HelloWorld) http.Handle(\u0026quot;/test_handle\u0026quot;, \u0026amp;TestHandleStruct{content: \u0026quot;test handle\u0026quot;}) if err := http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil); err != nil { log.Println(\u0026quot;start server on 8080\u0026quot;) } log.Fatal(\u0026quot;start server failed.\u0026quot;) } func main() { // 启动HTTP服务 server.Init() } 构建 需要在工程根目录下新建一个Dockerfile\n内容如下：\n# 拉取Go语言的版本 FROM golang:1.16 # 在容器内设置工作目录 WORKDIR /app # 把文件复制到当前工作目录 COPY . . FROM alpine:latest as prod # 设置GOPROXY的环境变量 ENV GOPROXY=\u0026quot;https://goproxy.cn\u0026quot; # 编译项目 #RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o ./bin/go_http_server_demo ./src/main/main.go ADD ./bin/go_http_server_demo ./ # 暴露端口 EXPOSE 8080 # 可执行文件 CMD [\u0026quot;./go_http_server_demo\u0026quot;] 然后到根目录下编译项目，注意编译时要根据当前机器的类型设置环境变量：\nCGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o ./bin/go_http_server_demo ./src/main/main.go bin目录下就会多个一个 go_http_server_demo 文件。\n⇒ docker build -t go_http_server_demo . [+] Building 5.6s (7/7) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 503B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/alpine:latest 5.4s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 70B 0.0s =\u0026gt; CACHED [prod 1/2] FROM docker.io/library/alpine:latest@sha256:234cb88d3020898631af0ccbbcca9a66ae7306ecd30c9720690858c1b007d2a0 0.0s =\u0026gt; [prod 2/2] ADD ./bin/go_http_server_demo ./ 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:ee46f381509fe33f160de563c185b368dcd200adddcb9b663a6656f75b767621 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/go_http_server_demo 0.0s 没有报错说明镜像构建完成。\n⇒ docker images REPOSITORY TAG IMAGE ID CREATED SIZE go_http_server_demo latest ee46f381509f 47 minutes ago 11.7MB 启动 docker run --name go_http_server_demo -p 8080:8080 -d go_http_server_demo docker ps # 查看启动情况 \u0026ndash;name go_http_server_demo 指定container的名称 -p 8080:8080 将本地的8080端口映射到容器的8080端口 -d 指定容器后台运行，执行后当前终端可以退出 go_http_server_demo 指定镜像名称 ⇒ docker exec -it 105fdcefe795 sh / # ps PID USER TIME COMMAND 1 root 0:00 ./go_http_server_demo 12 root 0:00 sh 19 root 0:00 ps / # 可以登录到容器中查看容器的运行情况。\ncurl -X GET \u0026quot;http://127.0.0.1:8080/hello_world\u0026quot; 最后可以在本机访问接口测试容器是否运行正常。\n",
      "summary": "摘要：Docker构建Go工程镜像。\n工程 工程是一个比较简单的Http server的demo，现在将这个工程构建为docker镜像。\nimport ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net/http\u0026quot; ) func Init() { log.Println(\u0026quot;start server\u0026quot;) http.HandleFunc(\u0026quot;/hello_world\u0026quot;, HelloWorld) http.Handle(\u0026quot;/test_handle\u0026quot;, \u0026amp;TestHandleStruct{content: \u0026quot;test handle\u0026quot;}) if err := http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil); err != nil { log.Println(\u0026quot;start server on 8080\u0026quot;) } log.Fatal(\u0026quot;start server failed.\u0026quot;) } func main() { // 启动HTTP服务 server.Init() } 构建 需要在工程根目录下新建一个Dockerfile\n内容如下：\n# 拉取Go语言的版本 FROM golang:1.16 # 在容器内设置工作目录 WORKDIR /app # 把文件复制到当前工作目录 COPY . . FROM alpine:latest as prod # 设置GOPROXY的环境变量 ENV GOPROXY=\u0026quot;https://goproxy.cn\u0026quot; # 编译项目 #RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o ."
    },{
      "title": "Docker入门",
      "url": "https://qiref.github.io/post/2021/06/21/docker%E5%85%A5%E9%97%A8/",
      "content": "摘要：Docker简单使用。\nDocker 是一个开源的应用容器引擎，基于 Go 语言开发，并遵从 Apache2.0 协议开源。 Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。 容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。\nDocker下载地址：https://www.docker.com/get-started\n安装完成后，才可以执行docker的相关命令。\n$ docker system info Client: Context: default Debug Mode: false Plugins: buildx: Build with BuildKit (Docker Inc., v0.5.1-docker) compose: Docker Compose (Docker Inc., 2.0.0-beta.3) scan: Docker Scan (Docker Inc., v0.8.0) Server: Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 20.10.7 Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true userxattr: false Logging Driver: json-file Cgroup Driver: cgroupfs Cgroup Version: 1 Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: inactive Runtimes: runc io.containerd.runc.v2 io.containerd.runtime.v1.linux Default Runtime: runc Init Binary: docker-init containerd version: d71fcd7d8303cbf684402823e425e9dd2e99285d runc version: b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7 init version: de40ad0 Security Options: seccomp Profile: default Kernel Version: 5.10.25-linuxkit Operating System: Docker Desktop OSType: linux Architecture: x86_64 CPUs: 6 Total Memory: 1.941GiB Name: docker-desktop ID: TA2L:IL5R:BGA2:7NS2:RE5C:LOZ6:7RHP:275V:F5OQ:KAJP:QHWN:VOQU Docker Root Dir: /var/lib/docker Debug Mode: false HTTP Proxy: http.docker.internal:3128 HTTPS Proxy: http.docker.internal:3128 Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Live Restore Enabled: false mac上停止docker服务：\n⇒ launchctl list |grep docker - 0 com.docker.helper 11963 0 application.com.docker.docker.10740542.10740926 ⇒ launchctl stop application.com.docker.docker.10740542.10740926 运行hello world容器 $ docker run busybox echo \u0026quot;hello world\u0026quot; Unable to find image 'busybox:latest' locally latest: Pulling from library/busybox b71f96345d44: Pull complete Digest: sha256:930490f97e5b921535c153e0e7110d251134cc4b72bbb8133c6a5065cc68580d Status: Downloaded newer image for busybox: docker run busybox echo \u0026quot;hello world\u0026quot; 这行命令启动了一个叫busybox的容器，并且执行了echo \u0026quot;hello world\u0026quot;。\n然后docker先在本地查询busybox，发现没有找到，然后就从远程拉取镜像，拉取完成后启动镜像。\n⇒ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest 69593048aa3a 13 days ago 1.24MB 可以发现本地多了一个busybox的镜像。\n镜像 docker image ls 可以列出本地的所有镜像，然后具体看看镜像信息。\nREPOSITORY TAG IMAGE ID CREATED SIZE busybox latest 69593048aa3a 13 days ago 1.24MB 各个选项说明:\nREPOSITORY：表示镜像的仓库源 TAG：镜像的标签 IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小 同一仓库源可以有多个TAG，代表这个仓库源的不同个版本，如 ubuntu 仓库源里，有 15.10、14.04 等多个不同的版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。 当镜像没有显式指定TAG时，Docker会默认指定为latest。例如上述的镜像。\n如果想run指定版本的镜像：\ndocker run \u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; 镜像常用命令 $ docker images # 镜像列表 等价于 docker image ls $ docker search httpd # 查找镜像 $ docker pull httpd # 拉取镜像 $ docker push httpd # 向镜像仓库推送镜像 $ docker run httpd # 运行镜像 $ docker rmi httpd # 删除镜像 $ docker tag 860c279d2fec runoob/centos:dev #执行这条命令不会得到新镜像，只会给之前的镜像加一个TAG： #REPOSITORY TAG IMAGE ID CREATED SIZE #busybox latest 69593048aa3a 13 days ago 1.24MB #busybox v1.0 69593048aa3a 13 days ago 1.24MB $ docker commit -m=\u0026quot;has update\u0026quot; -a=\u0026quot;Archie\u0026quot; ac68f5f16776 busybox:v2 # 更新镜像，需要借助container # -m: 提交的描述信息； # -a: 指定镜像作者 # ac68f5f16776：容器 ID # busybox:v2：指定要创建的目标镜像名以及TAG $ docker build -t go_http_server_demo . # 构建镜像，前提是当前目录下有Dockerfile，-t go_http_server_demo 指定image的名称，. 基于当前目录构建。 容器 如果把镜像比作calss的话，那容器就是这个class的实例对象。\n⇒ docker run --name busybox -it busybox / # ls bin dev etc home proc root sys tmp usr var / # exit docker run 命令从指定镜像运行容器，-i: 交互式操作；-t: 终端。要退出终端，直接输入 exit；\u0026ndash;name busybox 指定容器的名称。\ndocker run --name go_http_server_demo -p 8080:8080 -d go_http_server_demo \u0026ndash;name 可以指定container的名称；-p 8080:8080 会将本地的端口8080映射到容器的8080端口； -d 表示这个容器会进入后台运行； 最后指定了镜像的名称。\ndocker ps -a 查看运行的所有容器。\n⇒ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 33e3123ff520 busybox \u0026quot;sh\u0026quot; 17 seconds ago Exited (0) 2 seconds ago busybox 如果指定\u0026ndash;name，下次再执行run时，会出现如下错误：\ndocker: Error response from daemon: Conflict. The container name \u0026quot;/busybox\u0026quot; is already in use by container \u0026quot;33e3123ff520985d12eed46e31051bf2b393fe45fa0e353d5ffa16ab835e940c\u0026quot;. You have to remove (or rename) that container to be able to reuse that name. 因为已经指定过这个name，并且这个容器已经启动过，如果还想启动这个容器，那只需要再启动这个容器即可，无需执行run。\n容器常用命令 $ docker run --name busybox -it busybox # 从镜像中启动一个容器 $ docker ps -a # 列出所有容器。包括已经停止的 $ docker ps # 列出正在运行的容器 $ docker stop \u0026lt;容器 ID\u0026gt; # 停止一个容器 $ docker start \u0026lt;容器 ID\u0026gt; # 启动一个已停止的容器 $ docker restart \u0026lt;容器 ID\u0026gt; # 重启容器 $ docker attach f59fcdac1cc0 # 进入容器，退出时会导致容器的退出 $ docker exec -it 33e3123ff520 sh # 进入容器，退出时不会导致容器的退出,sh 为run容器时指定的COMMAND $ docker container rm -f 33e3123ff520 # 删除容器 ",
      "summary": "摘要：Docker简单使用。\nDocker 是一个开源的应用容器引擎，基于 Go 语言开发，并遵从 Apache2.0 协议开源。 Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。 容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。\nDocker下载地址：https://www.docker.com/get-started\n安装完成后，才可以执行docker的相关命令。\n$ docker system info Client: Context: default Debug Mode: false Plugins: buildx: Build with BuildKit (Docker Inc., v0.5.1-docker) compose: Docker Compose (Docker Inc., 2.0.0-beta.3) scan: Docker Scan (Docker Inc., v0.8.0) Server: Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 20.10.7 Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true userxattr: false Logging Driver: json-file Cgroup Driver: cgroupfs Cgroup Version: 1 Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: inactive Runtimes: runc io."
    },{
      "title": "Kubernates组件",
      "url": "https://qiref.github.io/post/2021/06/21/kubernates%E7%BB%84%E4%BB%B6/",
      "content": "摘要：Kubernates基础概念及其组件。\nKubernetes是一个开源的容器编排引擎，用来对容器化应用进行自动化部署、扩缩容和管理，简称K8s，K8s 这个缩写是因为k和s之间有八个字符。Google 在 2014 年开源了 Kubernetes 项目。\n控制面组件 kube-apiserver apiserver是 Kubernetes 控制面的组件， 该组件公开了 Kubernetes API。\netcd etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。\nkube-scheduler 负责监视新创建的、未指定运行节点（node）的 Pods，选择节点让 Pod 在上面运行。\n调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。\nkube-controller-manager 从逻辑上讲，每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。\n这些控制器包括:\n节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应 任务控制器（Job controller）: 监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成 端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod) 服务帐户和令牌控制器（Service Account \u0026amp; Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌 cloud-controller-manager 云控制器管理器是指嵌入特定云的控制逻辑的 控制平面组件。 云控制器管理器允许您链接集群到云提供商的应用编程接口中， 并把和该云平台交互的组件与只和您的集群交互的组件分离开。 cloud-controller-manager 仅运行特定于云平台的控制回路。 如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境， 所部署的环境中不需要云控制器管理器。\n与 kube-controller-manager 类似，cloud-controller-manager 将若干逻辑上独立的 控制回路组合到同一个可执行文件中，供你以同一进程的方式运行。 你可以对其执行水平扩容（运行不止一个副本）以提升性能或者增强容错能力。\n下面的控制器都包含对云平台驱动的依赖：\n节点控制器（Node Controller）: 用于在节点终止响应后检查云提供商以确定节点是否已被删除 路由控制器（Route Controller）: 用于在底层云基础架构中设置路由 服务控制器（Service Controller）: 用于创建、更新和删除云提供商负载均衡器 Node 组件 维护运行的 Pod 并提供 Kubernetes 运行环境。\nkubelet 一个在集群中每个节点（node）上运行的代理。 它保证容器（containers）都 运行在 Pod 中。\nkubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。\nkube-proxy kube-proxy 是集群中每个节点上运行的网络代理， 实现 Kubernetes 服务（Service） 概念的一部分。\nkube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。\n如果操作系统提供了数据包过滤层并可用的话，kube-proxy 会通过它来实现网络规则。否则， kube-proxy 仅转发流量本身。\n来源：https://kubernetes.io/zh/docs/concepts/overview/components/\n",
      "summary": "摘要：Kubernates基础概念及其组件。\nKubernetes是一个开源的容器编排引擎，用来对容器化应用进行自动化部署、扩缩容和管理，简称K8s，K8s 这个缩写是因为k和s之间有八个字符。Google 在 2014 年开源了 Kubernetes 项目。\n控制面组件 kube-apiserver apiserver是 Kubernetes 控制面的组件， 该组件公开了 Kubernetes API。\netcd etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。\nkube-scheduler 负责监视新创建的、未指定运行节点（node）的 Pods，选择节点让 Pod 在上面运行。\n调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。\nkube-controller-manager 从逻辑上讲，每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。\n这些控制器包括:\n节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应 任务控制器（Job controller）: 监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成 端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod) 服务帐户和令牌控制器（Service Account \u0026amp; Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌 cloud-controller-manager 云控制器管理器是指嵌入特定云的控制逻辑的 控制平面组件。 云控制器管理器允许您链接集群到云提供商的应用编程接口中， 并把和该云平台交互的组件与只和您的集群交互的组件分离开。 cloud-controller-manager 仅运行特定于云平台的控制回路。 如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境， 所部署的环境中不需要云控制器管理器。\n与 kube-controller-manager 类似，cloud-controller-manager 将若干逻辑上独立的 控制回路组合到同一个可执行文件中，供你以同一进程的方式运行。 你可以对其执行水平扩容（运行不止一个副本）以提升性能或者增强容错能力。"
    },{
      "title": "Go语言实现httpServer",
      "url": "https://qiref.github.io/post/2021/06/19/go%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0httpserver/",
      "content": "摘要：使用Go语言原生包实现Http Server。\n启动一个Http Server 使用Go语言原生的net/http库可以很简单实现一个http server。\nlog.Println(\u0026quot;start server\u0026quot;) if err := http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil); err != nil { log.Println(\u0026quot;start server on 8080\u0026quot;) } log.Fatal(\u0026quot;start server failed.\u0026quot;) 没错，只要这么几行代码，就开启了一个http server，监听8080端口。\n接收Http请求 http.HandleFunc 开启了Http server后，无法处理Http请求这个就是个空的Server，下面给它加上处理Http Request的能力。\nfunc init() { log.Println(\u0026quot;start server\u0026quot;) http.HandleFunc(\u0026quot;/hello_world\u0026quot;, HelloWorld) if err := http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil); err != nil { log.Println(\u0026quot;start server on 8080\u0026quot;) } log.Fatal(\u0026quot;start server failed.\u0026quot;) } func HelloWorld(w http.ResponseWriter, r *http.Request) { _, err := w.Write([]byte(\u0026quot;hello world\u0026quot;)) if err != nil { log.Println(err) } } http.HandleFunc(\u0026quot;/hello_world\u0026quot;, HelloWorld) 这行代码指定了一个路由对应的方法，然后访问http://127.0.0.1:8080/hello_world 可以得到hello world的字符串，这段字符串也是HelloWorld(w http.ResponseWriter, r *http.Request) 函数的输出。\nhttp.Handle func init() { log.Println(\u0026quot;start server\u0026quot;) http.HandleFunc(\u0026quot;/hello_world\u0026quot;, HelloWorld) http.Handle(\u0026quot;/test_handle\u0026quot;, \u0026amp;TestHandleStruct{content: \u0026quot;test handle\u0026quot;}) if err := http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil); err != nil { log.Println(\u0026quot;start server on 8080\u0026quot;) } log.Fatal(\u0026quot;start server failed.\u0026quot;) } type TestHandleStruct struct { content string } // 实现 Handler interface func (handle *TestHandleStruct) ServeHTTP(w http.ResponseWriter, r *http.Request) { _, err := fmt.Fprintf(w, handle.content) if err != nil { log.Fatal(\u0026quot;response failed\u0026quot;) } } 还有一种方式可以实现处理http request，就是实现Handle接口，实现接口需要借助TestHandleStruct结构体去实现，定义结构体的方法实现接口方法ServeHTTP(w http.ResponseWriter, r *http.Request)\n然后访问http://127.0.0.1:8080/test_handle 就可以得到test handle字符串。\n其实这两种实现方式底层都是一样的：\n// Handle registers the handler for the given pattern // in the DefaultServeMux. // The documentation for ServeMux explains how patterns are matched. func Handle(pattern string, handler Handler) { DefaultServeMux.Handle(pattern, handler) } // HandleFunc registers the handler function for the given pattern // in the DefaultServeMux. // The documentation for ServeMux explains how patterns are matched. func HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { DefaultServeMux.HandleFunc(pattern, handler) } ",
      "summary": "摘要：使用Go语言原生包实现Http Server。\n启动一个Http Server 使用Go语言原生的net/http库可以很简单实现一个http server。\nlog.Println(\u0026quot;start server\u0026quot;) if err := http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil); err != nil { log.Println(\u0026quot;start server on 8080\u0026quot;) } log.Fatal(\u0026quot;start server failed.\u0026quot;) 没错，只要这么几行代码，就开启了一个http server，监听8080端口。\n接收Http请求 http.HandleFunc 开启了Http server后，无法处理Http请求这个就是个空的Server，下面给它加上处理Http Request的能力。\nfunc init() { log.Println(\u0026quot;start server\u0026quot;) http.HandleFunc(\u0026quot;/hello_world\u0026quot;, HelloWorld) if err := http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil); err != nil { log.Println(\u0026quot;start server on 8080\u0026quot;) } log.Fatal(\u0026quot;start server failed.\u0026quot;) } func HelloWorld(w http.ResponseWriter, r *http.Request) { _, err := w.Write([]byte(\u0026quot;hello world\u0026quot;)) if err !"
    },{
      "title": "Go语言反射",
      "url": "https://qiref.github.io/post/2021/06/17/go%E8%AF%AD%E8%A8%80%E5%8F%8D%E5%B0%84/",
      "content": "摘要：理解Go语言的反射机制，反射是指在程序运行期对程序本身进行访问和修改的能力。\n反射基础信息 func reflectDemo() { str := \u0026quot;reflect\u0026quot; fmt.Println(reflect.ValueOf(str)) fmt.Println(reflect.TypeOf(str)) } // 结果 // reflect // string reflect.ValueOf()获取数据运行时的值。 reflect.TypeOf()获取数据类型信息。\n// Type values are comparable, such as with the == operator, // so they can be used as map keys. // Two Type values are equal if they represent identical types. type Type interface { // To compare two Values, compare the results of the Interface method. // Using == on two Values does not compare the underlying values // they represent. type Value struct { 从源码中可以看出，Type是个interface，而Value是个struct。\nreflect包中除了有TypeOf还有kind，type是具体的类型信息，kind是类型的种类，基本数据类型type和kind是一样的。\nfmt.Println(reflect.TypeOf(dog{})) typeOf := reflect.TypeOf(dog{}) fmt.Println(typeOf.Kind()) // reflect_demo.dog // struct 反射指针元素 func reflectDemo1() { list := \u0026amp;ListNode{} fmt.Println(\u0026quot;reflect.TypeOf \u0026quot;, reflect.TypeOf(list)) fmt.Println(\u0026quot;reflect value \u0026quot;, reflect.ValueOf(list)) fmt.Println(\u0026quot;reflect kind \u0026quot;, reflect.TypeOf(list).Kind()) } type ListNode struct { val int next *ListNode } // 结果： // reflect.TypeOf *reflect_demo.ListNode // reflect value \u0026amp;{0 \u0026lt;nil\u0026gt;} // reflect kind ptr ptr 是指针类型。 示例中的指针是个结构体指针，通过反射机制可以获取到结构体中的字段信息。\nfunc reflectDemo2() { list := ListNode{val: 12, next: nil} typeOf := reflect.TypeOf(list) for i := 0; i \u0026lt; typeOf.NumField(); i++ { fieldType := typeOf.Field(i) fmt.Println(\u0026quot;fileName: \u0026quot;, fieldType.Name, \u0026quot; fileType: \u0026quot;, fieldType.Type, \u0026quot; fieldTag: \u0026quot;, fieldType.Tag) } } type ListNode struct { Name string `json:\u0026quot;Name\u0026quot;` val int next *ListNode } // 输出 // fileName: Name fileType: string fieldTag: json:\u0026quot;name\u0026quot; // fileName: val fileType: int fieldTag: // fileName: next fileType: *reflect_demo.ListNode fieldTag: typeOf.Field(i) 可以获取结构体中的字段信息，注意这里的list不能是指针类型，why？ 因为这是个指针，不是结构体，可以通过Elem（）方法获取指向的元素。\nlist := \u0026amp;ListNode{val: 12, next: nil} typeOf := reflect.TypeOf(list) if typeOf.Kind() == reflect.Ptr { for i := 0; i \u0026lt; typeOf.Elem().NumField(); i++ { fieldType := typeOf.Elem().Field(i) fmt.Println(\u0026quot;fileName: \u0026quot;, fieldType.Name, \u0026quot; fileType: \u0026quot;, fieldType.Type, \u0026quot; fieldTag: \u0026quot;, fieldType.Tag) } } 还有一种方式，通过value也可以获取元素的kind等信息。\nfunc reflectDemo3() { list := \u0026amp;ListNode{val: 12, next: nil} indirect := reflect.Indirect(reflect.ValueOf(list)) if indirect.Kind() == reflect.Ptr { fmt.Println(\u0026quot;Ptr\u0026quot;) } if indirect.Kind() == reflect.Struct { fmt.Println(\u0026quot;struct\u0026quot;) } } // 输出 // struct ",
      "summary": "摘要：理解Go语言的反射机制，反射是指在程序运行期对程序本身进行访问和修改的能力。\n反射基础信息 func reflectDemo() { str := \u0026quot;reflect\u0026quot; fmt.Println(reflect.ValueOf(str)) fmt.Println(reflect.TypeOf(str)) } // 结果 // reflect // string reflect.ValueOf()获取数据运行时的值。 reflect.TypeOf()获取数据类型信息。\n// Type values are comparable, such as with the == operator, // so they can be used as map keys. // Two Type values are equal if they represent identical types. type Type interface { // To compare two Values, compare the results of the Interface method. // Using == on two Values does not compare the underlying values // they represent."
    },{
      "title": "Go语言defer、panic、recover",
      "url": "https://qiref.github.io/post/2021/06/15/go%E8%AF%AD%E8%A8%80deferpanicrecover/",
      "content": "摘要：理解Go语言defer、panic、recover。\ndefer Go 语言的 defer 会在当前函数返回前执行传入的函数，它会经常被用于关闭文件描述符、关闭数据库连接以及解锁资源，总结一句话就是完成函数执行完的收尾工作。\nfunc DeferDemo() { defer fmt.Println(\u0026quot;this is defer println\u0026quot;) fmt.Println(\u0026quot;this is println\u0026quot;) } // 输出 // this is println // this is defer println 运行以上代码每次都是第二个println先输出，然后才是defer关键字修饰的println输出。\n如果有多个defer，输出顺序又会如何？\nfunc MultiDeferDemo() { for i := 0; i \u0026lt; 5; i++ { defer fmt.Println(\u0026quot; defer \u0026quot;, i) } } // 输出 // defer 4 // defer 3 // defer 2 // defer 1 // defer 0 每次最先输出的都是循环的最后一个println，可以得出：多个defer，运行顺序遵循LIFO规则。\ndefer的值传递问题。\n基于defer的机制，可以用来统计函数的执行耗时。\nfunc MethodElapsedTime() { start := time.Now() defer fmt.Println(\u0026quot;elapsed \u0026quot;, time.Since(start)) time.Sleep(time.Second * 2) } // 输出 // elapsed 169ns 上述代码的运行结果并不会和预期一致，调用 defer 关键字会立刻拷贝函数中引用的外部参数，所以 time.Since(start) 的结果不是在 main 函数退出之前计算的，而是在 defer 关键字调用时计算的。\n如果想要达到预期结果，可以给defer传入匿名函数，这样调用defer关键字时，虽然也会进行参数拷贝，但是拷贝的是函数指针，并不是匿名函数的参数，这样就能拿到正确的参数。\nfunc MethodElapsedTime1() { start := time.Now() defer func() { fmt.Println(\u0026quot;elapsed \u0026quot;, time.Since(start)) }() time.Sleep(time.Second * 2) } // 输出 // elapsed 2.001068253s panic When youpanicin Go, you’re freaking out, it’s not someone elses problem, it’s game over man.\n从这句话可以得出，Panic是Go中的严重错误，会影响到程序的运行。\n当panic异常发生时，程序会中断运行，并立即执行在该goroutine中被延迟的函数（defer 机制）,随后，程序崩溃并输出日志信息。日志信息包括panic value和函数调用的堆栈跟踪信息。\nfunc PanicDemo() { defer fmt.Println(\u0026quot;defer println\u0026quot;) go func() { defer fmt.Println(\u0026quot;goroutine defer println\u0026quot;) panic(\u0026quot;\u0026quot;) }() time.Sleep(time.Second * 2) } // 运行结果 // goroutine defer println // panic: // //goroutine 7 [running]: // archieyao.github.com/base/src/panic_demo.PanicDemo.func1() // /Users/archieyao/GoProjects/GoMod/base/src/panic_demo/panic_demo.go:16 +0x95 // created by archieyao.github.com/base/src/panic_demo.PanicDemo // /Users/archieyao/GoProjects/GoMod/base/src/panic_demo/panic_demo.go:14 +0x98 以上示例可以很好演示Panic的运行流程，在运行goroutine的匿名函数时，遇到了Panic，此时程序会先运行goroutine内的defer修饰的代码，然后输出崩溃日志，其中，最外层的 defer fmt.Println(\u0026quot;defer println\u0026quot;) 并未执行。\n当panic不在goroutine中出现时，例如以下示例：\nfunc PanicDemo1() { defer fmt.Println(\u0026quot;defer println 1\u0026quot;) panic(\u0026quot;panic\u0026quot;) defer fmt.Println(\u0026quot;defer println 2\u0026quot;) } 此时程序运行的结果，会运行Panic前的defer，并且会运行Panic前的所有defer。\n基于Panic的特性，当程序遇到Panic时，会运行Panic前的defer，如果在defer中构建匿名函数，在函数中再次Panic，就可以形成Panic嵌套。\nfunc panicDemo2() { defer func() { defer func() { panic(\u0026quot;panic 3\u0026quot;) }() panic(\u0026quot;panic 2\u0026quot;) }() panic(\u0026quot;panic 1\u0026quot;) } // panic: panic 1 // panic: panic 2 // panic: panic 3 [recovered] // panic: panic 3 当多个Panic嵌套时，如果Panic都需要被执行的defer中，那每个Panic都会执行。\nrecover recover一般都是用于恢复Panic，让程序崩溃后继续运行，类似于其他语言中的异常处理，当异常抛出后程序奔溃，但当捕获异常并处理后，程序不会崩溃。\nfunc recoverDemo1() { catchErr() fmt.Println(\u0026quot;after recover println\u0026quot;) } func catchErr() { defer fmt.Println(\u0026quot;defer println\u0026quot;) defer func() { if err := recover(); err != nil { fmt.Println(\u0026quot;recover\u0026quot;) } }() panic(\u0026quot;panic\u0026quot;) } // 运行结果 // recover // defer println // after recover println recover一般都是在defer中运行，常用写法如下：\nfunc simpleRecover() { defer func() { if r := recover(); r != nil { fmt.Println(\u0026quot;recover\u0026quot;) } }() panic(\u0026quot;panic\u0026quot;) // 注意 这行不会执行 fmt.Println(\u0026quot;bala\u0026quot;) } 一般在defer修饰的匿名函数中recover，并且加入判断，是否已经捕获到panic；值得注意的是，上述代码中，fmt.Println(\u0026quot;bala\u0026quot;) 这行是不会执行的，因为在simpleRecover()函数中，已经发生了Panic，程序已经中断了，会跳过后续的代码，然后去执行panic前的defer代码，然后在defer中恢复，此时虽然程序已经恢复到正常运行状态，但历史由于panic跳过的代码是无法回溯的。\nrecover() 的作用范围仅限于当前的所属 goroutine。发生 panic 时只会执行当前协程中的defer函数，其它协程里面的 defer 不会执行。\nfunc simpleRecover() { defer func() { if r := recover(); r != nil { fmt.Println(\u0026quot;recover\u0026quot;) } }() go func() { panic(\u0026quot;panic\u0026quot;) }() time.Sleep(time.Second*2) fmt.Println(\u0026quot;bala\u0026quot;) } 以上代码中，由于panic是在新开启的goroutine中执行，recover是无法恢复这个goroutine中的panic，所以上述代码依然会崩溃。\nfunc simpleRecover() { go func() { defer func() { if r := recover(); r != nil { fmt.Println(\u0026quot;recover\u0026quot;) } }() panic(\u0026quot;panic\u0026quot;) }() time.Sleep(time.Second*2) fmt.Println(\u0026quot;bala\u0026quot;) } 如果把defer也放到新开启的goroutine中，就可以正常recover这个panic。此时代码也会正常往后运行，fmt.Println(\u0026quot;bala\u0026quot;) 这行也会输出，因为goroutine中panic已经恢复，不会跳过外层函数的代码。\n参考：\nhttps://draveness.me/golang/docs/part2-foundation/ch05-keyword/golang-defer/\nhttps://segmentfault.com/a/1190000021141276\n",
      "summary": "摘要：理解Go语言defer、panic、recover。\ndefer Go 语言的 defer 会在当前函数返回前执行传入的函数，它会经常被用于关闭文件描述符、关闭数据库连接以及解锁资源，总结一句话就是完成函数执行完的收尾工作。\nfunc DeferDemo() { defer fmt.Println(\u0026quot;this is defer println\u0026quot;) fmt.Println(\u0026quot;this is println\u0026quot;) } // 输出 // this is println // this is defer println 运行以上代码每次都是第二个println先输出，然后才是defer关键字修饰的println输出。\n如果有多个defer，输出顺序又会如何？\nfunc MultiDeferDemo() { for i := 0; i \u0026lt; 5; i++ { defer fmt.Println(\u0026quot; defer \u0026quot;, i) } } // 输出 // defer 4 // defer 3 // defer 2 // defer 1 // defer 0 每次最先输出的都是循环的最后一个println，可以得出：多个defer，运行顺序遵循LIFO规则。\ndefer的值传递问题。\n基于defer的机制，可以用来统计函数的执行耗时。"
    },{
      "title": "Go语言包管理",
      "url": "https://qiref.github.io/post/2021/06/10/go%E8%AF%AD%E8%A8%80%E5%8C%85%E7%AE%A1%E7%90%86/",
      "content": "摘要：Go语言包管理。\n包使用规范 包的习惯用法：\n包名一般是小写的，使用一个简短且有意义的名称。 包名一般要和所在的目录同名，也可以不同，包名中不能包含- 等特殊符号。 包一般使用域名作为目录名称，这样能保证包名的唯一性，比如 GitHub 项目的包一般会放到GOPATH/src/github.com/userName/projectName 目录下。 包名为 main 的包为应用程序的入口包，编译不包含 main 包的源码文件时不会得到可执行文件。 一个文件夹下的所有源码文件只能属于同一个包，同样属于同一个包的源码文件不能放在多个文件夹下。 Go 语言中，所有的定义，比如函数、变量、结构体等，如果首字母是大写，那么就可以被其他包使用；同一包下，不存在引用问题。\n基于包的封装 在Go语言中封装就是把抽象出来的字段和对字段的操作封装在一起，数据被保护在内部，程序的其它包只能通过被授权的方法，才能对字段进行操作。\n封装的好处： 隐藏实现细节； 可以对数据进行验证，保证数据安全合理。\n封装的实现步骤：\n将结构体、字段的首字母小写； 给结构体所在的包提供一个工厂模式的函数，首字母大写，类似一个构造函数； 提供一个首字母大写的 Set 方法（类似其它语言的 public），用于对属性判断并赋值； 提供一个首字母大写的 Get 方法（类似其它语言的 public），用于获取属性的值。 包的初始化 每个包都允许有一个 init 函数，当这个包被导入时，会执行该包的这个 init 函数，做一些初始化任务。 对于 init 函数的执行有两点需要注意:\ninit 函数优先于 main 函数执行 在一个包引用链中，包的初始化是深度优先的。比如，有这样一个包引用关系：main→A→B→C，那么初始化顺序为 C.init→B.init→A.init→main 封装引用实例 建立如下工程结构，在main包中需要访问model包中的内容。\nproject |---src |---main -main.go |---model -student.go student.go\ntype student struct { Name string idCardNum string // 私有，外部包不可访问 Age int8 } func NewStudent(stuName string, age int8) *student { return \u0026amp;student{ Name: stuName, Age: age, } } // 定义结构体方法 func (stu *student) SetIdCardNum(idCN string) { stu.idCardNum = idCN } // 定义结构体方法 func (stu *student) GetIdCardNum() string { return stu.idCardNum } main.go\n// 引入model包 import \u0026quot;../model\u0026quot; func main() { stu := model.NewStudent(\u0026quot;张三\u0026quot;,34) stu.SetIdCardNum(\u0026quot;42093222324234\u0026quot;) fmt.Println(*stu) } 运行程序可能会遇到以下错误：\nbuild command-line-arguments: cannot find module for xxxx\n该错误与go环境变量GO111MODULE相关：\nGO111MODULE=off 无模块支持,go 会从 GOPATH 和 vendor 文件夹寻找包 GO111MODULE=on 模块支持,go 会忽略GOPATH 和 vendor 文件夹,只根据 go.mod 下载依赖 GO111MODULE=auto 在 $GOPATH/src 外面且根目录有 go.mod 文件时，开启模块支持 在使用模块的时候，GOPATH 是无意义的,不过它还是会把下载的依赖储存在 $GOPATH/src/mod 中,也会把 go install 的结果放在 $GOPATH/bin 中 设置方式：\ngo env -w GO111MODULE=auto go env 在main中调用发现只能访问student结构体的Name和Age字段，idCardNum需要通过get方法获取，这样就达到包的权限控制效果。\n包引用 包引用可使用相对路径和绝对路径；引用时也可以对包设置alias；还可以匿名引用。\n包省略前缀 import ( . \u0026quot;fmt\u0026quot; ) 这个点的含义就是这个包导入之后在你调用这个包的函数时，你可以省略前缀的包名，也就是前面你调用的 fmt.Println (\u0026ldquo;hello world\u0026rdquo;) 可以省略的写成 Println (\u0026ldquo;hello world\u0026rdquo;)\n包设置alias import model1 \u0026quot;../model\u0026quot; func main() { stu := model1.NewStudent(\u0026quot;张三\u0026quot;, 34) stu.SetIdCardNum(\u0026quot;42093222324234\u0026quot;) } 包匿名引用 import _ \u0026quot;../model\u0026quot; 注意匿名引用的包并不能直接使用其中的变量和方法，不使用匿名引入的情况下，如果引入了一个未使用的包会导致编译错误，但使用匿名引入包不会导致编译错误。\nimport ( \u0026quot;database/sql\u0026quot; _ \u0026quot;github.com/lib/pq\u0026quot; // enable support for Postgres _ \u0026quot;github.com/go-sql-driver/mysql\u0026quot; // enable support for MySQL ) db, err = sql.Open(\u0026quot;postgres\u0026quot;, dbname) // OK db, err = sql.Open(\u0026quot;mysql\u0026quot;, dbname) // OK db, err = sql.Open(\u0026quot;sqlite3\u0026quot;, dbname) // returns error: unknown driver \u0026quot;sqlite3\u0026quot; 导入一个包，只想执行包里的 init 函数，来运行一些初始化任务，此时也可以使用匿名导入。\n绝对路径引入 基于以上工程目录结构，也可以使用绝对路径引入，绝对路径是从 $GOPATH/src 或 $GOROOT 或者 $GOPATH/pkg/mod 目录下搜索包并导入。\nimport \u0026quot;model\u0026quot; 注意绝对路径引入需要保证GOPATH在当前目录下，使用goland也可以设置当前工程的GOPATH。\n相对路径引入 相对路径是从当前目录开始。\nmain包中引用如下：\nimport \u0026quot;../model\u0026quot; 参考：\nhttps://juejin.cn/post/6844904167073382408 http://c.biancheng.net/view/91.html\n",
      "summary": "摘要：Go语言包管理。\n包使用规范 包的习惯用法：\n包名一般是小写的，使用一个简短且有意义的名称。 包名一般要和所在的目录同名，也可以不同，包名中不能包含- 等特殊符号。 包一般使用域名作为目录名称，这样能保证包名的唯一性，比如 GitHub 项目的包一般会放到GOPATH/src/github.com/userName/projectName 目录下。 包名为 main 的包为应用程序的入口包，编译不包含 main 包的源码文件时不会得到可执行文件。 一个文件夹下的所有源码文件只能属于同一个包，同样属于同一个包的源码文件不能放在多个文件夹下。 Go 语言中，所有的定义，比如函数、变量、结构体等，如果首字母是大写，那么就可以被其他包使用；同一包下，不存在引用问题。\n基于包的封装 在Go语言中封装就是把抽象出来的字段和对字段的操作封装在一起，数据被保护在内部，程序的其它包只能通过被授权的方法，才能对字段进行操作。\n封装的好处： 隐藏实现细节； 可以对数据进行验证，保证数据安全合理。\n封装的实现步骤：\n将结构体、字段的首字母小写； 给结构体所在的包提供一个工厂模式的函数，首字母大写，类似一个构造函数； 提供一个首字母大写的 Set 方法（类似其它语言的 public），用于对属性判断并赋值； 提供一个首字母大写的 Get 方法（类似其它语言的 public），用于获取属性的值。 包的初始化 每个包都允许有一个 init 函数，当这个包被导入时，会执行该包的这个 init 函数，做一些初始化任务。 对于 init 函数的执行有两点需要注意:\ninit 函数优先于 main 函数执行 在一个包引用链中，包的初始化是深度优先的。比如，有这样一个包引用关系：main→A→B→C，那么初始化顺序为 C.init→B.init→A.init→main 封装引用实例 建立如下工程结构，在main包中需要访问model包中的内容。\nproject |---src |---main -main.go |---model -student.go student.go\ntype student struct { Name string idCardNum string // 私有，外部包不可访问 Age int8 } func NewStudent(stuName string, age int8) *student { return \u0026amp;student{ Name: stuName, Age: age, } } // 定义结构体方法 func (stu *student) SetIdCardNum(idCN string) { stu."
    },{
      "title": "Go语言channel",
      "url": "https://qiref.github.io/post/2021/06/09/go%E8%AF%AD%E8%A8%80channel/",
      "content": "摘要：Go语言中，协程之间通过channel相互通信，可以从一个Go协程将值发送到通道，然后在别的协程中接收。\nchannel 定义 定义channel的语法为：make(chan val-type)，val-type就是需要传递值的类型。 chan1 \u0026lt;- val 表示将val发送到channel chann1中， r := \u0026lt;- chann1表示从chann1中读取消息。\nfunc Ping(c *chan string, s string) { *c \u0026lt;- s } func Pong(c *chan string) string { return \u0026lt;-*c } // main func main() { c := make(chan string) go Ping(\u0026amp;c, \u0026quot;ping\u0026quot;) go func() { pong := Pong(\u0026amp;c) fmt.Println(pong) }() time.Sleep(time.Second * 2) } // 结果 // ping 需要注意的是，向通道中发送消息和从通道中接收消息，都是阻塞的，如果发送和接收不是成对出现，就会发生错误。 将上文中代码改成这样：\nc := make(chan string) Ping(\u0026amp;c, \u0026quot;ping\u0026quot;) //go func() { // pong := Pong(\u0026amp;c) // fmt.Println(pong) //}() time.Sleep(time.Second * 2) //fatal error: all goroutines are asleep - deadlock! channel方向 func pong(ping \u0026lt;-chan string, pong chan\u0026lt;- string) { msg := \u0026lt;-ping pong \u0026lt;- msg } 在chan的定义中，箭头的方向是固定的，\u0026lt;-箭头方向只能向左。\n\u0026lt;-chan 表示该channel只能用于接收消息，不能用其发送消息。 chan\u0026lt;- 表示该channel只能用于发送消息，不能用其接收消息。 channel缓冲 默认通道是 无缓冲 的，这意味着只有在对应的接收（\u0026lt;- chan）通道准备好接收时，才允许进行发送（chan \u0026lt;-）。可缓存通道允许在没有对应接收方的情况下，缓存限定数量的值。\nmessages := make(chan string, 2) messages \u0026lt;- \u0026quot;1\u0026quot; messages \u0026lt;- \u0026quot;2\u0026quot; make 构建一个channel时，可以指定缓冲区大小，当channel中超过2个元素时，就会报错。\nchannel同步 func work(done chan bool) { fmt.Println(\u0026quot;working ...\u0026quot;) time.Sleep(time.Second * 3) fmt.Println(\u0026quot;done\u0026quot;) done \u0026lt;- true } // main done := make(chan bool) go work(done) \u0026lt;-done // 输出 // working ... // done 程序将在接收到通道中 work() 发出的通知前一直阻塞，如果把 \u0026lt;- done 这行代码从序中移除，程序甚至会在work()还没开始运行时就结束了。\nchannel遍历 for 和 range为基本的数据结构提供了迭代的功能。我们也可以使用这个语法来遍历从通道中取得的值。\nfunc loop(c chan string) { fmt.Println(\u0026quot;range over chan start.\u0026quot;) for s := range c { fmt.Println(s) } fmt.Println(\u0026quot;range over chan end.\u0026quot;) } // main chanForRange := make(chan string, 3) chanForRange \u0026lt;- \u0026quot;l\u0026quot; chanForRange \u0026lt;- \u0026quot;m\u0026quot; chanForRange \u0026lt;- \u0026quot;n\u0026quot; close(chanForRange) loop(chanForRange) // 输出结果 // range over chan start. // l // m // n // range over chan end. 这里遍历需要关闭chanForRange，否则chanForRange会一直等待输入，但后续没有往channel中写入消息，会导致成型陷入死锁。\n可以看出，在channel关闭后，依然可以遍历channel。\nselect Go 语言中的 select 能够让 Goroutine 同时等待多个 Channel 可读或者可写，在多个文件或者 Channel状态改变之前，select 会一直阻塞当前线程或者 Goroutine。\nc1 := make(chan string) c2 := make(chan string) go func() { time.Sleep(time.Second * 1) c1 \u0026lt;- \u0026quot;1\u0026quot; }() go func() { time.Sleep(time.Second * 1) c2 \u0026lt;- \u0026quot;2\u0026quot; }() for i := 0; i \u0026lt; 2; i++ { select { case msg1 := \u0026lt;-c1: fmt.Println(i) fmt.Println(\u0026quot;receive msg1 : \u0026quot;, msg1) case msg2 := \u0026lt;-c2: fmt.Println(i) fmt.Println(\u0026quot;receive msg2 : \u0026quot;, msg2) } } // 运行结果： // 0 // receive msg1 : 1 // 1 // receive msg2 : 2 这里每一次循环都会进入一次select，然后会执行其中的一个case，如果没有进入case，程序就会出现死锁；因此这里的循环次数需要和channel发送消息的次数一致，因为select默认会阻塞。\nfor i := 0; i \u0026lt; 5; i++ { select { case msg1 := \u0026lt;-c1: fmt.Println(i) fmt.Println(\u0026quot;receive msg1 : \u0026quot;, msg1) case msg2 := \u0026lt;-c2: fmt.Println(i) fmt.Println(\u0026quot;receive msg2 : \u0026quot;, msg2) default: fmt.Println(\u0026quot;default\u0026quot;) } } select配上default之后，当case条件不满足时，select就不会陷入阻塞。\n多协程执行任务, 并收集执行结果 import ( \u0026quot;fmt\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;testing\u0026quot; \u0026quot;time\u0026quot; ) func TestMain(t *testing.T) { rstChan := make(chan map[string]int, 5) // 这里必须指定 chan 的容量 var wg sync.WaitGroup for i := 0; i \u0026lt; 5; i++ { i := i wg.Add(1) go func() { // 模拟执行任务 defer wg.Done() if i%2 == 0 { // 模拟任务执行失败的场景, 会出现不往 rstChan 写入消息的情况 m := make(map[string]int) m[fmt.Sprintf(\u0026quot;%d\u0026quot;, i)] = i time.Sleep(time.Second * 3) rstChan \u0026lt;- m } }() } fmt.Println(\u0026quot;wait\u0026quot;) wg.Wait() fmt.Println(\u0026quot;wait finish\u0026quot;) size := len(rstChan) // 提前读取 rstChan size, 消费数据 len(rstChan) 会改变 for j := 0; j \u0026lt; size; j++ { item := \u0026lt;-rstChan fmt.Println(item) } defer close(rstChan) fmt.Println(\u0026quot;done\u0026quot;) } // 输出 === RUN TestMain wait wait finish map[2:2] map[4:4] map[0:0] done --- PASS: TestMain (3.00s) PASS ok awesome-test/src/main 3.002s ",
      "summary": "摘要：Go语言中，协程之间通过channel相互通信，可以从一个Go协程将值发送到通道，然后在别的协程中接收。\nchannel 定义 定义channel的语法为：make(chan val-type)，val-type就是需要传递值的类型。 chan1 \u0026lt;- val 表示将val发送到channel chann1中， r := \u0026lt;- chann1表示从chann1中读取消息。\nfunc Ping(c *chan string, s string) { *c \u0026lt;- s } func Pong(c *chan string) string { return \u0026lt;-*c } // main func main() { c := make(chan string) go Ping(\u0026amp;c, \u0026quot;ping\u0026quot;) go func() { pong := Pong(\u0026amp;c) fmt.Println(pong) }() time.Sleep(time.Second * 2) } // 结果 // ping 需要注意的是，向通道中发送消息和从通道中接收消息，都是阻塞的，如果发送和接收不是成对出现，就会发生错误。 将上文中代码改成这样：\nc := make(chan string) Ping(\u0026amp;c, \u0026quot;ping\u0026quot;) //go func() { // pong := Pong(\u0026amp;c) // fmt."
    },{
      "title": "Go语言接口",
      "url": "https://qiref.github.io/post/2021/06/05/go%E8%AF%AD%E8%A8%80%E6%8E%A5%E5%8F%A3/",
      "content": "摘要：Go支持接口，接口是方法特征的命名集合。\ngo语言接口 go语言中有接口的概念，接口是方法特征的命名集合。它把所有的具有共性的方法定义在一起，任何其他类型只要实现了这些方法就是实现了这个接口。\n注意，实现了这些方法就算实现了这个接口。\n定义接口 // 定义geometry接口 type geometry interface { area() float64 peri() float64 } 接口的定义也比较简单。定义和实现规则如下：\n/* 定义接口 */ type interface_name interface { method_name1 [return_type] method_name2 [return_type] method_name3 [return_type] ... method_namen [return_type] } /* 定义结构体 */ type struct_name struct { /* variables */ } /* 实现接口方法 */ func (struct_name_variable struct_name) method_name1() [return_type] { /* 方法实现 */ } ... func (struct_name_variable struct_name) method_namen() [return_type] { /* 方法实现*/ } 实现接口 Go语言中接口的实现都是隐式的，默认实现了接口的所有方法就隐式地实现了接口。\n假如现已定义上文中的geometry接口，现在要实现该接口。\n// 定义rect结构体 type rect struct { width, height float64 } // rect 实现geometry接口，实现接口的方式与定义方法类似 func (r rect) area() float64 { return r.width * r.height } // rect 实现geometry接口，只有实现了接口的所有方法才算实现接口 func (r rect) peri() float64 { return r.height*2 + r.width*2 } 注意：只有实现了接口的所有方法才算实现接口。\n调用方式：\nvar geo1 geometry = rect{height: 1, width: 3} fmt.Println(\u0026quot;geo1.area()\u0026quot;, geo1.area()) fmt.Println(\u0026quot;geo1.peri()\u0026quot;, geo1.peri()) 接口也可以作为函数参数，传参传入具体的实现。\n// 定义调用接口方法的函数 func measure(g geometry) { fmt.Println(g.peri()) fmt.Println(g.area()) } // --- main() rect1 := rect{height: 3, width: 2} measure(rect1) 接口类型 接口也是Go语言中的一种类型，这种类型可以出现在变量的定义、函数的入参、函数的返回值中。\n// define Print func Print(i interface{}) { fmt.Println(i) } v1 := 22 v2 := \u0026quot;ss\u0026quot; v3 := make([]string, 1) v3 = append(v3, \u0026quot;333\u0026quot;) Print(v1) Print(v2) Print(v3) // 结果 // 22 // ss // [ 333] Print()函数中，参数为interface{}类型，但并不是任意类型，在调用函数时，将参数转换成了interface{}类型，这里是类型转换。\n接口和指针 看以下两个例子：\n通过结构体实现接口 // 定义rect结构体 type rect struct { width, height float64 } // rect 实现geometry接口，实现接口的方式与定义方法类似 func (r rect) area() float64 { return r.width * r.height } // rect 实现geometry接口，只有实现了接口的所有方法才算实现接口 func (r rect) peri() float64 { return r.height*2 + r.width*2 } 通过结构体指针实现接口 // 定义rect结构体 type rect struct { width, height float64 } // rect 实现geometry接口，实现接口的方式与定义方法类似 func (r *rect) area() float64 { return r.width * r.height } // rect 实现geometry接口，只有实现了接口的所有方法才算实现接口 func (r *rect) peri() float64 { return r.height*2 + r.width*2 } 在Go语言中，结构体实现接口有两种方式，通过结构体实现、通过结构体指针实现。但两种方式不能同时存在，这两种实现方式也会导致接口的调用有些差别。\nvar geo1 geometry = rect{height: 1, width: 3} // 使用结构体初始化变量 var geo1 geometry = \u0026amp;rect{height: 1, width: 3} // 使用结构体指针初始化变量 结构体实现接口 结构体指针实现接口 结构体指针初始化变量 通过 通过 结构体初始化变量 通过 不通过 当使用结构体实现接口时，无论初始化变量是结构体还是结构体指针，都可以编译通过；当使用结构体指针实现接口时，初始化变量为结构体时无法编译通过。\n为什么使用结构体指针实现接口时，初始化变量为结构体时无法编译通过？\n原因是在函数调用阶段，Go语言在传递参数时都是值传递。无论初始变量是rect{}还是\u0026amp;rect{}，在调用方法时都会发生值拷贝。\n对于\u0026amp;rect{}来说，方法调用阶段会拷贝一个新的\u0026amp;rect{}指针，新的指针与原来的指针都指向同一个结构体，所以编译器可以隐式地解引用（dereference），获取到原始的结构体。 对于rect{}来说，方法调用阶段会拷贝一个新的结构体，这是一个全新的rect{}，但因为方法的参数是*rect，编译器不会无中生有创建一个新的指针，就算创建新的指针，也不会指向原始的结构体。 当我们使用指针实现接口时，只有指针类型的变量才会实现该接口；当我们使用结构体实现接口时，指针类型和结构体类型都会实现该接口。当然这并不意味着我们应该一律使用结构体实现接口，这个问题在实际工程中也没那么重要。\n再谈接口类型 了解了接口参数的隐式转换后，再看上文提到的，接口不是任何一种类型，看以下示例：\ntype TestStruct struct{} func NilOrNot(v interface{}) bool { return v == nil } func main() { var s *TestStruct fmt.Println(s == nil) // true fmt.Println(NilOrNot(s)) // false } 调用NilOrNot函数时，发生了隐式类型转换，除了向方法传入参数之外，变量的赋值也会触发隐式类型转换，在转换过程中，*TestStruct类型会转换为interface{}类型，转换后的变量不仅包含转换前的变量，还包含变量的类型信息，所以转换后的变量与 nil 不相等。\n",
      "summary": "摘要：Go支持接口，接口是方法特征的命名集合。\ngo语言接口 go语言中有接口的概念，接口是方法特征的命名集合。它把所有的具有共性的方法定义在一起，任何其他类型只要实现了这些方法就是实现了这个接口。\n注意，实现了这些方法就算实现了这个接口。\n定义接口 // 定义geometry接口 type geometry interface { area() float64 peri() float64 } 接口的定义也比较简单。定义和实现规则如下：\n/* 定义接口 */ type interface_name interface { method_name1 [return_type] method_name2 [return_type] method_name3 [return_type] ... method_namen [return_type] } /* 定义结构体 */ type struct_name struct { /* variables */ } /* 实现接口方法 */ func (struct_name_variable struct_name) method_name1() [return_type] { /* 方法实现 */ } ... func (struct_name_variable struct_name) method_namen() [return_type] { /* 方法实现*/ } 实现接口 Go语言中接口的实现都是隐式的，默认实现了接口的所有方法就隐式地实现了接口。"
    },{
      "title": "Go语言指针",
      "url": "https://qiref.github.io/post/2021/06/05/go%E8%AF%AD%E8%A8%80%E6%8C%87%E9%92%88/",
      "content": "摘要：Go支持指针，允许在程序中通过引用传递值或者数据结构。\ngo语言中的指针和C语言中的指针类似，但比C语言中的指针更简单。\n// Go语言取地址符号是\u0026amp;，放到变量前会返回对应变量的内存地址 var i1 int = 1 var j = i1 fmt.Println(\u0026amp;i1) fmt.Println(\u0026amp;j) // 定义指针变量 var var_name *var_type s := \u0026quot;sss\u0026quot; p := 2181 var ip *string = \u0026amp;s var port *int = \u0026amp;p fmt.Println(*ip) fmt.Println(*port) 变量、指针和地址三者的关系是，每个变量都拥有地址，指针的值就是地址。\n通过\u0026amp; 获取对应变量的内存地址。 通过* 获取指针的值，也就是指针取值。取地址操作符 \u0026amp; 和取值操作符 * 是一对互补操作符，\u0026amp; 取出地址，* 根据地址取出地址指向的值。\n变量、指针地址、指针变量、取地址、取值的相互关系和特性如下：\n对变量进行取地址操作使用\u0026amp;操作符，可以获得这个变量的指针变量。 指针变量的值是指针地址。 对指针变量进行取值操作使用*操作符，可以获得指针变量指向的原变量的值。 通过New()创建指针 Go语言还提供了另外一种方法来创建指针变量，格式如下： new(type) 这个type可以为int。\n// create ptr by new() func createPtr() { str := new(string) *str = \u0026quot;ssss\u0026quot; fmt.Println(\u0026quot;str is : \u0026quot;, str) fmt.Println(\u0026quot;*str is : \u0026quot;, *str) // str is : 0xc000116050 // *str is : ssss } new() 函数可以创建一个对应类型的指针，创建过程会分配内存，被创建的指针指向默认值。\n数组指针 const numlen = 4 // 数组指针 nums := []int{1, 2, 3, 4} // 指针数组的长度必须等与数组长度 var ptr [numlen]*int for i2 := range nums { ptr[i2] = \u0026amp;nums[i2] } for i := 0; i \u0026lt; numlen; i++ { fmt.Println(*ptr[i]) } 指针的指针 // 指针的指针 pp := 12 p1 := \u0026amp;pp p2 := \u0026amp;p1 fmt.Println(\u0026quot;pp: \u0026quot;, pp) fmt.Println(\u0026quot;p1: \u0026quot;, *p1) fmt.Println(\u0026quot;p2: \u0026quot;, **p2) 函数指针 谈到函数指针，就不得不提到值传递和引用传递了。函数的参数中，如果是值传递，在函数体内部得到的是参数的拷贝，对参数修改不会影响原始参数；如果是引用传递，在函数体内部的到的就是参数的内存地址，对参数的修改会影响原始参数本身。\n// 值传递 func zeroval(ival int) { ival = 0 } // 引用传递 func zeroptr(iptr *int) { *iptr = 0 } zeroval 有一个 int 型参数，所以使用值传递。zeroval函数将从调用方得到一个ival形参的拷贝。\nzeroptr 有一和上面不同的 *int 参数，意味着它用了一个 int指针。函数体内的 *iptr 引用这个指针，从它内存地址得到这个地址对应的当前值。对一个引用的指针赋值将会改变这个指针引用的真实地址的值。\n通过函数指针可以优雅地实现两值交换。\n// 函数指针参数 x := 100 y := 200 fmt.Println(\u0026quot;x : \u0026quot;, x) fmt.Println(\u0026quot;y : \u0026quot;, y) swap(\u0026amp;x, \u0026amp;y) fmt.Println(\u0026quot;x : \u0026quot;, x) fmt.Println(\u0026quot;y : \u0026quot;, y) //--- func swap(x *int, y *int) { // 优雅的写法 *x, *y = *y, *x //tmp := *x //*x = *y //*y = tmp } 优雅地实现swap就是通过指针交换两值，如果说交换指针会怎么样呢？\n// swap ptr func swap1(x, y *int) { x, y = y, x } // main() aa := 1 bb := 3 swap1(\u0026amp;aa, \u0026amp;bb) fmt.Println(\u0026quot;aa: \u0026quot;, aa, \u0026quot; bb: \u0026quot;, bb) // 结果： // aa: 1 bb: 3 结果表明，交换是不成功的。上面代码中的 swap1() 函数交换的是 aa 和 bb 的地址，在交换完毕后，aa 和 bb 的变量值确实被交换。但和 aa、bb 关联的两个变量并没有实际关联。这就像写有两座房子的卡片放在桌上一字摊开，交换两座房子的卡片后并不会对两座房子有任何影响。\n空指针 当一个指针被定义后没有分配到任何变量时，它的值为 nil。\nnil 指针也称为空指针。\nnil在概念上和其它语言的null、None、nil、NULL一样，都指代空值。\n一个指针变量通常缩写为 ptr。\n// nilPtr func nilPtr() { var ptr *int fmt.Println(\u0026quot;ptr val is : \u0026quot;, ptr) } // 输出： // ptr val is : \u0026lt;nil\u0026gt; ",
      "summary": "摘要：Go支持指针，允许在程序中通过引用传递值或者数据结构。\ngo语言中的指针和C语言中的指针类似，但比C语言中的指针更简单。\n// Go语言取地址符号是\u0026amp;，放到变量前会返回对应变量的内存地址 var i1 int = 1 var j = i1 fmt.Println(\u0026amp;i1) fmt.Println(\u0026amp;j) // 定义指针变量 var var_name *var_type s := \u0026quot;sss\u0026quot; p := 2181 var ip *string = \u0026amp;s var port *int = \u0026amp;p fmt.Println(*ip) fmt.Println(*port) 变量、指针和地址三者的关系是，每个变量都拥有地址，指针的值就是地址。\n通过\u0026amp; 获取对应变量的内存地址。 通过* 获取指针的值，也就是指针取值。取地址操作符 \u0026amp; 和取值操作符 * 是一对互补操作符，\u0026amp; 取出地址，* 根据地址取出地址指向的值。\n变量、指针地址、指针变量、取地址、取值的相互关系和特性如下：\n对变量进行取地址操作使用\u0026amp;操作符，可以获得这个变量的指针变量。 指针变量的值是指针地址。 对指针变量进行取值操作使用*操作符，可以获得指针变量指向的原变量的值。 通过New()创建指针 Go语言还提供了另外一种方法来创建指针变量，格式如下： new(type) 这个type可以为int。\n// create ptr by new() func createPtr() { str := new(string) *str = \u0026quot;ssss\u0026quot; fmt."
    },{
      "title": "Go语言goroutine",
      "url": "https://qiref.github.io/post/2021/06/03/go%E8%AF%AD%E8%A8%80goroutine/",
      "content": "摘要：Go语言goroutine\ngoroutine协程 Go 协程 在执行上来说是轻量级的线程。go语言层面并不支持多进程或多线程，但是协程更好用，协程被称为用户态线程，不存在CPU上下文切换问题，效率非常高。\ngo语言中启动一个协程非常简单，只需要在执行函数前加上go关键字，就可以启用goroutine。\nfunc main() { // 使用匿名函数启用goroutine go func() { fmt.Println(\u0026quot;goroutine\u0026quot;) }() // 调用函数启用goroutine go func1() } func func1() { fmt.Println(\u0026quot;f1() was called.\u0026quot;) } 没错就是这么简单，在go语言中，goroutine会被放到运行队列runtime.runqput中，然后由调度器调度。并非是每一个协程都会有一个对应的线程去执行，协程比线程的粒度更细。\n但是上述代码并不会有输出结果，因为还没等func1()函数执行完成，main()就已经执行完成了。所以在main()函数执行完成之前sleep一下就可以看到func1()的执行结果。\ntime.Sleep(time.Second * 1) WaitGroup sleep肯定是不靠谱的，go语言中可以等待协程执行完成后再回到主线程。\n// 定义全局变量 var WG = sync.WaitGroup{} func main() { WG.Add(1) go func1() WG.Wait() } func func1() { fmt.Println(\u0026quot;f1() was called.\u0026quot;) WG.Done() } 在调用func1()之前，调用全局变量WG.Add()方法，然后启用goroutine调用func1()，然后调用WG.Wait()函数进行等待，fun1()调用结束后，调用WG.Done()。 通过试验可以发现：Add()方法中的数值与Done()方法的数量应该保持一致。当Add(2)时，Done()方法应该执行两次。直到 WaitGroup 计数器恢复为 0； 即所有协程的工作都已经完成。 看源码可以发现，Done()与Add()实际上是一个函数。\n// Done decrements the WaitGroup counter by one. func (wg *WaitGroup) Done() { wg.Add(-1) } 多个goroutine如何执行 func main() { loop := 5 WG.Add(loop) for i := 0; i \u0026lt; loop; i++ { go func2(i) } WG.Wait() } // define func2 func func2(i int) { fmt.Println(\u0026quot;func2() was called. i is : \u0026quot;, i) WG.Done() } // 运行结果： //func2() was called. i is : 4 //func2() was called. i is : 2 //func2() was called. i is : 3 //func2() was called. i is : 0 //func2() was called. i is : 1 每个goroutine的运行并不规则，每个协程在并发执行。\u0026#x1f914;\n从实现上，每一个goroutine都会加入队列中，然后这组协程由调度器通过各种调度策略进行调度。然后会开启多个线程去调度协程工作队列， 调度器最多可以创建 10000 个线程，但是其中大多数的线程都不会执行用户代码，大部分都进行调度工作，最多只会有 GOMAXPROCS 个活跃线程能够正常运行。在默认情况下，运行时会将 GOMAXPROCS 设置成当前机器的核数，我们也可以在程序中使用 runtime.GOMAXPROCS 来改变最大的活跃线程数。\nfunc main() { runtime.GOMAXPROCS(1) fmt.Println(runtime.NumGoroutine()) for i := 0; i \u0026lt; 10; i++ { go say(\u0026quot;Hello World: \u0026quot; + strconv.Itoa(i)) } fmt.Println(runtime.NumGoroutine()) for { } } func say(s string) { println(s) } 网上很多地方给出这个例子，并且说当runtime.GOMAXPROCS(1)的情况下，上述代码是不会运行的，只有当参数大于1时，才可以正常运行，但是该示例在go version go1.16.4 darwin/amd64 环境下可以正常运行，猜测是go协程调度策略新版本作了优化。\n参考：\nhttps://zhuanlan.zhihu.com/p/74047342\nhttp://books.studygolang.com/gobyexample/goroutines/\n",
      "summary": "摘要：Go语言goroutine\ngoroutine协程 Go 协程 在执行上来说是轻量级的线程。go语言层面并不支持多进程或多线程，但是协程更好用，协程被称为用户态线程，不存在CPU上下文切换问题，效率非常高。\ngo语言中启动一个协程非常简单，只需要在执行函数前加上go关键字，就可以启用goroutine。\nfunc main() { // 使用匿名函数启用goroutine go func() { fmt.Println(\u0026quot;goroutine\u0026quot;) }() // 调用函数启用goroutine go func1() } func func1() { fmt.Println(\u0026quot;f1() was called.\u0026quot;) } 没错就是这么简单，在go语言中，goroutine会被放到运行队列runtime.runqput中，然后由调度器调度。并非是每一个协程都会有一个对应的线程去执行，协程比线程的粒度更细。\n但是上述代码并不会有输出结果，因为还没等func1()函数执行完成，main()就已经执行完成了。所以在main()函数执行完成之前sleep一下就可以看到func1()的执行结果。\ntime.Sleep(time.Second * 1) WaitGroup sleep肯定是不靠谱的，go语言中可以等待协程执行完成后再回到主线程。\n// 定义全局变量 var WG = sync.WaitGroup{} func main() { WG.Add(1) go func1() WG.Wait() } func func1() { fmt.Println(\u0026quot;f1() was called.\u0026quot;) WG.Done() } 在调用func1()之前，调用全局变量WG.Add()方法，然后启用goroutine调用func1()，然后调用WG.Wait()函数进行等待，fun1()调用结束后，调用WG.Done()。 通过试验可以发现：Add()方法中的数值与Done()方法的数量应该保持一致。当Add(2)时，Done()方法应该执行两次。直到 WaitGroup 计数器恢复为 0； 即所有协程的工作都已经完成。 看源码可以发现，Done()与Add()实际上是一个函数。\n// Done decrements the WaitGroup counter by one."
    },{
      "title": "Go语言基本数据结构",
      "url": "https://qiref.github.io/post/2021/06/01/go%E8%AF%AD%E8%A8%80%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/",
      "content": "摘要：Go语言基本数据结构\n数组 strings := [3]string{\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;2\u0026quot;} intArray := [8]int{1, 2, 3, 4, 5, 5, 5, 55} 定义两个数组,fmt.Println(len(strings)) 可以使用len()函数得到数组的长度，strings[1]可以获取指定数组下标的元素。\n所以通过以下方式可以遍历数组：\nfor i := 0; i \u0026lt; len(strings); i++ { fmt.Println(strings[i]) } 下文提供了更加优雅的方式。\nslice Go数组的长度不可变，Go提供了一种内置类型切片:slice，与数组相比，切片的长度不是固定的，可以动态扩容、添加元素。\nslice1 := make([]string, 2) fmt.Println(slice1) slice1[0] = \u0026quot;22\u0026quot; slice1[1] = \u0026quot;222\u0026quot; fmt.Println(slice1) slice1 = append(slice1, \u0026quot;33\u0026quot;, \u0026quot;44\u0026quot;, \u0026quot;55\u0026quot;) fmt.Println(slice1) fmt.Println(len(slice1)) // 裁剪，从index 2 到index 4 sliceSub := slice1[2:4] fmt.Println(sliceSub) // 裁剪，从index 2 到最后 sliceSub2 := slice1[2:] fmt.Println(sliceSub2) make() 方法可以构建一个slice，并可以指定初始化大小和容量； len() 可以获取slice的大小； append() 可以想slice中添加元素； 可以通过裁剪，下标移动等方式删除slice。 slice从头部删除 a = []int{1, 2, 3} a = a[1:] // 删除开头1个元素 a = a[N:] // 删除开头N个元素 a = append(a[:0], a[1:]...) // 删除开头1个元素 a = append(a[:0], a[N:]...) // 删除开头N个元素 a = a[:copy(a, a[1:])] // 删除开头1个元素 a = a[:copy(a, a[N:])] // 删除开头N个元素 slice从中间删除 a = append(a[:i], a[i+1:]...) // 删除中间1个元素 a = append(a[:i], a[i+N:]...) // 删除中间N个元素 a = a[:i+copy(a[i:], a[i+1:])] // 删除中间1个元素 a = a[:i+copy(a[i:], a[i+N:])] // 删除中间N个元素 slice从尾部删除 a = a[:len(a)-1] // 删除尾部1个元素 a = a[:len(a)-N] // 删除尾部N个元素 slice 扩容 slice2 := make([]string, 1, 1) // cap()函数, 计算slice长度可以达到多少 fmt.Println(\u0026quot;cap(slice1) is :\u0026quot;, cap(slice1)) slice2[0] = \u0026quot;0\u0026quot; slice2 = append(slice2, \u0026quot;3\u0026quot;) fmt.Println(\u0026quot;cap(slice2) is :\u0026quot;, cap(slice2)) numbers := []int{0, 1, 2, 3, 4, 5} numbers = append(numbers, 6) fmt.Println(numbers) fmt.Println(cap(numbers)) 通过cap()函数可以获取slice的容量，容量是make()函数的第三个参数，超过设置的容量再往slice中添加元素就会使slice扩容。\n手动实现扩容：\n// 实现切片扩容 var capslice = make([]int, 10, 10) for i := range capslice { capslice[i] = i } fmt.Println(capslice) // 创建新的切片，并且容量是之前切片的2倍 var capslice1 = make([]int, len(capslice), cap(capslice)*2) copy(capslice1, capslice) fmt.Println(capslice1, \u0026quot;caplice cap is :\u0026quot;, cap(capslice1)) map map是go预制的一种字典数据结构。\n// map[key-type]val-type map1 := make(map[string]string) map1[\u0026quot;1\u0026quot;] = \u0026quot;11\u0026quot; map2 := make(map[string]int) map2[\u0026quot;2\u0026quot;] = 2 map2[\u0026quot;3\u0026quot;] = 3 fmt.Println(\u0026quot;map1:\u0026quot;, map1) fmt.Println(\u0026quot;map2\u0026quot;, map2) map结构也可以嵌套：\nmap3 := make(map[string]int) map3[\u0026quot;a\u0026quot;] = 1 map3[\u0026quot;b\u0026quot;] = 33 delete(map3, \u0026quot;b\u0026quot;) fmt.Println(map3) fmt.Println(len(map3)) m := map[string]map[string]int{} m[\u0026quot;aa\u0026quot;] = map3 fmt.Println(m) map中有个比较有意思的特性，获取某个元素是否在map中存在。\n// 有意思的特性，_, psr := 赋值结果为bool _, psr := map3[\u0026quot;a\u0026quot;] fmt.Println(psr) // 输出结果为： true 使用range遍历 nums := []int{1, 2, 3, 4, 5, 6} // 遍历slice for i := range nums { fmt.Println(nums[i]) } // 遍历map map1 := map[string]string{\u0026quot;aa\u0026quot;: \u0026quot;11\u0026quot;, \u0026quot;bb\u0026quot;: \u0026quot;22\u0026quot;} for s := range map1 { fmt.Println(map1[s]) } // 优雅地遍历map for k, v := range map1 { fmt.Println(\u0026quot;k : \u0026quot;, k, \u0026quot; v : \u0026quot;, v) } // 遍历字符串，获取Unicode for i, c := range \u0026quot;balabala\u0026quot; { fmt.Println(i, \u0026quot; -\u0026gt; \u0026quot;, c) } ",
      "summary": "摘要：Go语言基本数据结构\n数组 strings := [3]string{\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;2\u0026quot;} intArray := [8]int{1, 2, 3, 4, 5, 5, 5, 55} 定义两个数组,fmt.Println(len(strings)) 可以使用len()函数得到数组的长度，strings[1]可以获取指定数组下标的元素。\n所以通过以下方式可以遍历数组：\nfor i := 0; i \u0026lt; len(strings); i++ { fmt.Println(strings[i]) } 下文提供了更加优雅的方式。\nslice Go数组的长度不可变，Go提供了一种内置类型切片:slice，与数组相比，切片的长度不是固定的，可以动态扩容、添加元素。\nslice1 := make([]string, 2) fmt.Println(slice1) slice1[0] = \u0026quot;22\u0026quot; slice1[1] = \u0026quot;222\u0026quot; fmt.Println(slice1) slice1 = append(slice1, \u0026quot;33\u0026quot;, \u0026quot;44\u0026quot;, \u0026quot;55\u0026quot;) fmt.Println(slice1) fmt.Println(len(slice1)) // 裁剪，从index 2 到index 4 sliceSub := slice1[2:4] fmt.Println(sliceSub) // 裁剪，从index 2 到最后 sliceSub2 := slice1[2:] fmt."
    },{
      "title": "Go语言变量",
      "url": "https://qiref.github.io/post/2021/05/31/go%E8%AF%AD%E8%A8%80%E5%8F%98%E9%87%8F/",
      "content": "摘要：Go语言变量\nGo语言中的变量定义相对严格，当定义一个局部变量为使用时，会编译报错，在go语言中，无需要多余的代码。但全局变量除外，定义全局变量允许暂不使用。\n全局变量 全局变量可以被全局访问\n定义全局变量：\n// global variable var x, y int var ( a int b bool ) 基本常量 常量一经被定义后无法被重新赋值，常量可以定义为全局的，也可以定义为局部的。\n定义常量：\n// 这是一个常量 const CONST1 = 111 iota常量 iota，特殊常量，可以认为是一个可以被编译器修改的常量。 iota 在 const关键字出现时将被重置为 0(const 内部的第一行之前)，const 中每新增一行常量声明将使 iota 计数一次(iota 可理解为 const 语句块中的行索引)。\nconst ( a = iota b = iota c = iota ) // 以上写法可以简写为 const ( d = iota e f ) fmt.Println(a, b, c) fmt.Println(d, e, f) // 输出结果： // 0 1 2 // 0 1 2 iota常量可以恢复计数\n// iota可以恢复计数 const ( aa = iota bb cc dd = \u0026quot;lalala\u0026quot; ee = 2 ff = true gg = iota // 恢复计数 hh ) fmt.Println(aa, bb, cc, dd, ee, ff, gg, hh) // 输出结果为： // 0 1 2 lalala 2 true 6 7 关于iota一个有趣的例子\n// iota 移位运算 const ( ii = 1 \u0026lt;\u0026lt; iota jj = 2 \u0026lt;\u0026lt; iota kk = 3 \u0026lt;\u0026lt; iota ll // 此处等价于 ll = 3 \u0026lt;\u0026lt; iota mm // 此处等价于 ll = 3 \u0026lt;\u0026lt; iota ) fmt.Println(ii, jj, kk, ll, mm) // 输出结果为： // 1 4 12 24 48 变量 定义变量：\n// 定义一个变量并对其赋值 var intVal int intVal = 1 // 定义一个变量对其赋值，与上文中的效果一样 intVal1 := 1 go语言中，int类型比较特殊，可以有多种类型的int。\n// int 数据类型 包含了以下所有数据类型 var var1 int = -9223372036854775808 // 无符号 8 位整型 (0 到 255) var var2 uint8 = 255 // 无符号 16 位整型 (0 到 65535) var var3 uint16 = 65535 // 无符号 32 位整型 (0 到 4294967295) var var4 uint32 = 4294967295 // 无符号 64 位整型 (0 到 18446744073709551615) var var5 uint64 = 18446744073709551615 // 有符号 8 位整型 (-128 到 127) var var6 int8 = 127 // 有符号 16 位整型 (-32768 到 32767) var var7 int16 = 32767 // 有符号 32 位整型 (-2147483648 到 2147483647) var var8 int32 = 2147483647 // 有符号 64 位整型 (-9223372036854775808 到 9223372036854775807) var var9 int64 = 9223372036854775807 fmt.Println(var1, var2, var3, var4, var5, var6, var7, var8, var9) go类型强转 // go 类型转换 var sum int = 16 var sumf float32 sumf = float32(sum / 2) fmt.Printf(\u0026quot;%f\\n\u0026quot;, sumf) ",
      "summary": "摘要：Go语言变量\nGo语言中的变量定义相对严格，当定义一个局部变量为使用时，会编译报错，在go语言中，无需要多余的代码。但全局变量除外，定义全局变量允许暂不使用。\n全局变量 全局变量可以被全局访问\n定义全局变量：\n// global variable var x, y int var ( a int b bool ) 基本常量 常量一经被定义后无法被重新赋值，常量可以定义为全局的，也可以定义为局部的。\n定义常量：\n// 这是一个常量 const CONST1 = 111 iota常量 iota，特殊常量，可以认为是一个可以被编译器修改的常量。 iota 在 const关键字出现时将被重置为 0(const 内部的第一行之前)，const 中每新增一行常量声明将使 iota 计数一次(iota 可理解为 const 语句块中的行索引)。\nconst ( a = iota b = iota c = iota ) // 以上写法可以简写为 const ( d = iota e f ) fmt.Println(a, b, c) fmt.Println(d, e, f) // 输出结果： // 0 1 2 // 0 1 2 iota常量可以恢复计数"
    },{
      "title": "MySQL执行计划",
      "url": "https://qiref.github.io/post/2020/10/21/mysql%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92/",
      "content": "什么是SQL执行计划 EXPLAIN命令是查看查询优化器如何决定执行查询的主要的方法，学会解释EXPLAIN将帮助我们了解SQL优化器是如何工作的。执行计划可以告诉我们SQL如何使用索引，连接查询的执行顺序，查询的数据行数。 要使用EXPLAIN,只需要在查询的SELECT关键字之前增加EXPLAIN这个词。\nMySQL [dev]\u0026gt; explain select * from TableName where Name like '%c'; +----+-------------+--------------+------------+------+---------------+------+---------+------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+--------------+------------+------+---------------+------+---------+------+------+----------+-------------+ | 1 | SIMPLE | ClusterGroup | NULL | ALL | NULL | NULL | NULL | NULL | 254 | 11.11 | Using where | +----+-------------+--------------+------------+------+---------------+------+---------+------+------+----------+-------------+ 1 row in set, 1 warning (0.01 sec) 执行计划参数说明 id 是一位数字，表示select语句的顺序。 id值相同时，执行顺序从上到下。id值不同时，id值大的先执行。\nselect_type 这一列显示了对应行是简单还是复杂的select，如果是simple意味着查询不包括子查询和UNION。\nselect type 说明 SIMPLE 不包含子查询和union操作 PRIMARY 查询中包含任何子查询，那么最外层的查询则被标记为PRIMARY SUBQUERY select中的子查询 DEPENDENT SUBQUERY 依赖外部结果的子查询 partitions 对于分区表，显示查询的分区id，对于非分区表，显示为NULL。\ntype select type 值 说明 性能 值 含义 高 system 这是const联接类型的一个特例，当查询的表只有一行时使用。 const 表中有且只有一个匹配的行时使用，如对主键或是唯一索引的查询，这是效率最高的联接方式。 eq_ref 唯一索引或主键索引查找，对于每个索引，表中只有一条记录与之匹配。 ref 非唯一索引，查找，返回某个单独值的所有行。 ref_or_null 类似于ref类型的查询，但是附加了对null值列的查询。 index_merge 该联接类型表示使用了索引合并优化方法。 range 索引范围扫描，常见于betwteen、\u0026gt;、\u0026lt;这样的查询条件。 index FULL INDEX SCAN 全索引扫描，跟ALL的区别是这里遍历的是索引树。 低 ALL FULL TABLE SCAN 全表扫描，这是效率最差的联接方式。 possible keys 指出MySQL能使用哪些索引来优化查询，查询所涉及的列上的索引都会被列出，但不一定会被使用。\nkey 查询优化器优化查询实际所使用的索引，如果没有可用的索引，则显示为NULL，如查询使用了覆盖索引，则该索引仅出现在列中。\nkey_len 表示索引字段的最大可能长度，key_len的长度由字段定义而来，并非数据的实际长度。\nref 表示哪些列或者常量被用于查找索引列上的值。\nrows 表示MySQL通过哪些列或者常量被用于查找索引列山的值，rows值的大小是个统计抽样结果，并不十分准确。\nfiltered 表示返回结果的行数占需读取行数的百分比，Filter列的值越大越好。\nExtra distinct 优化distinct操作，在找到第一匹配的元组后即停止找同样值的动作。 Not exists 使用not exists来优化查询。 using filesort 使用额外操作进行排序，通常会出现在order by 或在group by查询中。 using index 使用了覆盖索引进行查询。\n",
      "summary": "什么是SQL执行计划 EXPLAIN命令是查看查询优化器如何决定执行查询的主要的方法，学会解释EXPLAIN将帮助我们了解SQL优化器是如何工作的。执行计划可以告诉我们SQL如何使用索引，连接查询的执行顺序，查询的数据行数。 要使用EXPLAIN,只需要在查询的SELECT关键字之前增加EXPLAIN这个词。\nMySQL [dev]\u0026gt; explain select * from TableName where Name like '%c'; +----+-------------+--------------+------------+------+---------------+------+---------+------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+--------------+------------+------+---------------+------+---------+------+------+----------+-------------+ | 1 | SIMPLE | ClusterGroup | NULL | ALL | NULL | NULL | NULL | NULL | 254 | 11.11 | Using where | +----+-------------+--------------+------------+------+---------------+------+---------+------+------+----------+-------------+ 1 row in set, 1 warning (0."
    },{
      "title": "Centos搭建公共yum源",
      "url": "https://qiref.github.io/post/2019/12/14/centos%E6%90%AD%E5%BB%BA%E5%85%AC%E5%85%B1yum%E6%BA%90/",
      "content": "摘要：记录在Centos7中如何挂载ISO镜像作yum源，并借助http服务作公共yum源。\n部署yum私服 上传centos镜像文件到服务器\nmount -t iso9660 -o loop centos-7-x86_64-dvd-1511.iso /mnt/cdrom/ （卸载：umoutn /mnt/cdrom)\n挂载成功！ 将软件链接到http服务发布路径下 确定当前服务器是否安装了httpd服务\nln -s /mnt/cdrom/ /var/www/html/CentOS7 检查http服务\nsystemctl status httpd.service 启动HTTP服务器\nsystemctl enable httpd.service systemctl start httpd.service 界面查看\ncd /etc/yum.repos.d/ mkdir bak mv centos-* bak vi CentOS-Base.repo [base] name=CentOS-$releasever - Base baseurl=http://192.168.67.15/CentOS7/ gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 分发到所有服务器\nscp -r /etc/yum.repos.d/ hadoop-01:/etc/ scp -r /etc/yum.repos.d/ hadoop-02:/etc/ 检查是否正成功安装yum 源\nyum clean all yum makecache yum list 如果能看到软件列表则说明安装成功。\n",
      "summary": "摘要：记录在Centos7中如何挂载ISO镜像作yum源，并借助http服务作公共yum源。\n部署yum私服 上传centos镜像文件到服务器\nmount -t iso9660 -o loop centos-7-x86_64-dvd-1511.iso /mnt/cdrom/ （卸载：umoutn /mnt/cdrom)\n挂载成功！ 将软件链接到http服务发布路径下 确定当前服务器是否安装了httpd服务\nln -s /mnt/cdrom/ /var/www/html/CentOS7 检查http服务\nsystemctl status httpd.service 启动HTTP服务器\nsystemctl enable httpd.service systemctl start httpd.service 界面查看\ncd /etc/yum.repos.d/ mkdir bak mv centos-* bak vi CentOS-Base.repo [base] name=CentOS-$releasever - Base baseurl=http://192.168.67.15/CentOS7/ gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 分发到所有服务器\nscp -r /etc/yum.repos.d/ hadoop-01:/etc/ scp -r /etc/yum.repos.d/ hadoop-02:/etc/ 检查是否正成功安装yum 源\nyum clean all yum makecache yum list 如果能看到软件列表则说明安装成功。"
    },{
      "title": "逆波兰表达式算法",
      "url": "https://qiref.github.io/post/2019/09/04/%E9%80%86%E6%B3%A2%E5%85%B0%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%AE%97%E6%B3%95/",
      "content": "摘要：将中缀表达式转化为后缀表达式，以及计算后缀表达式的算法。\nimport org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.util.HashSet; import java.util.Scanner; import java.util.Stack; /** * @author YaoQi * Date: 2019/1/5 15:45 * Modified: * Description: 中缀表达式转后缀表达式 */ public class InfixToSuffixHandler { private static final Logger logger = LoggerFactory.getLogger(InfixToSuffixHandler.class); private static HashSet\u0026lt;Character\u0026gt; opStr = new HashSet\u0026lt;\u0026gt;(); static { logger.info(\u0026quot;Initialization operator\u0026quot;); opStr.add('+'); opStr.add('-'); opStr.add('*'); opStr.add('/'); logger.info(\u0026quot;Initialization finished\u0026quot;); } /** * 判断字符是否为操作符 * * @param c 字符 * @return */ private static boolean isOpStr(char c) { return opStr.contains(c); } /** * 判断字符是否为操作数 * * @param c 字符 * @return */ private static boolean isOperand(char c) { return c \u0026gt;= '0' \u0026amp;\u0026amp; c \u0026lt;= '9'; } /** * 得到当前操作符的优先级 * * @param c 操作符 * @return */ private static int priority(char c) { switch (c) { case '*': case '/': return 3; case '+': case '-': return 2; case '(': return 1; default: { logger.error(\u0026quot;c: {} is not operator marks\u0026quot;, c); return 0; } } } /** * 用后缀表达式求值 * * @param suffixExpr 后缀表达式 * @return */ public static int numberCalculate(String suffixExpr) { Stack\u0026lt;Integer\u0026gt; count = new Stack\u0026lt;\u0026gt;(); char c; int number1, number2; for (int i = 0; i \u0026lt; suffixExpr.length(); i++) { if (isOperand(c = suffixExpr.charAt(i))) { count.push(c - '0'); } else { number2 = count.pop(); number1 = count.pop(); switch (c) { case '+': count.push(number1 + number2); break; case '-': count.push(number1 - number2); break; case '*': count.push(number1 * number2); break; case '/': count.push(number1 / number2); break; default: break; } } } return count.pop(); } /** * 将中缀表达式转化为后缀表达式 * * @param expression 中缀表达式 * @return 返回后缀表达式 */ public static StringBuilder getSuffixExpression(String expression) { //保存已将建立的后缀表达式 StringBuilder suffixExpr = new StringBuilder(); //操作符栈 Stack\u0026lt;Character\u0026gt; opStr = new Stack\u0026lt;\u0026gt;(); //输入表达式某个位置的字符 char c; //运算符栈中弹出的字符 char pop; for (int i = 0; i \u0026lt; expression.length(); i++) { c = expression.charAt(i); //如果当前字符为操作数 if (isOperand(c)) { suffixExpr.append(c); } else if (isOpStr(c)) { //如果当前字符为操作符 if (opStr.isEmpty()) { opStr.push(c); } else { while (!opStr.isEmpty() \u0026amp;\u0026amp; priority(opStr.peek()) \u0026gt;= priority(c)) { pop = opStr.pop(); suffixExpr.append(pop); } opStr.push(c); } } else if ('(' == c) { //如果当前字符为‘(’ opStr.push(c); } else if (')' == c) { //如果当前字符为‘)’ while ((pop = opStr.pop()) != '(') { suffixExpr.append(pop); } } else { logger.error(\u0026quot;c: {} is not valid in {}\u0026quot;, c, expression); } } while (!opStr.isEmpty()) { suffixExpr.append(opStr.pop()); } System.out.println(\u0026quot;转换后的后缀表达式为：\u0026quot; + suffixExpr); return suffixExpr; } public static void main(String[] args) { Scanner scan = new Scanner(System.in, \u0026quot;UTF-8\u0026quot;); //输入的表达式 String expression = scan.nextLine(); // 生成后缀表达式 StringBuilder suffixExpr = getSuffixExpression(expression); System.out.println(\u0026quot;后缀表达式计算的结果为：\u0026quot; + numberCalculate(suffixExpr.toString())); } } tips 一般的代数表达式都是中缀表达式，也就是操作数在操作符两边，后缀表达式（逆波兰表达式）就是操作符在操作数后面。\n例如：\na+b \u0026mdash;\u0026gt; a,b,+\na+(b-c) \u0026mdash;\u0026gt; a,b,c,-,+\na+(b-c)d \u0026mdash;\u0026gt; a,b,c,-,d,,+\n算法核心思想：将中缀表达式转为后缀表达式，再使用栈来对后缀表达式求值。 求值的过程就是遇到操作符就计算栈内的表达式，遇到操作数就入栈，最终栈中的元素就是最终的结果了。\n该算法还可以扩展为逻辑运算符，例如: B and C not D 在处理类似于这种逻辑表达式转换的时候也能使用该算法，笔者曾在实践中使用过改造该算法进行ES查询语句的构建，支持逻辑表达式的搜索。\n",
      "summary": "摘要：将中缀表达式转化为后缀表达式，以及计算后缀表达式的算法。\nimport org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.util.HashSet; import java.util.Scanner; import java.util.Stack; /** * @author YaoQi * Date: 2019/1/5 15:45 * Modified: * Description: 中缀表达式转后缀表达式 */ public class InfixToSuffixHandler { private static final Logger logger = LoggerFactory.getLogger(InfixToSuffixHandler.class); private static HashSet\u0026lt;Character\u0026gt; opStr = new HashSet\u0026lt;\u0026gt;(); static { logger.info(\u0026quot;Initialization operator\u0026quot;); opStr.add('+'); opStr.add('-'); opStr.add('*'); opStr.add('/'); logger.info(\u0026quot;Initialization finished\u0026quot;); } /** * 判断字符是否为操作符 * * @param c 字符 * @return */ private static boolean isOpStr(char c) { return opStr."
    },{
      "title": "spark学习笔记-RDD基础算子",
      "url": "https://qiref.github.io/post/2019/07/27/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-rdd%E5%9F%BA%E7%A1%80%E7%AE%97%E5%AD%90/",
      "content": "摘要：学习spark过程中的笔记，记录spark中的基础算子，以及RDD的基本概念。\nspark transform operation 源码地址：https://github.com/YaoQi17/sparkLearning/tree/master/sparkRDD\n总结 RDD(Resilient Distributed Dataset) 弹性分布式数据集，是一组分布式的数据集合，里面的元素可并行计算，可分区； RDD允许用户在执行多个查询时显示地将工作集缓存在内存中，例如persist()；\n创建方式 创建RDD的两种方式：\n读取外界文件 外界文件不局限于系统文件，包括HDFS、HBase等\nsparkSession.sparkContext.textFile(\u0026quot;sparkRDD/src/main/resources/data.txt\u0026quot;) 通过并行化的方式创建 val sparkSession = getDefaultSparkSession val dataArray = Array(1, 2, 3, 4, 5, 6) // 创建一个RDD val rdd = sparkSession.sparkContext.parallelize(dataArray) 通过并行化的方式创建还可以指定分区的数量\n/** Distribute a local Scala collection to form an RDD. * * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call * to parallelize and before the first action on the RDD, the resultant RDD will reflect the * modified collection. Pass a copy of the argument to avoid this. * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection */ def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) } RDD编程 RDD中包含两种类型的算子：Transformation和Action；\nTransformation 算子 Transformation 转换操作，将一个RDD转化为另外一个RDD，但是该过程具有延迟加载性； Transformation算子不会马上执行，只有当遇到Action算子时才会执行。\n常见的Transformation算子：\nmap(function) 由一个RDD转化为另外一个RDD，function的每一次输出组成另外一个RDD；\nfilter(function) 由一个RDD的元素经过筛选，满足function条件的元素组成一个新的RDD；\nflatMap(function) 类似于map，但是每一个元素可以被转化为多个元素，function应该返回一个序列；\n/** * flatMap实例，flatMap返回的是一组元素，官网说是一个Seq ，而不是一个元素 */ def flatMapDemo(): Unit = { val sparkSession = getDefaultSparkSession val textData = sparkSession.sparkContext.textFile(\u0026quot;sparkRDD/src/main/resources/data.txt\u0026quot;) // val flatData = textData.flatMap(row =\u0026gt; row.split(\u0026quot; \u0026quot;)) val flatData = textData.map(row =\u0026gt; row.split(\u0026quot; \u0026quot;)) flatData.collect().foreach(println(_)) } mapPartitions(function) 类似于map，但独立地在RDD的每一个分片上运行，函数类型是：Iterator[T] =\u0026gt; Iterator[U]；\n传值调用（call-by-value）：先计算参数表达式的值，再应用到函数内部，在函数外部求值；\n传名调用（call-by-name）：将未计算的参数表达式直接应用到函数内部，在函数内部求值；\nIterator[T] =\u0026gt; Iterator[U] 就是表示该函数为传名调用。\nmapPartitionsWithIndex(function) 类似于mapPartitions，但是传入的参数中多了一个索引值，该索引值为RDD分片数的索引值； （传入的函数类型为：(Int, Iterator) =\u0026gt; Iterator）\n/** * mapPartitionsWithIndex使用示例 */ def mapPartitionsWithIndexDemo(): Unit = { val sparkSession = getDefaultSparkSession val rddData = sparkSession.sparkContext.parallelize(Array(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;, \u0026quot;e\u0026quot;)) val data = rddData.mapPartitionsWithIndex((index: Int, row: Iterator[String]) =\u0026gt; { row.toList.map(x =\u0026gt; \u0026quot;[partID:\u0026quot; + index + \u0026quot;:\u0026quot; + x + \u0026quot;]\u0026quot;).iterator }) data.collect().foreach(println(_)) } sample(withReplacement, fraction, seed) 根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子；\nunion(otherDataset) 对源RDD和参数中的RDD求并集后返回一个新的RDD；\n/** * 求并集示例 */ def unionRDD(): Unit = { val sparkSession = getDefaultSparkSession val rddData1 = sparkSession.sparkContext.parallelize(List(\u0026quot;s\u0026quot;, \u0026quot;d\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;a\u0026quot;)) val rddData2 = rddData1.map(row =\u0026gt; { if (row.equals(\u0026quot;s\u0026quot;)) { row + \u0026quot;s\u0026quot; } else { row } }) println(\u0026quot;求并集:\u0026quot;) rddData1.union(rddData2).foreach(println(_)) println(\u0026quot;求交集:\u0026quot;) rddData1.intersection(rddData2).foreach(println(_)) val rddData3 = rddData1.union(rddData2) println(\u0026quot;去重:\u0026quot;) rddData3.distinct().collect().foreach(println(_)) } intersection(otherDataset) 对源RDD和参数中的RDD求交集后返回一个新的RDD；\ndistinct([numTasks])) 对源RDD进行去重后返回一个新的RDD；\ngroupByKey([numTasks]) 在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD；\n/** * 分组示例 */ def groupByKeyDemo(): Unit = { val sparkSession = getDefaultSparkSession val textFileRDD = sparkSession.sparkContext.textFile(\u0026quot;sparkRDD/src/main/resources/data.txt\u0026quot;) val rddData = textFileRDD.flatMap(_.split(\u0026quot; \u0026quot;)).map(row =\u0026gt; (row, 1)) rddData.groupByKey().map(row =\u0026gt; { val count = row._2.sum (row._1, count) }).collect().foreach(println(_)) rddData.reduceByKey((x, y) =\u0026gt; x + y).collect().foreach(println(_)) // rddData.groupByKey().collect().foreach(println(_)) } reduceByKey(func, [numTasks]) 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置； 与groupByKey的不同在于reduceByKey中可以传入一个函数，处理规约后的每个值；groupByKey则是将分组后的值都放到Iterator中； val textFileRDD = sparkSession.sparkContext.textFile(\u0026quot;sparkRDD/src/main/resources/data.txt\u0026quot;) val rddData = textFileRDD.flatMap(_.split(\u0026quot; \u0026quot;)).map(row =\u0026gt; (row, 1)) rddData.groupByKey().map(row =\u0026gt; { val count = row._2.sum (row._1, count) }).collect().foreach(println(_)) rddData.reduceByKey((x, y) =\u0026gt; x + y).collect().foreach(println(_)) 看完reduceByKey之后再去看看distinct()的源码，就会发现很有意思：\ndef distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { map(x =\u0026gt; (x, null)).reduceByKey((x, y) =\u0026gt; x, numPartitions).map(_._1) } 先是将一个RDD转化为(x,null) 这种二元结构，然后按照每个key进行规约，这样就能保证key只有一个，而x,y都为null，最后只需要再将规约后的key取出来，就是去重后的RDD了。\naggregateByKey (zeroValue)(seqOp, combOp, [numTasks]) 先按分区聚合 ，再总的聚合 ；每次要跟初始值交流 例如：aggregateByKey(0)(+,+) 对k/y的RDD进行操作； /** * 聚合，暂时还没理解 */ def aggregateByKeyDemo(): Unit = { val sparkSession = getDefaultSparkSession val textFileRDD = sparkSession.sparkContext.textFile(\u0026quot;sparkRDD/src/main/resources/data.txt\u0026quot;) val rddData = textFileRDD.flatMap(_.split(\u0026quot; \u0026quot;)).map(row =\u0026gt; (row, 1)) val aggregateRDD = rddData.aggregateByKey(0)(_ + _, _ + _) aggregateRDD.collect().foreach(println(_)) } sortByKey([ascending], [numPartitions]) 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD； /** * 特殊的排序 */ def sortByKeyDemo(): Unit = { val sparkSession = getDefaultSparkSession val rddData2 = sparkSession.sparkContext.parallelize(List(\u0026quot;CSDN\u0026quot;, \u0026quot;ITEYE\u0026quot;, \u0026quot;CNBLOG\u0026quot;, \u0026quot;OSCHINA\u0026quot;, \u0026quot;GITHUB\u0026quot;)) val rddData3 = sparkSession.sparkContext.parallelize(1 to rddData2.count().toInt) val rddData4 = rddData2.zip(rddData3) rddData4.sortByKey().collect().foreach(println(_)) } sortBy(func,[ascending], [numTasks]) 与sortByKey类似，排序的对象也是（K,V）结构，第一个参数是根据什么排序， 第二个是怎么排序 false倒序 ，第三个排序后分区数 ，默认与原RDD一样； def sortByDemo(): Unit = { val sparkSession = getDefaultSparkSession val rddData = sparkSession.sparkContext.parallelize(List(3, 23, 4, 6, 234, 87)) val newRdd = rddData.mapPartitionsWithIndex((index: Int, row: Iterator[Int]) =\u0026gt; { row.toList.map(r =\u0026gt; (index, r)).iterator }) newRdd.sortBy(_._2, ascending = false).collect().foreach(println(_)) } join(otherDataset, [numTasks]) 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD ,相当于内连接（求交集)； /** * zip 和 join 操作 */ def joinDemo(): Unit = { val sparkSession = getDefaultSparkSession val rddDataName = sparkSession.sparkContext.parallelize(List(\u0026quot;Tom\u0026quot;, \u0026quot;Rose\u0026quot;, \u0026quot;Jack\u0026quot;, \u0026quot;Jerry\u0026quot;)) val rddDataId = sparkSession.sparkContext.parallelize(List(\u0026quot;1001\u0026quot;, \u0026quot;1002\u0026quot;, \u0026quot;1003\u0026quot;, \u0026quot;1004\u0026quot;)) val rddDataAge = sparkSession.sparkContext.parallelize(List(12, 22, 13, 20)) val rddIdAndName = rddDataId.zip(rddDataName) rddIdAndName.collect().foreach(println(_)) val rddIdAndAge = rddDataId.zip(rddDataAge) rddIdAndAge.collect().foreach(println(_)) val fullRdd = rddIdAndName.join(rddIdAndAge) fullRdd.collect().foreach(println(_)) } cogroup(otherDataset, [numTasks]) 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD；在2.3之后没有该方法。 小细节1 引用成员变量 以下代码中map中引用了class中的成员变量；\nclass MyClass { val field = \u0026quot;Hello\u0026quot; def doStuff(rdd: RDD[String]): RDD[String] = { rdd.map(x =\u0026gt; field + x) } } 但这种方式等价于： scala rdd.map(x =\u0026gt; this.field + x) ，这种情况会引用整个this； 正确的做法是这样的：\ndef doStuff(rdd: RDD[String]): RDD[String] = { // 复制一份副本到本地 val field_ = this.field rdd.map(x =\u0026gt; field_ + x) } 在map中引用成员变量，应该在进行转换之前就复制一份副本到本地，然后使用本地的副本而不是去引用成员变量；\n小细节2 求和操作 不能在代码中直接使用foreach求和\nvar counter = 0 var rdd = sc.parallelize(data) // Wrong: Don't do this!! rdd.foreach(x =\u0026gt; counter += x) println(\u0026quot;Counter value: \u0026quot; + counter) ",
      "summary": "摘要：学习spark过程中的笔记，记录spark中的基础算子，以及RDD的基本概念。\nspark transform operation 源码地址：https://github.com/YaoQi17/sparkLearning/tree/master/sparkRDD\n总结 RDD(Resilient Distributed Dataset) 弹性分布式数据集，是一组分布式的数据集合，里面的元素可并行计算，可分区； RDD允许用户在执行多个查询时显示地将工作集缓存在内存中，例如persist()；\n创建方式 创建RDD的两种方式：\n读取外界文件 外界文件不局限于系统文件，包括HDFS、HBase等\nsparkSession.sparkContext.textFile(\u0026quot;sparkRDD/src/main/resources/data.txt\u0026quot;) 通过并行化的方式创建 val sparkSession = getDefaultSparkSession val dataArray = Array(1, 2, 3, 4, 5, 6) // 创建一个RDD val rdd = sparkSession.sparkContext.parallelize(dataArray) 通过并行化的方式创建还可以指定分区的数量\n/** Distribute a local Scala collection to form an RDD. * * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call * to parallelize and before the first action on the RDD, the resultant RDD will reflect the * modified collection."
    },{
      "title": "SpringBoot HBase",
      "url": "https://qiref.github.io/post/2019/07/11/springboot-hbase/",
      "content": "摘要:记录自己写的一个基于SpringBoot操作HBase的组件，支持kerberos认证方式访问，本文相当于API文档。\nHBase 组件接口文档 源码地址：https://github.com/YaoQi17/HBase-Component\n使用说明 基本概念\ntable: 表\ncolumnFamily:列族，一个表下可以有多个列族，但是不建议设置多个列族，HBase建议设计长窄型的表而不是短宽型。\nqualifier:列，一个列族下可以有多列，一个表中的列可以是不对齐的，但是这样效率不高，同一张表中的列最好是相同的。\ncell:一列数据下的一个单元格，一个列下可以有多个单元格，根据版本号区分，默认每次读取最新版本的数据，cell下的存储是数据本身。\nrow: 行，多列数据组成一行，一行中有多个qualifier。\nrowKey: 行健，用于唯一标识一行数据，一行下有多列，行健的设计直接关系到查询的效率。\nHBase配置 以下配置为最基础配置，缺一不可。\nHBase: conf: quorum: 192.168.80.234:2181,192.168.80.235:2181,192.168.80.241:2181 znodeParent: /hbase-unsecure #如果有更多配置，写在config下，例如： #config: # key: value # key: value 如果需要更多配置，需要在config中配置，以key-value的形式书写。\n参数说明 quorum是HBase中zookeeper的配置，znodeParent是HBase配置在zookeeper中的路径。\n简单示例 引入组件jar包：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.semptian.hbase.component\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hbase-component\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 在需要的地方注入HBaseOperations接口，该接口的实现类是HBaseTemplate，通过这个类来操作HBase。\n@Autowired private HBaseOperations hBaseDao; 查询一条数据，通过rowKey查询：\npublic void testQueryTable() { Result result = hBaseDao.queryByTableNameAndRowKey( \u0026quot;LBS\u0026quot;, 9223372036854775803L); System.out.println(result.isEmpty()); result.listCells().forEach(cell -\u0026gt; { System.out.println( \u0026quot;row:\u0026quot; + Bytes.toLong(CellUtil.cloneRow(cell)) + \u0026quot;,family:\u0026quot;+ Bytes.toString(CellUtil.cloneFamily(cell)) + \u0026quot;, qualifier: \u0026quot; + Bytes.toString(CellUtil.cloneQualifier(cell)) + \u0026quot;, value:\u0026quot; + Bytes.toString(CellUtil.cloneValue(cell))); }); } 表的基本操作 新建表 创建表通过HBaseTemplate就可以实现，HBaseTemplate类中带有这个方法。\n操作示例：\nhBaseDao.createTable(\u0026quot;HBASE-COMPONENT_1\u0026quot;, \u0026quot;CF1\u0026quot;, \u0026quot;CF2\u0026quot;); 上述代码创建了一张表，HBASE-COMPONENT_1 是表名，CF1,CF2代表这个表有两个列族。\n如果有多个列族可以往后面加，列族不建议设置很多个。\n删除表 hBaseDao.dropTable(\u0026quot;HBASE-COMPONENT_1\u0026quot;); 参数是表名，通过表名删除表。\n判断表是否存在 hBaseDao.tableExists(\u0026quot;lbs\u0026quot;); 这里的表名是区分大小写的。返回值：boolean。\n新增数据 新增一条数据 需要注意的是在HBase中的存储的数据是不分格式的，都是以字节数组的形式存储，因此在存储一条数据时需要将数据都转化成字节数组。\nString格式的数据能直接转换为字节数组getBytes()，但是其他格式的数据需要借助工具作转换。\n这里需要格外注意rowKey的格式，用什么格式存就决定了用什么格式取。\nhBaseDao.put(\u0026quot;HBase-component\u0026quot;, \u0026quot;1534154424340\u0026quot;, \u0026quot;CF1\u0026quot;, \u0026quot;test_1\u0026quot;, Bytes.toBytes(\u0026quot;testData\u0026quot;)); 参数说明：\n(1) tableName 目标数据表 (2) rowName rowKey (3) familyName 列族名 (4) qualifier 列名 (5) data 字节数组类型的数据 这里新增一条数据是填充数据到一个cell中去。\n批量新增数据 String rowKey = String.valueOf(System.currentTimeMillis()); Put put = new Put(rowKey.getBytes()); String defaultColumn = \u0026quot;CF1\u0026quot;; String column1 = \u0026quot;col1\u0026quot;; String column2 = \u0026quot;col2\u0026quot;; String column3 = \u0026quot;col3\u0026quot;; String value = \u0026quot;test\u0026quot;; put.addColumn(defaultColumn.getBytes(), column1.getBytes(), value.getBytes()); put.addColumn(defaultColumn.getBytes(), column2.getBytes(), value.getBytes()); put.addColumn(defaultColumn.getBytes(), column3.getBytes(), value.getBytes()); List\u0026lt;Put\u0026gt; putList = new ArrayList\u0026lt;\u0026gt;(); putList.add(put); putList.add(put); putList.add(put); putList.add(put); putList.add(put); hBaseDao.putBatch(\u0026quot;HBase-component\u0026quot;, putList); 批量插入数据就是使用多个Put对象，putBatch(\u0026hellip;)方法的参数：表名，putList(多个put的集合)。 注意批量插入数据也都是插入字节数组格式的数据。\n删除数据 删除一条数据 hBaseDao.delete(\u0026quot;HBase-component\u0026quot;, \u0026quot;1534210201115\u0026quot;, \u0026quot;CF1\u0026quot;, \u0026quot;col2\u0026quot;); 参数说明：\n(1) 表名\n(2) rowKey\n(3) 列族名\n(4) 列名\n这里删除是删除一个cell下的数据\n批量删除数据 String tableName = \u0026quot;HBase-component\u0026quot;; String rowKey1 = \u0026quot;1534164113922\u0026quot;; String rowKey2 = \u0026quot;1534168248328\u0026quot;; List\u0026lt;Delete\u0026gt; deleteList = new ArrayList\u0026lt;\u0026gt;(); Delete delete = new Delete(rowKey1.getBytes()); Delete delete1 = new Delete(rowKey2.getBytes()); deleteList.add(delete); deleteList.add(delete1); hBaseDao.deleteBatch(tableName, deleteList); 批量删除需要借助Delete对象。\n查询 单条结果查询 Result result = hBaseDao.queryByTableNameAndRowKey(\u0026quot;LBS\u0026quot;, 9223372036854775803L); System.out.println(result.isEmpty()); result.listCells().forEach(cell -\u0026gt; { System.out.println( \u0026quot; row:\u0026quot; + Bytes.toLong(CellUtil.cloneRow(cell)) + \u0026quot; family:\u0026quot;+ Bytes.toString(CellUtil.cloneFamily(cell)) + \u0026quot; qualifier: \u0026quot; + Bytes.toString(CellUtil.cloneQualifier(cell)) + \u0026quot; value:\u0026quot; + Bytes.toString(CellUtil.cloneValue(cell))); }); queryByTableNameAndRowKey()该方法是通过表名和rowKey查询数据，这里的rowKey支持多种类型，Long，double，Integer几种类型。 至于这里传什么类型的参数，取决于插入数据时rowKey的类型，虽然HBase里存储的都是字节数组，但是对类型是敏感的，如果类型对不上可能会出错。\n批量扫描 // 构建scan Scan scan = new Scan(); // 设置时间戳,计算时间差 Long timeDifference = 2L * 30L * 24L * 60L * 60L * 1000L; Long endTime = System.currentTimeMillis(); Long fromTime = endTime - timeDifference; // 设置时间过滤器 FilterList filterList = new FilterList(); Filter startTimeFilter = new SingleColumnValueFilter( DEFAULT_COLUMN_FAMILY.getBytes(), DATA_CREATE_TIME.getBytes(), CompareFilter.CompareOp.GREATER, Bytes.toBytes(fromTime) ); Filter endTimeFilter = new SingleColumnValueFilter( DEFAULT_COLUMN_FAMILY.getBytes(), DATA_CREATE_TIME.getBytes(), CompareFilter.CompareOp.LESS, Bytes.toBytes(endTime) ); filterList.addFilter(startTimeFilter); filterList.addFilter(endTimeFilter); scan.setFilter(filterList); // 获取结果集 ResultScanner resultScanner = hBaseTemplate.queryByScan(TABLE_NAME, scan); // 遍历结果集 try{ if (resultScanner != null) { resultScanner.forEach(result -\u0026gt; { List\u0026lt;Cell\u0026gt; cellList = result.listCells(); ... } } }finally{ if (resultScanner != null) { resultScanner.close(); } } 批量查询可以通过queryByScan()方法实现，第一个参数是表名，第二个参数是scan，通过构建不同的scan来查询，过滤器也是在构建scan对象是添加的，可以添加多个过滤器。\n需要注意的是这里的ResultScanner类，在遍历结果集时需要使用try-finally结构，在使用完resultScanner对象之后关闭该对象。HBase官方文档上强调了这一点。因此在使用ResultScanner对象时需要格外注意。\n常见过滤器：\n行健过滤器：RowFilter\n列族过滤器：FamilyFilter\n值过滤器：ValueFilter\n列过滤器：QualifierFilter\n单列值过滤器：SingleColumnValueFilter(会返回满足条件的行)\n单列值排除过滤器：SingleColumnExcludeFilter(返回排除了该列的结果，与单列值过滤器相反)\n前缀过滤器：PrefixFilter(这个过滤器是针对行健的，在构造方法中传入字节数组形式的内容，过滤器会去匹配行健)\n页数过滤器：PageFilter(使用pageFilter过滤器的时候需要注意，并不是设置了页数大小就能返回相应数目的结果)\nString tableName = \u0026quot;RECOMMEND_ENGINE_DATA_MODEL\u0026quot;; Scan scan = new Scan(); PageFilter pageFilter = new PageFilter(1); scan.setFilter(pageFilter); ResultScanner resultScanner = hBaseDao.queryByScan(tableName, scan); try{ resultScanner.forEach(result -\u0026gt; { result.listCells().forEach(cell -\u0026gt; { // process }); }finally{ if (resultScanner != null) { resultScanner.close(); } } 上面这段代码中设置了页面大小为1，预期是返回一条数据，但是结果会返回两条数据，这时返回的结果数会取决于regionServer的数量。\n如果是FilterList，FilterList的顺序会影响PageFilter的效果。\n一般比较型过滤器，需要用CompareFilter.CompareOp中的比较运算符。所有的过滤器都是用Scan对象去设置。\n多过滤器查询 String tableName = \u0026quot;HBase-component\u0026quot;; Scan scan = new Scan(); PageFilter pageFilter = new PageFilter(1); SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter( \u0026quot;CF1\u0026quot;.getBytes(), \u0026quot;col1\u0026quot;.getBytes(), CompareFilter.CompareOp.EQUAL, new SubstringComparator(\u0026quot;group\u0026quot;)); singleColumnValueFilter.setFilterIfMissing(true); FilterList filterList = new FilterList(); filterList.addFilter(singleColumnValueFilter); filterList.addFilter(pageFilter); scan.setFilter(filterList); ResultScanner resultScanner = hBaseDao.queryByScan(tableName, scan); try { resultScanner.forEach(result -\u0026gt; { result.listCells().forEach(cell -\u0026gt; { System.out.println( \u0026quot; row:\u0026quot; + Bytes.toString(CellUtil.cloneRow(cell)) + \u0026quot; family:\u0026quot;+ Bytes.toString(CellUtil.cloneFamily(cell)) + \u0026quot; qualifier: \u0026quot; + Bytes.toString(CellUtil.cloneQualifier(cell))+ \u0026quot; value:\u0026quot; + Bytes.toString(CellUtil.cloneValue(cell))); }); }); } finally { if (resultScanner != null) { resultScanner.close(); } } 多过滤器需要用到FilterList，也是直接设置到Scan对象中。多过滤器的时候需要注意过滤器的顺序问题，例如上面代码中如果将两个过滤器调换顺序，查询的结果也是不一样的。\n结果集的映射 在HBase中，默认所有的顺序都是按照字母序排列，例如CF1列族下有多个列：col1、col2、col3，那么在遍历结果集时，listCells()中的cell的顺序总是按照列名的字母序来排列的。\n所以cellList.get(0)就是对应col1中的数据，cellList.get(1)就是对应col2中的数据，cellList.get(2)就是对应col3中的数据。\n如果列名为a、b、c那分别对应的下标为cellList.get(0)、cellList.get(1)、cellList.get(2)\n",
      "summary": "摘要:记录自己写的一个基于SpringBoot操作HBase的组件，支持kerberos认证方式访问，本文相当于API文档。\nHBase 组件接口文档 源码地址：https://github.com/YaoQi17/HBase-Component\n使用说明 基本概念\ntable: 表\ncolumnFamily:列族，一个表下可以有多个列族，但是不建议设置多个列族，HBase建议设计长窄型的表而不是短宽型。\nqualifier:列，一个列族下可以有多列，一个表中的列可以是不对齐的，但是这样效率不高，同一张表中的列最好是相同的。\ncell:一列数据下的一个单元格，一个列下可以有多个单元格，根据版本号区分，默认每次读取最新版本的数据，cell下的存储是数据本身。\nrow: 行，多列数据组成一行，一行中有多个qualifier。\nrowKey: 行健，用于唯一标识一行数据，一行下有多列，行健的设计直接关系到查询的效率。\nHBase配置 以下配置为最基础配置，缺一不可。\nHBase: conf: quorum: 192.168.80.234:2181,192.168.80.235:2181,192.168.80.241:2181 znodeParent: /hbase-unsecure #如果有更多配置，写在config下，例如： #config: # key: value # key: value 如果需要更多配置，需要在config中配置，以key-value的形式书写。\n参数说明 quorum是HBase中zookeeper的配置，znodeParent是HBase配置在zookeeper中的路径。\n简单示例 引入组件jar包：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.semptian.hbase.component\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hbase-component\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 在需要的地方注入HBaseOperations接口，该接口的实现类是HBaseTemplate，通过这个类来操作HBase。\n@Autowired private HBaseOperations hBaseDao; 查询一条数据，通过rowKey查询：\npublic void testQueryTable() { Result result = hBaseDao.queryByTableNameAndRowKey( \u0026quot;LBS\u0026quot;, 9223372036854775803L); System.out.println(result.isEmpty()); result.listCells().forEach(cell -\u0026gt; { System.out.println( \u0026quot;row:\u0026quot; + Bytes.toLong(CellUtil.cloneRow(cell)) + \u0026quot;,family:\u0026quot;+ Bytes.toString(CellUtil.cloneFamily(cell)) + \u0026quot;, qualifier: \u0026quot; + Bytes."
    },{
      "title": "工厂模式",
      "url": "https://qiref.github.io/post/2019/07/11/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/",
      "content": "摘要：详细结束工厂模式（Factory Pattern）的使用，以及在Java中的实现方式。\n简介 工厂模式（Factory Pattern）是 Java 中最常用的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。工厂模式主要是解决创建对象的问题，典型的应用就是在spring中的IOC，反转控制，反转控制就是把创建对象的权限交给框架，所以spring就是一个生产对象的工厂。\n思路 工厂模式的思路就是设计一个产生对象的机制，让生产对象的过程交给第三方，在工厂模式中，不会对客户端暴露创建逻辑，并且使用通用接口接收新创建的对象。\n实现过程 新建抽象的接口 新建具体的实体类，实现抽象的接口 创建实例化对象的工厂 在客户端中通过工厂创建具体的实体对象，对象可以用抽象接口接收。 这种方式是最简单的实现方式：\n// 创建接口 public interface Shape { void draw(); } // 创建实体类Circle public class Circle implements Shape { @Override public void draw() { System.out.println(\u0026quot;drawing a circle\u0026quot;); } } // 创建实体类Rectangle public class Rectangle implements Shape { @Override public void draw() { System.out.println(\u0026quot;drawing a Rectangle\u0026quot;); } } // 创建实体类Square public class Square implements Shape { @Override public void draw() { System.out.println(\u0026quot;drawing a square\u0026quot;); } } 然后创建工厂类，生成对应的实体类。\npublic class ShapeFactory { public static Shape getShapes(String shapeType) { if (shapeType == null) { System.out.println(\u0026quot;shapeType is null\u0026quot;); throw new RuntimeException(); } else if (shapeType.equalsIgnoreCase(\u0026quot;Rectangle\u0026quot;)) { return new Rectangle(); } else if (shapeType.equalsIgnoreCase(\u0026quot;Square\u0026quot;)) { return new Square(); }else if(shapeType.equalsIgnoreCase(\u0026quot;Circle\u0026quot;)){ return new Circle(); }else { System.out.println(\u0026quot;nothing to do\u0026quot;); return null; } } // 测试简单工厂模式 @Test public void testSimpleFactoryPattern(){ Shape circle = ShapeFactory.getShapes(\u0026quot;circle\u0026quot;); circle.draw(); Shape rectangle = ShapeFactory.getShapes(\u0026quot;Rectangle\u0026quot;); rectangle.draw(); Shape square = ShapeFactory.getShapes(\u0026quot;Square\u0026quot;); square.draw(); } } 这种方式实现工厂模式很简单，但是缺点也很明显，比如，在增加一个实现Shape接口的实体类，又需要去修改ShapeFactory中的代码，这样其实不符合设计模式的原则，对扩展开放，对修改关闭。\n工厂模式的改进 分析一下，之所以每新增一个类都需要去修改工厂的代码，是因为在工厂中，生成类的代码太具体了，要想改变这种情况，就需要把这个工厂生成类实例的过程变得抽象化，在Java中，生成对象的方法不止一种，还可以利用反射机制，工厂接收的是和类相关的参数，可以把这个参数换成需要生成实例的类，这样工厂中生成类的代码就很抽象了。具体代码如下：\npublic class ShapeFactory { public static Shape getClass(Class\u0026lt;? extends Shape\u0026gt; clazz) { Shape shape = null; try { // 通过反射生成一个类的实例 shape = (Shape) Class.forName(clazz.getName()).newInstance(); } catch (InstantiationException e) { e.printStackTrace(); } catch (IllegalAccessException e) { e.printStackTrace(); } catch (ClassNotFoundException e) { e.printStackTrace(); } return shape; } // 测试反射改进后的工厂 @Test public void testReflectShapeFac(){ Shape rectangle = ShapeFactory.getClass(Rectangle.class); rectangle.draw(); Shape square = ShapeFactory.getClass(Square.class); square.draw(); Shape circle = ShapeFactory.getClass(Circle.class); circle.draw(); } } 这样工厂的生产过程就很抽象了，但是还有一个问题，这个工厂只能生成实现Shape接口的类实例，如果出现了另外一种接口，就有需要新增一个工厂，这样也未尝不可，因为只是扩展而已，但是又出现了一个新的问题，这些工厂中的生产对象的代码都差不多，只是强转的接口不同，代码还是有优化的空间的。工厂可以进一步抽象：\n// 改进后的工厂方法 public static \u0026lt;T\u0026gt; T getClass(Class\u0026lt;? extends T\u0026gt; clazz){ T obj = null; try { obj= (T) Class.forName(clazz.getName()).newInstance(); } catch (InstantiationException e) { e.printStackTrace(); } catch (IllegalAccessException e) { e.printStackTrace(); } catch (ClassNotFoundException e) { e.printStackTrace(); } return obj; } 借助配置文件的工厂模式 到这里，可以去思考一下在spring中，是怎么利用工厂模式的，首先你需要去xml中配置你需要实例化的类，然后读取这个配置文件，通过反射生成这个类的实例返回。其实这里也可以简单模仿一下：\n// 解析Properties配置文件 public class PropertiesUtil { private static Properties properties = new Properties(); public static String getPackageByName(String name) { return properties.getProperty(name); } // 解析配置文件 private static Map\u0026lt;String, String\u0026gt; parseProperties() { Map\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); InputStream inputStream = PropertiesUtil.class.getClassLoader().getResourceAsStream(\u0026quot;application.properties\u0026quot;); try { properties.load(inputStream); } catch (IOException e) { System.err.println(\u0026quot;file is not exists\u0026quot;); e.printStackTrace(); } return map; } } 配置文件中的内容如下：\n# 类名和包名的映射 circle=com.factory.pattern.simple.Circle rectangle=com.factory.pattern.simple.Rectangle square=com.factory.pattern.simple.Square 然后根据提供的配置文件的报名，通过反射实例化对应的类：\npublic class ConfigFactory { // 通过配置文件中的包名生成实例 public static \u0026lt;T\u0026gt; T getNewInstance(String className) { String packageName = PropertiesUtil.getPackageByName(className); try { return (T) Class.forName(packageName).newInstance(); } catch (InstantiationException e) { e.printStackTrace(); } catch (IllegalAccessException e) { e.printStackTrace(); } catch (ClassNotFoundException e) { e.printStackTrace(); } return null; } } 这种方式需要需要注意的是，加载配置文件一定是在调用工厂的前面，因为需要读取报名，把对应的数据读到内存中，这也就是spring中先启动容器的原因。\n",
      "summary": "摘要：详细结束工厂模式（Factory Pattern）的使用，以及在Java中的实现方式。\n简介 工厂模式（Factory Pattern）是 Java 中最常用的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。工厂模式主要是解决创建对象的问题，典型的应用就是在spring中的IOC，反转控制，反转控制就是把创建对象的权限交给框架，所以spring就是一个生产对象的工厂。\n思路 工厂模式的思路就是设计一个产生对象的机制，让生产对象的过程交给第三方，在工厂模式中，不会对客户端暴露创建逻辑，并且使用通用接口接收新创建的对象。\n实现过程 新建抽象的接口 新建具体的实体类，实现抽象的接口 创建实例化对象的工厂 在客户端中通过工厂创建具体的实体对象，对象可以用抽象接口接收。 这种方式是最简单的实现方式：\n// 创建接口 public interface Shape { void draw(); } // 创建实体类Circle public class Circle implements Shape { @Override public void draw() { System.out.println(\u0026quot;drawing a circle\u0026quot;); } } // 创建实体类Rectangle public class Rectangle implements Shape { @Override public void draw() { System.out.println(\u0026quot;drawing a Rectangle\u0026quot;); } } // 创建实体类Square public class Square implements Shape { @Override public void draw() { System."
    },{
      "title": "Git学习笔记",
      "url": "https://qiref.github.io/post/2019/06/16/git%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/",
      "content": "摘要：学习Git的一些笔记，记录了Git的一些常见命令，以及Git中文件的生命周期。\ngit文件状态变化 状态说明： 状态转换: 正常流程 git clone 从远程拉一个工程下来 增加一个文件 git status 查看状态 git add 把文件从Untracked\u0026ndash;\u0026gt;Staged git rm \u0026ndash;cached git commit 提交 git push 把master分支的内容提交到远端 git diff 查看变化 操作实例 回滚还没有commit的文件 操作实例 回滚某个提交 操作实例 分支操作 查看分支 创建分支 分支上增加内容 推送分支到远程 两个分支进行比较 分支合并到master上 从远程拉一个分支 tag操作 其他 git覆盖本地修改，pull 远程 git修改上一次commit message git重命名分支 git文件状态变化 状态说明： Untracked: 刚新加的文件，还没有纳入git管理范围\nUnModified: 已经committed的文件\nModified: 已经committed的文件，通过vi等修改后，就变成Modified\nStaged: git add 后的文件\n状态转换: Untracked-\u0026gt;Staged: 通过git add 来完成\nUnModified-\u0026gt;Modified: 修改文件内容来完成，比如vi命令\nModified-\u0026gt;Staged: 通过git add 来完成\nUnModified-\u0026gt;Untracked: 通过git rm 来完成\nStaged-\u0026gt;UnModified: 通过git commit 来完成\n正常流程 git clone 从远程拉一个工程下来 $ git clone git@github.com:sotrip/gittest.git Cloning into 'gittest'... warning: You appear to have cloned an empty repository. Checking connectivity... done. 增加一个文件 $ vi 1.txt #里面内容如下: the first line git status 查看状态 $ git status On branch master #表示我们目前在master分支上 Initial commit Untracked files: #有哪些文件是Untracked状态，有1.txt (use \u0026quot;git add ...\u0026quot; to include in what will be committed) 1.txt nothing added to commit but untracked files present (use \u0026quot;git add\u0026quot; to track) git add 把文件从Untracked\u0026ndash;\u0026gt;Staged $ git add 1.txt #成功后，没有输出 $ git status #再次查看 On branch master Initial commit Changes to be committed: #表示1.txt已经是staged了，可以被提交了 (use \u0026quot;git rm --cached ...\u0026quot; to unstage) #如果不想提交了，可以用git rm --cached 1.txt new file: 1.txt git rm \u0026ndash;cached 文件已经是staged了，但想要退回原来的状态\n$ git rm --cached 1.txt rm '1.txt' $ git status # 再来看又回来2.3这一步了 On branch master Initial commit Untracked files: (use \u0026quot;git add ...\u0026quot; to include in what will be committed) 1.txt $ git add 1.txt # 我们还是再加上 $ git status On branch master Initial commit Changes to be committed: # 1.txt 又改为staged状态 准备提交 (use \u0026quot;git rm --cached ...\u0026quot; to unstage) new file: 1.txt git commit 提交 $ git commit -m \u0026quot;first commit\u0026quot; #-m后面是我们这一次提交的注释 [master (root-commit) e6b0e7d] first commit 1 file changed, 1 insertion(+) create mode 100644 1.txt git push 把master分支的内容提交到远端 $ git push origin master Warning: Permanently added the RSA host key for IP address '*.*.*.*' to the list of known hosts. Counting objects: 3, done. Writing objects: 100% (3/3), 214 bytes | 0 bytes/s, done. Total 3 (delta 0), reused 0 (delta 0) To git@github.com:sotrip/gittest.git * [new branch] master -\u0026gt; master git diff 查看变化 命令概括\ngit diff #查看 Modified的文件，做了哪些修改 git diff --staged # 查看 Staged的文件，做了哪些修改 操作实例 $ vi 1.txt #在后面增加一行,变成如下 the first line the second line $ git status On branch master Your branch is up-to-date with 'origin/master'. Changes not staged for commit: #这个表示1.txt已经变为Modified了，not staged (use \u0026quot;git add ...\u0026quot; to update what will be committed) (use \u0026quot;git checkout -- ...\u0026quot; to discard changes in working directory) modified: 1.txt no changes added to commit (use \u0026quot;git add\u0026quot; and/or \u0026quot;git commit -a\u0026quot;) $ git diff #查看Modified的文件，修改了哪些地方 diff --git a/1.txt b/1.txt index 137b7fd..067030b 100644 --- a/1.txt +++ b/1.txt @@ -1 +1,2 @@ the first line +the second line $ git add 1.txt #把1.txt加入到staged中 $ git status On branch master Your branch is up-to-date with 'origin/master'. Changes to be committed: (use \u0026quot;git reset HEAD ...\u0026quot; to unstage) modified: 1.txt $ git diff #这个时候不会输出任何东西，因为没有Modified的文件了 $ git diff --staged #查看staged的文件和上一次commit有哪些修改 diff --git a/1.txt b/1.txt index 137b7fd..067030b 100644 --- a/1.txt +++ b/1.txt @@ -1 +1,2 @@ the first line +the second line 回滚还没有commit的文件 命令概括\n$ git reset HEAD 1.txt #文件已经Staged的了，用这个来回滚到Modified状态，但是内容不会回滚 $ git checkout 1.txt #如果文件是Modified，不想做修改了，恢复原样，使用这个 操作实例 $ git status On branch master Your branch is up-to-date with 'origin/master'. Changes to be committed: (use \u0026quot;git reset HEAD ...\u0026quot; to unstage) modified: 1.txt $ git diff --staged diff --git a/1.txt b/1.txt index 137b7fd..067030b 100644 --- a/1.txt +++ b/1.txt @@ -1 +1,2 @@ the first line +the second line $ git reset HEAD 1.txt #把1.txt 的状态由Staged变为Staged， 但是1.txt的内容不会变 Unstaged changes after reset: M1.txt $ git status On branch master Your branch is up-to-date with 'origin/master'. Changes not staged for commit: #可以看出1.txt 由Staged变为Modified (use \u0026quot;git add ...\u0026quot; to update what will be committed) (use \u0026quot;git checkout -- ...\u0026quot; to discard changes in working directory) modified: 1.txt no changes added to commit (use \u0026quot;git add\u0026quot; and/or \u0026quot;git commit -a\u0026quot;) $ cat 1.txt #查看内容，发现 1.txt的内容并没有回滚 the first line the second line $ git checkout 1.txt #回滚 $ git status On branch master Your branch is up-to-date with 'origin/master'. nothing to commit, working directory clean $ cat 1.txt #内容已经回滚 the first line 回滚某个提交 命令概括\n$ git revert HEAD #回滚上一次提交 $ git revert HEAD^ #回滚上上次提交 $ git revert #commit no# #回滚某一次提交 $ git revert -m 1 #commit no# #回滚某一次merge提交 操作实例 增加了2.txt 并提交了，现在想回滚\n$ vi 2.txt #在里面增加内容 $ git add 2.txt #把文件由Untracked 变为 Staged $ git commit -m \u0026quot;2.txt commit\u0026quot; #提交 $ git log 查看提交日志 commit 710c5e84bd02e5d041b537b8732b9e80fee257a1 #这个是我们2.txt的提交 Author: xxx Date: Thu Apr 7 22:10:00 2016 +0800 2.txt commit commit e6b0e7d844154d5473a37baed2ef56807dca16b3 Author: xxx Date: Wed Apr 6 22:42:44 2016 +0800 first commit $ git show 710c5e84bd02e5d041b537b8732b9e80fee257a1 #可以查看这次修改的内容 $ git revert 710c5e84bd02e5d041b537b8732b9e80fee257a1 #回滚提交 [master d3ab103] Revert \u0026quot;2.txt commit\u0026quot; 1 file changed, 1 deletion(-) delete mode 100644 2.txt $ git revert -m 1 9d55502f0e05bef943d6b4e7cf57aafe08e42d63 #保留master,去除别的分支提交过来的 分支操作 查看分支 $ git branch #查看目前有哪些分支 * master #只有一个分支,\u0026quot;*\u0026quot;表示当前是在master分支上 创建分支 $ git branch first-branch #打出第一个分支，名字是first-branch $ git branch first-branch # 分支已经有了 * master #\u0026quot;*\u0026quot;表示当前是在master分支上 $ git checkout first-branch Switched to branch 'first-branch' $ git branch * first-branch #已经成功切换到自己打的分支上了 master 分支上增加内容 $ vi 2.txt $ cat 2.txt #增加的内容如下 edit in first-branch $ git add 2.txt $ git commit -m \u0026quot;2.txt commit in first-branch\u0026quot; #在分支上提交 [first-branch 9abd8f2] 2.txt commit in first-branch 1 file changed, 2 insertions(+) create mode 100644 2.txt 推送分支到远程 推送到远程分支需要添加一个远程地址, 同时设置用户和邮箱\ngit remote add origin xxx.git # 设置用户 git config user.name \u0026quot;foo\u0026quot; # 设置邮箱 git config user.email \u0026quot;bar@mail.com\u0026quot; # 如果加上 --global 参数, 则是全局配置, 否则是设置当前project的的配置 git config --global $ git push origin first-branch Counting objects: 7, done. Delta compression using up to 4 threads. Compressing objects: 100% (5/5), done. Writing objects: 100% (7/7), 692 bytes | 0 bytes/s, done. Total 7 (delta 0), reused 0 (delta 0) To git@github.com:sotrip/gittest.git * [new branch] first-branch -\u0026gt; first-branch 两个分支进行比较 $ git diff master first-branch # 比较master与first-branch diff --git a/2.txt b/2.txt new file mode 100644 index 0000000..b09edf1 --- /dev/null +++ b/2.txt # 表示first-branch上多了一个2.txt @@ -0,0 +1,2 @@ +edit in first-branch+ 分支合并到master上 $ git checkout master $ git merge first-branch #把first-branch的内容合并到master上 Updating d3ab103..9abd8f2 Fast-forward 2.txt | 2 ++ 1 file changed, 2 insertions(+) create mode 100644 2.txt $ ls 1.txt 2.txt $ cat 2.txt edit in first-branch $ git log commit 9abd8f2d8fe7c08ca246464552dae25397694582 Author: xxx Date: Thu Apr 7 22:26:26 2016 +0800 2.txt commit in first-branch #在first-branch上提交的内容也显示在日志中 从远程拉一个分支 有两个办法，第一种是:\n$ git fecth origin $ git checkout first-branch Branch first-branch set up to track remote branch first-branch from origin. Switched to a new branch 'first-branch' 第二个办法:\n$ git checkout -t origin/first-branch Branch first-branch set up to track remote branch first-branch from origin. Switched to a new branch 'first-branch' tag操作 tag一般维护一个只读的版本，不再进行修改\n$ git tag -a v1.0 -m \u0026quot;v1.0 ready for publish\u0026quot; #创建一个tag ,名字是\u0026quot;v1.0\u0026quot; $ git tag #查看tag v1.0 $ git push origin v1.0 #推送tag 到github上 Counting objects: 1, done. Writing objects: 100% (1/1), 162 bytes | 0 bytes/s, done. Total 1 (delta 0), reused 0 (delta 0) To git@github.com:sotrip/gittest.git * [new tag] v1.0 -\u0026gt; v1.0 $ git checkout v1.0 #切换到这个tag 上 注意 最好不要在tag进行修改东西，就把tag维护成一个只读的版本\n其他 $ git rm 2.txt #删除2.txt 这个文件 $ git remote -v #可以查看远程的git的地址 git覆盖本地修改，pull 远程 $ git fetch --all $ git reset --hard origin/master $ git pull git修改上一次commit message $ git commit --amend git重命名分支 $ git branch -m oldname newname $ git push origin :oldbranch # 此命令会在origin仓库中匹配oldbranch分支，然后删除它。 ",
      "summary": "摘要：学习Git的一些笔记，记录了Git的一些常见命令，以及Git中文件的生命周期。\ngit文件状态变化 状态说明： 状态转换: 正常流程 git clone 从远程拉一个工程下来 增加一个文件 git status 查看状态 git add 把文件从Untracked\u0026ndash;\u0026gt;Staged git rm \u0026ndash;cached git commit 提交 git push 把master分支的内容提交到远端 git diff 查看变化 操作实例 回滚还没有commit的文件 操作实例 回滚某个提交 操作实例 分支操作 查看分支 创建分支 分支上增加内容 推送分支到远程 两个分支进行比较 分支合并到master上 从远程拉一个分支 tag操作 其他 git覆盖本地修改，pull 远程 git修改上一次commit message git重命名分支 git文件状态变化 状态说明： Untracked: 刚新加的文件，还没有纳入git管理范围\nUnModified: 已经committed的文件\nModified: 已经committed的文件，通过vi等修改后，就变成Modified\nStaged: git add 后的文件\n状态转换: Untracked-\u0026gt;Staged: 通过git add 来完成\nUnModified-\u0026gt;Modified: 修改文件内容来完成，比如vi命令\nModified-\u0026gt;Staged: 通过git add 来完成"
    },{
      "title": "SpringBoot中使用AOP",
      "url": "https://qiref.github.io/post/2019/06/16/springboot%E4%B8%AD%E4%BD%BF%E7%94%A8aop/",
      "content": "摘要：Spring中如何使用注解实现面向切面编程，以及如何使用自定义注解。\n场景 比如用户登录，每个请求发起之前都会判断用户是否登录，如果每个请求都去判断一次，那就重复地做了很多事情，只要是有重复的地方，就有优化的空间。现在就把重复的地方抽取出来，暂且称之为 \u0026quot; 拦截器 \u0026ldquo;，然后每次请求之前就先经过\u0026rdquo; 拦截器 \u0026ldquo;，这个编程的思想就可以称之为面向切面编程。AOP(Aspect Oriented Program)\n最典型的应用就是事务管理和权限验证，还有日志统计，下文中的案例就是接口执行时间的统计。\nspring中使用AOP（基于注解） 不得不说注解是个很巧妙的设计，使用很少量的信息描述数据，这类数据称之为元数据，描述数据的数据。关于注解的理解，这里有个传送门：http://www.importnew.com/10294.html\n下面的案例是在springBoot中进行的，直观地感受一下如何使用注解完成AOP。\n@Service public class UserService { public void getUser() { //To do something System.out.println(\u0026quot;getUser() has been called\u0026quot;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } } 切面是这样定义的：\n@Component @Aspect public class LoggerAspect { /** * getUser()执行之前执行 */ @Before(\u0026quot;execution(* com.springboot.demo.service.UserService.getUser(..))\u0026quot;) public void callBefore() { System.out.println(\u0026quot;before call method\u0026quot;); System.out.println(\u0026quot;begin........................\u0026quot;); } /** * getUser()执行之后执行 */ @After(\u0026quot;execution(* com.springboot.demo.service.UserService.getUser(..))\u0026quot;) public void callAfter() { System.out.println(\u0026quot;after call method\u0026quot;); System.out.println(\u0026quot;end..............................\u0026quot;); } } 来个单元测试验证一下：\n@RunWith(SpringRunner.class) @SpringBootTest public class UserServiceTest { @Autowired private UserService userService; @Test public void getUserTest() { userService.getUser(); } } 案例 假如有以下的业务场景: UserService业务类中有个getUser()这个方法，现在想统计一下这个方法的执行时间，可能需要测试这个接口的性能。通常做法是方法开始时获取系统当前时间，然后方法结束时获取当前时间，最后 excuteTime=endTime-startTime。\n如果现在不仅是这个方法需要统计，还有getUserByName()、getUserById()需要统计，上述的方法明显很笨了。\n使用AOP怎么解决？ 抽取公共部分为一个切面，方法执行前记录时间，然后执行目标方法，最后，目标方法执行完成之后再获取一次系统时间。\n具体实现如下：在LoggerAspect中再写一个方法，记录getUser()方法的执行时间。\n/** * 记录执行时间 * @param point 切点 * @return * @throws Throwable */ @Around(\u0026quot;execution(* com.springboot.demo.service.UserService.getUser(..))\u0026quot;) public Object getMethodExecuteTime(ProceedingJoinPoint point) throws Throwable { System.out.println(\u0026quot;---------------getMethodExecuteTime------------------\u0026quot;); long startTime = System.currentTimeMillis(); //调用目标方法 Object result = point.proceed(); long endTime = System.currentTimeMillis(); long executeTime = endTime - startTime; System.out.println(\u0026quot;executeTime=\u0026quot; + executeTime + \u0026quot;------------------\u0026quot;); return result; } @Around将目标方法再次封装，控制了它的调用时机，以此来记录getUser()的执行时间。但是好像并没有达到记录UserService中的多个方法的执行时间的目的。\n@Around(\u0026ldquo;execution(* com.springboot.demo.service.UserService.getUser(..))\u0026rdquo;)\n其中指定了切点是getUser()这个方法，这里的表达式很丰富，可以设置为：\n* com.springboot.demo.service.UserService.*(..)\n表示UserService中的每一个方法都是切点，甚至可以是这样：\n* com.springboot.demo.service..(..)\n表示service包下的所有类的所有方法都是切点，但是这样很明显不够灵活，如果能自定义地控制就更好了。\n自定义一个注解 如果用一个注解标注某个方法需要记录其执行时间，岂不是更加优雅。\n/** * @Description 标注某个方法需要记录执行时间 * @Author YaoQi * @Date 2018/7/6 15:51 */ @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface Logger { String value() default \u0026quot;\u0026quot;; } 注解是用来描述数据的，上面的这个注解的意思是：这个注解将作用于方法，并且在运行时有效。但是这样只是标注了，如何读取这个标注的信息？\n在LoggerAspect中加入这一个方法：\n/** * @param point * @return * @throws Throwable */ @Around(\u0026quot;@annotation(com.springboot.demo.annotation.Logger)\u0026quot;) public Object getMethodExecuteTimeForLogger(ProceedingJoinPoint point) throws Throwable { System.out.println(\u0026quot;---------------getMethodExecuteTime------------------\u0026quot;); long startTime = System.currentTimeMillis(); Object result = point.proceed(); long endTime = System.currentTimeMillis(); long executeTime = endTime - startTime; System.out.println(\u0026quot;executeTime=\u0026quot; + executeTime + \u0026quot;------------------\u0026quot;); return result; } 哪个方法需要记录执行时间就将@Logger放在对应的方法上：\n@Logger public void getUser() { System.out.println(\u0026quot;getUser() has been called\u0026quot;); } ",
      "summary": "摘要：Spring中如何使用注解实现面向切面编程，以及如何使用自定义注解。\n场景 比如用户登录，每个请求发起之前都会判断用户是否登录，如果每个请求都去判断一次，那就重复地做了很多事情，只要是有重复的地方，就有优化的空间。现在就把重复的地方抽取出来，暂且称之为 \u0026quot; 拦截器 \u0026ldquo;，然后每次请求之前就先经过\u0026rdquo; 拦截器 \u0026ldquo;，这个编程的思想就可以称之为面向切面编程。AOP(Aspect Oriented Program)\n最典型的应用就是事务管理和权限验证，还有日志统计，下文中的案例就是接口执行时间的统计。\nspring中使用AOP（基于注解） 不得不说注解是个很巧妙的设计，使用很少量的信息描述数据，这类数据称之为元数据，描述数据的数据。关于注解的理解，这里有个传送门：http://www.importnew.com/10294.html\n下面的案例是在springBoot中进行的，直观地感受一下如何使用注解完成AOP。\n@Service public class UserService { public void getUser() { //To do something System.out.println(\u0026quot;getUser() has been called\u0026quot;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } } 切面是这样定义的：\n@Component @Aspect public class LoggerAspect { /** * getUser()执行之前执行 */ @Before(\u0026quot;execution(* com.springboot.demo.service.UserService.getUser(..))\u0026quot;) public void callBefore() { System.out.println(\u0026quot;before call method\u0026quot;); System.out.println(\u0026quot;begin........................\u0026quot;); } /** * getUser()执行之后执行 */ @After(\u0026quot;execution(* com.springboot.demo.service.UserService.getUser(..))\u0026quot;) public void callAfter() { System."
    },{
      "title": "Another Note on A blogdown Tutorial",
      "url": "https://qiref.github.io/note/2017/06/14/another-note/",
      "content": "I just discovered an awesome tutorial on blogdown written by Alison. I have to admit this is the best blogdown tutorial I have seen so far.\n",
      "summary": "I just discovered an awesome tutorial on blogdown written by Alison. I have to admit this is the best blogdown tutorial I have seen so far."
    },{
      "title": "About Hugo XMin",
      "url": "https://qiref.github.io/about/",
      "content": "XMin is the first Hugo theme I have designed. The original reason that I wrote it was I needed a minimal example of Hugo themes when I was writing the blogdown book. Basically I wanted a simple theme that supports a navigation menu, a home page, other single pages, lists of pages, blog posts, categories, tags, and RSS. That is all. Nothing fancy. In terms of CSS and JavaScript, I really want to keep them minimal. In fact, this theme does not contain any JavaScript code at all, although on this example website I did introduce some JavaScript code (still relatively simple anyway). The theme does not contain any images, either, and is pretty much a plain-text theme.\nThe theme name \u0026ldquo;XMin\u0026rdquo; can be interpreted as \u0026ldquo;Xie\u0026rsquo;s Minimal theme\u0026rdquo; (Xie is my last name) or \u0026ldquo;eXtremely Minimal theme\u0026rdquo;.\nhugo.yaml (the config file) For this example site, I defined permalinks for two sections, post and note, so that the links to pages under these directories will contain the date info, e.g., https://xmin.yihui.org/post/2016/02/14/a-plain-markdown-post/. This is optional, and it is up to your personal taste of URLs.\npermalinks: note: \u0026quot;/note/:year/:month/:day/:slug/\u0026quot; post: \u0026quot;/post/:year/:month/:day/:slug/\u0026quot; You can define the menu through menu.main, e.g.,\nmenu: main: - name: Home url: \u0026quot;\u0026quot; weight: 1 - name: About url: \u0026quot;about/\u0026quot; weight: 2 - name: Categories url: \u0026quot;categories/\u0026quot; weight: 3 - name: Tags url: \u0026quot;tags/\u0026quot; weight: 4 - name: Subscribe url: \u0026quot;index.xml\u0026quot; Alternatively, you can add menu: main to the YAML metadata of any of your pages, so that these pages will appear in the menu.\nThe page footer can be defined in .Params.footer, and the text is treated as Markdown, e.g.,\nparams: footer: \u0026quot;\u0026amp;copy; [Yihui Xie](https://yihui.org) 2017 -- {Year}\u0026quot; Here {Year} means the year in which the site is built (usually the current year).\nCustom layouts There are two layout files under layouts/partials/ that you may want to override: head_custom.html and foot_custom.html. This is how you inject arbitrary HTML code to the head and foot areas. For example, this site has a file layouts/partials/foot_custom.html to support LaTeX math via KaTeX and center images automatically:\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;//cdn.jsdelivr.net/npm/katex/dist/katex.min.css\u0026quot;\u0026gt; \u0026lt;script src=\u0026quot;//cdn.jsdelivr.net/combine/npm/katex/dist/katex.min.js,npm/katex/dist/contrib/auto-render.min.js,npm/@xiee/utils/js/render-katex.js\u0026quot; defer\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;//cdn.jsdelivr.net/npm/@xiee/utils/js/center-img.min.js\u0026quot; defer\u0026gt;\u0026lt;/script\u0026gt; You can certainly enable highlight.js for syntax highlighting by yourself through head_custom.html and foot_custom.html if you want.\nIf you do not like the default fonts (e.g., Palatino), you may provide your own static/css/fonts.css under the root directory of your website to override the fonts.css in the theme.\nOther features I could have added more features to this theme, but I decided not to, since I have no intention to make this theme feature-rich. However, I will teach you how. I have prepared several examples via pull requests at https://github.com/yihui/hugo-xmin/pulls, so that you can see the implementations of these features when you check out the diffs in the pull requests. For example, you can:\nEnable Google Analytics\nEnable Disqus comments\nEnable highlight.js for syntax highlighting of code blocks\nDisplay categories and tags on a page\nAdd a table of contents\nAdd a link in the footer of each page to \u0026ldquo;Edit this page\u0026rdquo; on Github\nTo fully understand these examples, you have to read the section on Hugo templates in the blogdown book.\nDesign philosophy Lastly, a few words about my design philosophy for this theme: I have been relying on existing frameworks like Bootstrap for years since I\u0026rsquo;m not really a designer, and I was always scared by the complexity of CSS.\nWhen I started writing this theme, I asked myself, \u0026ldquo;What if I just write from scratch?\u0026rdquo; No Bootstrap. No Normalize.css. I don\u0026rsquo;t care about IE (life could be so much easier without IE) or inconsistencies among browsers (for personal websites). As long as the theme looks okay in Chrome, Firefox, and Safari, I\u0026rsquo;m done. Thanks to the simplicity of Markdown, you cannot really produce very complicated HTML, and I think styling the HTML output from Markdown is much simpler than general HTML documents. For example, I do not need to care much about form elements like textareas or buttons.\nAfter I finished this theme, I started to wonder why I\u0026rsquo;d need normalize.css at all. The default appearance of modern browsers actually looks pretty good in my eyes, after I tweak the typeface a little bit.\nCompared to inconsistencies across browsers, I care much more about these properties of HTML elements:\nTables should always be centered, and striped tables are easier to read especially when they are wide. Tables should not have vertical borders. An image should be centered if it is the only child element of a paragraph. The max-width of images, videos, and iframes should be 100%. I hope you can enjoy this theme. The source code is on Github. Happy hacking!\n",
      "summary": "XMin is the first Hugo theme I have designed. The original reason that I wrote it was I needed a minimal example of Hugo themes when I was writing the blogdown book. Basically I wanted a simple theme that supports a navigation menu, a home page, other single pages, lists of pages, blog posts, categories, tags, and RSS. That is all. Nothing fancy. In terms of CSS and JavaScript, I really want to keep them minimal."
    }]
}