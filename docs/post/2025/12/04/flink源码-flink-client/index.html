<!DOCTYPE html>
<html lang="cn-zh">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Flink源码-Flink Client | 大道至简</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PGMJFXZJRT"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/languages/scala.min.js"></script>

<script>
hljs.configure({languages: []});
hljs.highlightAll();
</script>

<link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/github.min.css"  rel="stylesheet">

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-PGMJFXZJRT');
</script>

<link rel="stylesheet" href="/css/custom.css">
<link rel="stylesheet" href="/css/heatmap.css">
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/post/">Posts</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/note/">Note</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
      <li class="search-container">
        <input type="text" id="search-input" placeholder="⌘ K" autocomplete="off">
        <div id="search-results"></div>
      </li>
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Flink源码-Flink Client</span></h1>

<h2 class="date">2025/12/04</h2>
<p class="terms">
  
  
  
  
  Tags: <a href="/tags/flink">Flink</a> 
  
  
</p>
</div>


<nav id="TableOfContents">
  <ul>
    <li><a href="#flink-client-layer-学习指南">Flink Client Layer 学习指南</a></li>
    <li><a href="#作业提交入口">作业提交入口</a></li>
    <li><a href="#transformation--streamgraph-转换">Transformation → StreamGraph 转换</a></li>
    <li><a href="#streamgraph--jobgraph-转换">StreamGraph → JobGraph 转换</a>
      <ul>
        <li><a href="#转换的具体工作">转换的具体工作</a></li>
      </ul>
    </li>
    <li><a href="#pipelineexecutor-体系">PipelineExecutor 体系</a></li>
    <li><a href="#clusterclient-体系">ClusterClient 体系</a></li>
    <li><a href="#cli-命令行工具">CLI 命令行工具</a></li>
    <li><a href="#packagedprogramjar-包加载">PackagedProgram（JAR 包加载）</a></li>
    <li><a href="#application-mode">Application Mode</a></li>
    <li><a href="#deployment-抽象">Deployment 抽象</a></li>
    <li><a href="#学习路线总结">学习路线总结</a></li>
    <li><a href="#调试实践步骤">调试实践步骤</a>
      <ul>
        <li><a href="#修改-filesinkdemo-添加调试代码">修改 FileSinkDemo 添加调试代码</a></li>
        <li><a href="#设置断点开始调试">设置断点开始调试</a></li>
        <li><a href="#阅读源码理解核心流程">阅读源码理解核心流程</a></li>
      </ul>
    </li>
    <li><a href="#实战手动启动集群并提交作业">实战：手动启动集群并提交作业</a>
      <ul>
        <li><a href="#启动本地-standalone-集群">启动本地 Standalone 集群</a></li>
        <li><a href="#通过-restclusterclient-提交作业">通过 RestClusterClient 提交作业</a></li>
      </ul>
    </li>
    <li><a href="#client-layer-的核心职责">Client Layer 的核心职责</a></li>
  </ul>
</nav>


<main>
<h2 id="flink-client-layer-学习指南">Flink Client Layer 学习指南</h2>
<p>根据 Flink 源码的架构，Client Layer 主要负责<strong>作业提交、图转换、集群交互</strong>。本文规划了一个从易到难、循序渐进的学习路径。</p>
<hr>
<h2 id="作业提交入口">作业提交入口</h2>
<p><strong>起点文件：</strong> <code>StreamExecutionEnvironment.java</code></p>
<pre><code class="language-java">// FileSinkDemo.java 中的这行代码是整个 Client Layer 的入口
env.execute(&quot;Local FileSystem Debug&quot;);

// 学习路径：
// 1. StreamExecutionEnvironment.execute()
// 2. StreamExecutionEnvironment.executeAsync()
// 3. StreamExecutionEnvironment.getStreamGraph()
// 4. PipelineExecutor.execute()
</code></pre>
<p><strong>学习方式：</strong></p>
<pre><code class="language-java">// 创建调试 Demo
public class ClientLayerLearningDemo {
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        StreamExecutionEnvironment env = 
            StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf);
        
        env.setParallelism(2);
        
        // 简单的作业
        DataStream&lt;String&gt; source = env.fromElements(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);
        source.map(x -&gt; x.toUpperCase()).print();
        
        // 在这里打断点，开始追踪 Client Layer 的执行流程
        env.execute(&quot;Client Layer Learning&quot;);
    }
}
</code></pre>
<p><strong>重点方法：</strong></p>
<ol>
<li><code>StreamExecutionEnvironment.execute()</code></li>
<li><code>StreamExecutionEnvironment.executeAsync(StreamGraph)</code></li>
<li><code>StreamExecutionEnvironment.getStreamGraph()</code></li>
<li><code>PipelineExecutor.execute()</code></li>
</ol>
<hr>
<h2 id="transformation--streamgraph-转换">Transformation → StreamGraph 转换</h2>
<p><strong>核心文件：</strong></p>
<ul>
<li><code>StreamGraph.java</code></li>
<li><code>StreamGraphGenerator.java</code></li>
</ul>
<p><strong>学习目标：</strong></p>
<ul>
<li>理解 <code>Transformation</code> 是什么（逻辑算子）</li>
<li>理解 <code>StreamGraph</code> 是什么（逻辑执行图）</li>
<li>理解如何从 Transformation 生成 StreamGraph</li>
</ul>
<p><strong>学习示例：</strong></p>
<pre><code class="language-java">public class StreamGraphLearningDemo {
    public static void main(String[] args) {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 每个算子都会生成一个 Transformation
        DataStream&lt;String&gt; source = env.fromData(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);
        DataStream&lt;String&gt; mapped = source.map(String::toUpperCase);
        DataStream&lt;String&gt; filtered = mapped.filter(x -&gt; !x.equals(&quot;B&quot;));
        filtered.print();

        // 查看 Transformation 列表
        List&lt;Transformation&lt;?&gt;&gt; transformations = env.getTransformations();
        System.out.println(&quot;Transformations count: &quot; + transformations.size());
        for (Transformation&lt;?&gt; t : transformations) {
            System.out.println(&quot;  - &quot; + &quot; [&quot; + t + &quot;]&quot;);
        }

        // 获取 StreamGraph（不清空 transformations）
        StreamGraph streamGraph = env.getStreamGraph(false);

        // 打印 StreamGraph 信息
        System.out.println(&quot;streamGraph Job Name: &quot; + streamGraph.getJobName());
        System.out.println(&quot;StreamNodes: &quot; + streamGraph.getStreamNodes());

        System.out.println(&quot;StreamEdges: &quot;);
        streamGraph
                .getStreamNodes()
                .forEach(
                        node -&gt; {
                            System.out.println(
                                    &quot; - node: &quot;
                                            + node.getOperatorName()
                                            + &quot; StreamEdge: &quot;
                                            + streamGraph.getStreamEdges(node.getId()));
                        });
    }
}



Transformations count: 3
  -  [OneInputTransformation{id=2, name='Map', outputType=String, 
      parallelism=12}]
  -  [OneInputTransformation{id=3, name='Filter', outputType=String, 
      parallelism=12}]
  -  [LegacySinkTransformation{id=4, name='Print to Std. Out', 
      outputType=String, parallelism=12}]

streamGraph Job Name: Flink Streaming Job
StreamNodes: [Source: Collection Source-1, Map-2, Filter-3, 
             Sink: Print to Std. Out-4]

StreamEdges: 
 - node: Source: Collection Source StreamEdge: [(Source: Collection Source-1 -&gt; Map-2, 
   typeNumber=0, outputPartitioner=REBALANCE, exchangeMode=UNDEFINED, 
   bufferTimeout=100, outputTag=null, uniqueId=0)]
 - node: Map StreamEdge: [(Map-2 -&gt; Filter-3, typeNumber=0, outputPartitioner=FORWARD, 
   exchangeMode=UNDEFINED, bufferTimeout=100, outputTag=null, uniqueId=0)]
 - node: Filter StreamEdge: [(Filter-3 -&gt; Sink: Print to Std. Out-4, typeNumber=0, 
   outputPartitioner=FORWARD, exchangeMode=UNDEFINED, bufferTimeout=100, 
   outputTag=null, uniqueId=0)]
 - node: Sink: Print to Std. Out StreamEdge: []
</code></pre>
<p><strong>关键类和方法：</strong></p>
<pre><code class="language-java">// 1. Transformation 体系（逻辑算子）
org.apache.flink.api.dag.Transformation
  ├── SourceTransformation          // Source 算子
  ├── OneInputTransformation        // map, filter 等单输入算子
  ├── TwoInputTransformation        // join, coGroup 等双输入算子
  ├── SinkTransformation            // Sink 算子
  └── PartitionTransformation       // shuffle, rebalance 等

// 2. StreamGraph 生成
StreamGraphGenerator.generate()
  └── StreamGraphGenerator.transformations()
      └── StreamGraphGenerator.transform(Transformation)
          └── 根据不同的 Transformation 类型生成 StreamNode 和 StreamEdge
</code></pre>
<p><strong>重点方法：</strong></p>
<ul>
<li><code>StreamGraphGenerator.generate()</code> — 生成 StreamGraph 入口</li>
<li><code>StreamGraphGenerator.transform()</code> — 转换每个 Transformation</li>
<li><code>StreamGraph.addNode()</code> / <code>StreamGraph.addEdge()</code> — 添加节点和边</li>
</ul>
<hr>
<h2 id="streamgraph--jobgraph-转换">StreamGraph → JobGraph 转换</h2>
<p><strong>核心文件：</strong></p>
<ul>
<li><code>StreamingJobGraphGenerator.java</code></li>
<li><code>JobGraph.java</code></li>
</ul>
<p><strong>学习目标：</strong></p>
<ul>
<li>理解 <code>JobGraph</code> 是什么（优化后的逻辑图，包含算子链）</li>
<li>理解 Operator Chain 优化（算子链）</li>
<li>理解 StreamNode → JobVertex 的转换</li>
<li>理解 StreamEdge → JobEdge 的转换</li>
</ul>
<p><strong>为什么需要转换？</strong></p>
<p>StreamGraph 和 JobGraph 代表了 Flink 作业的两个不同抽象层次：</p>
<table>
<thead>
<tr>
<th>对比维度</th>
<th>StreamGraph</th>
<th>JobGraph</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>抽象层次</strong></td>
<td>逻辑执行图</td>
<td>优化后的逻辑图</td>
</tr>
<tr>
<td><strong>节点单位</strong></td>
<td>StreamNode（单个算子）</td>
<td>JobVertex（算子链）</td>
</tr>
<tr>
<td><strong>优化程度</strong></td>
<td>未优化，1:1 映射用户代码</td>
<td>已优化，包含算子链</td>
</tr>
<tr>
<td><strong>用途</strong></td>
<td>表达用户意图</td>
<td>提交给集群执行</td>
</tr>
</tbody>
</table>
<blockquote>
<p>注：真正的物理执行图是 <strong>ExecutionGraph</strong>，
它在 JobManager 端生成，将 JobVertex 按并行度展开为多个 ExecutionVertex。</p>
</blockquote>
<p><strong>转换的核心目的是性能优化</strong>，主要体现在 <strong>Operator Chain（算子链）</strong> 机制上：</p>
<pre><code>转换前（StreamGraph）：
Source → Map → Filter → KeyBy → Reduce → Sink
  ↓       ↓      ↓        ↓        ↓       ↓
 6个独立的 StreamNode

转换后（JobGraph）：
[Source → Map → Filter] → [Reduce → Sink]
         ↓                       ↓
   1个 JobVertex（Chain）   1个 JobVertex（Chain）
   
注：KeyBy 会导致 shuffle，断开 Chain
</code></pre>
<p><strong>Operator Chain 的好处：</strong></p>
<ol>
<li><strong>减少线程切换</strong>：Chain 内的算子在同一个线程中执行</li>
<li><strong>减少序列化开销</strong>：Chain 内数据以对象形式传递，无需序列化</li>
<li><strong>减少网络传输</strong>：Chain 内无网络 I/O</li>
<li><strong>减少 Task 数量</strong>：更少的 Task 意味着更少的调度开销</li>
</ol>
<h3 id="转换的具体工作">转换的具体工作</h3>
<p>转换过程由 <code>StreamingJobGraphGenerator</code> 完成：</p>
<p><strong>1. 构建 Operator Chain</strong> — 判断相邻算子是否可以合并</p>
<p><strong>2. StreamNode → JobVertex</strong> — 多个可 Chain 的 StreamNode 合并为一个 JobVertex，设置并行度、资源需求</p>
<p><strong>3. StreamEdge → JobEdge</strong> — Chain 之间的边转换为 JobEdge，配置数据分发策略</p>
<p><strong>4. 序列化算子逻辑</strong> — 将 UDF 序列化打包成 <code>StreamConfig</code> 存入 JobVertex</p>
<p>这种两层图结构体现了 <strong>关注点分离</strong> 的思想：</p>
<ul>
<li><strong>StreamGraph 关注表达</strong>：忠实反映用户的业务逻辑，便于理解和调试</li>
<li><strong>JobGraph 关注执行</strong>：包含所有优化，是真正提交给集群的执行计划</li>
</ul>
<p><strong>学习示例：</strong></p>
<pre><code class="language-java">public class JobGraphLearningDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env =
                StreamExecutionEnvironment.getExecutionEnvironment();

        env.setParallelism(2);

        // 构建一个可以观察 Operator Chain 的作业
        DataStream&lt;String&gt; source = env.fromData(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);

        // 这些算子会被 Chain 在一起（如果没有 shuffle）
        DataStream&lt;String&gt; result = source
                .map(String::toUpperCase)      // 会被 Chain
                .filter(x -&gt; !x.equals(&quot;B&quot;))    // 会被 Chain
                .keyBy(x -&gt; x)                  // 这里会断开 Chain（有 shuffle）
                .map(x -&gt; x + &quot;!&quot;);                       // 会被 Chain

        result.print();
        // 获取 StreamGraph
        StreamGraph streamGraph = env.getStreamGraph();
        System.out.println(&quot;StreamNodes count: &quot; + streamGraph.getStreamNodes().size());

        // 打印 StreamGraph 信息
        System.out.println(&quot;StreamNodes: &quot; + streamGraph.getStreamNodes());

        System.out.println(&quot;StreamEdges: &quot;);
        streamGraph
                .getStreamNodes()
                .forEach(
                        node -&gt; {
                            System.out.println(
                                    &quot; - node: &quot;
                                            + node.getOperatorName()
                                            + &quot; StreamEdge: &quot;
                                            + streamGraph.getStreamEdges(node.getId()));
                        });

        // 转换为 JobGraph
        JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(streamGraph);

        // 打印 JobGraph 信息
        System.out.println(&quot;\nJobGraph Info:&quot;);
        System.out.println(&quot;Job ID: &quot; + jobGraph.getJobID());
        System.out.println(&quot;Job Name: &quot; + jobGraph.getName());
        System.out.println(&quot;JobVertices count: &quot; + jobGraph.getNumberOfVertices());

        // 查看每个 JobVertex（被 Chain 后的算子）
        for (JobVertex vertex : jobGraph.getVertices()) {
            System.out.println(&quot;\nJobVertex: &quot; + vertex.getName());
            System.out.println(&quot;  ID: &quot; + vertex.getID());
            System.out.println(&quot;  Parallelism: &quot; + vertex.getParallelism());
            System.out.println(&quot;  Operator IDs: &quot; + vertex.getOperatorIDs());
            System.out.println(&quot;  Inputs: &quot; + vertex.getInputs().size());
        }
    }
}

StreamNodes count: 5
StreamNodes: [Source: Collection Source-1, Map-2, Filter-3, Map-5, Sink: Print to Std. Out-6]
StreamEdges: 
 - node: Source: Collection Source StreamEdge: [(Source: Collection Source-1 -&gt; Map-2, 
   typeNumber=0, outputPartitioner=REBALANCE, exchangeMode=UNDEFINED, 
   bufferTimeout=100, outputTag=null, uniqueId=0)]
 - node: Map StreamEdge: [(Map-2 -&gt; Filter-3, typeNumber=0, outputPartitioner=FORWARD, 
   exchangeMode=UNDEFINED, bufferTimeout=100, outputTag=null, uniqueId=0)]
 - node: Filter StreamEdge: [(Filter-3 -&gt; Map-5, typeNumber=0, outputPartitioner=HASH, 
   exchangeMode=UNDEFINED, bufferTimeout=100, outputTag=null, uniqueId=0)]
 - node: Map StreamEdge: [(Map-5 -&gt; Sink: Print to Std. Out-6, typeNumber=0, 
   outputPartitioner=FORWARD, exchangeMode=UNDEFINED, bufferTimeout=100, 
   outputTag=null, uniqueId=0)]
 - node: Sink: Print to Std. Out StreamEdge: []

JobGraph Info:
Job ID: 1ad251732c79901f3f421c5d66b325e9
Job Name: Flink Streaming Job
JobVertices count: 3

JobVertex: Map -&gt; Sink: Print to Std. Out
  ID: b5c8d46f3e7b141acf271f12622e752b
  Parallelism: 2
  Operator IDs: [org.apache.flink.runtime.OperatorIDPair@2c35e847, 
                 org.apache.flink.runtime.OperatorIDPair@7bd4937b]
  Inputs: 1

JobVertex: Map -&gt; Filter
  ID: 20ba6b65f97481d5570070de90e4e791
  Parallelism: 2
  Operator IDs: [org.apache.flink.runtime.OperatorIDPair@21e360a, 
                 org.apache.flink.runtime.OperatorIDPair@5ba3f27a]
  Inputs: 1

JobVertex: Source: Collection Source
  ID: bc764cd8ddf7a0cff126f51c16239658
  Parallelism: 1
  Operator IDs: [org.apache.flink.runtime.OperatorIDPair@58d75e99]
  Inputs: 0
</code></pre>
<p>从结果上来看，在构建 DataStream node 时，一共有6个节点，从结果来看，node 只有5个，为什么 <code>keyBy</code> 算子&quot;消失&quot;了？</p>
<p>核心原因： <code>keyBy</code> 不是一个真正的算子（Operator），它只是一个逻辑分区操作，因此不会生成独立的 StreamNode。</p>
<p>从源码可以看到，<code>keyBy</code> 方法只是：</p>
<pre><code class="language-java">public &lt;K&gt; KeyedStream&lt;T, K&gt; keyBy(KeySelector&lt;T, K&gt; key) {
    Preconditions.checkNotNull(key);
    return new KeyedStream&lt;&gt;(this, clean(key));  // 只是创建了一个 KeyedStream
}
</code></pre>
<p>而 <code>KeyedStream</code> 的构造函数做的事情是：</p>
<pre><code class="language-java">public KeyedStream(
        DataStream&lt;T&gt; dataStream,
        KeySelector&lt;T, KEY&gt; keySelector,
        TypeInformation&lt;KEY&gt; keyType) {
    this(
        dataStream,
        new PartitionTransformation&lt;&gt;(  // 创建一个分区转换
            dataStream.getTransformation(),
            new KeyGroupStreamPartitioner&lt;&gt;(  // 使用 Hash 分区器
                keySelector,
                StreamGraphGenerator.DEFAULT_LOWER_BOUND_MAX_PARALLELISM)),
        keySelector,
        keyType);
}
</code></pre>
<p>关键点：</p>
<ul>
<li><code>keyBy</code> <strong>不创建新的算子（Operator）</strong></li>
<li><code>keyBy</code> 只创建一个 <strong><code>PartitionTransformation</code></strong>（分区转换）</li>
<li><code>keyBy</code> 设置了 <strong><code>KeyGroupStreamPartitioner</code></strong>（Hash 分区器）</li>
</ul>
<p><code>keyBy</code> 的作用体现在 <strong>StreamEdge 的 outputPartitioner</strong> 上！</p>
<pre><code>StreamEdges: 
 - node: Filter StreamEdge: [(Filter-3 -&gt; Map-5, 
     typeNumber=0, 
     outputPartitioner=HASH,  ⬅️ 这就是 keyBy 的作用！
     exchangeMode=UNDEFINED, 
     bufferTimeout=100, 
     outputTag=null, 
     uniqueId=0)]
</code></pre>
<p>对比其他边：</p>
<pre><code>Source -&gt; Map:    outputPartitioner=REBALANCE  (默认分区)
Map -&gt; Filter:    outputPartitioner=FORWARD    (可以 Chain)
Filter -&gt; Map:    outputPartitioner=HASH       ⬅️ keyBy 设置的！
Map -&gt; Sink:      outputPartitioner=FORWARD    (可以 Chain)
</code></pre>
<p>为什么这样设计？</p>
<p>keyBy 不处理数据</p>
<pre><code class="language-java">// keyBy 不是一个数据转换操作
DataStream&lt;String&gt; stream = ...;

// map 会转换数据：String -&gt; String
stream.map(String::toUpperCase)  // 有实际的计算逻辑

// filter 会过滤数据：保留或丢弃
stream.filter(x -&gt; !x.equals(&quot;B&quot;))  // 有实际的计算逻辑

// keyBy 不转换数据，只是改变数据的分发方式
stream.keyBy(x -&gt; x)  // 没有计算逻辑，只是设置分区策略
</code></pre>
<p>keyBy 是逻辑操作</p>
<pre><code class="language-java">// keyBy 只是告诉 Flink：
// &quot;接下来的算子需要按照 key 分组处理数据&quot;

stream
    .keyBy(x -&gt; x)           // 设置分区策略
    .map(x -&gt; x + &quot;!&quot;)       // 这个 map 才是真正的算子
    .sum(1)                  // 这个 sum 才是真正的算子
</code></pre>
<p>避免不必要的网络传输；如果 <code>keyBy</code> 生成一个独立的算子，会导致：</p>
<pre><code>❌ 不好的设计（如果 keyBy 是算子）：
Source -&gt; Map -&gt; Filter -&gt; KeyBy算子 -&gt; Map -&gt; Sink
                              ↑
                         不必要的算子
                         
✅ 好的设计（keyBy 只是分区策略）：
Source -&gt; Map -&gt; Filter --[HASH分区]--&gt; Map -&gt; Sink
                              ↑
                         只是改变数据分发方式
</code></pre>
<p>keyBy 在不同阶段的体现</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>keyBy 的体现</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>用户代码</td>
<td><code>.keyBy(x -&gt; x)</code></td>
<td>用户调用 keyBy 方法</td>
</tr>
<tr>
<td>StreamGraph</td>
<td><code>StreamEdge.partitioner = KeyGroupStreamPartitioner</code></td>
<td>设置边的分区器为 HASH</td>
</tr>
<tr>
<td>JobGraph</td>
<td>打断 Operator Chain</td>
<td>因为分区策略改变，不能 Chain</td>
</tr>
<tr>
<td>ExecutionGraph</td>
<td>创建 IntermediateResultPartition</td>
<td>创建中间结果分区</td>
</tr>
<tr>
<td>物理执行</td>
<td>数据通过网络 Shuffle</td>
<td>按照 key 的 hash 值分发到不同的 Task</td>
</tr>
</tbody>
</table>
<p>类比理解</p>
<pre><code>keyBy 就像是道路上的&quot;分流指示牌&quot;：
- 它不是一个收费站（算子）
- 它只是告诉车辆（数据）应该走哪条路（分区）
- 车辆本身没有变化，只是行驶路线变了
</code></pre>
<p><strong>StreamGraph → JobGraph 关键概念：</strong></p>
<pre><code class="language-java">// Operator Chain 的条件（满足以下所有条件才能 Chain）：
1. 下游算子只有一个输入
2. 上游算子只有一个输出
3. 两个算子的并行度相同
4. 没有 shuffle（即 ForwardPartitioner）
5. 两个算子在同一个 SlotSharingGroup
6. Chain 策略允许（ChainingStrategy）

// StreamingJobGraphGenerator 的核心方法：
createJobGraph(StreamGraph)
  └── createJobVertex()              // 创建 JobVertex
      └── createChain()              // 构建 Operator Chain
          └── isChainable()          // 判断是否可以 Chain
</code></pre>
<p><strong>重点方法：</strong></p>
<ul>
<li><code>StreamingJobGraphGenerator.createJobGraph()</code> — 转换入口</li>
<li><code>StreamingJobGraphGenerator.createChain()</code> — 构建 Operator Chain</li>
<li><code>StreamingJobGraphGenerator.isChainable()</code> — 判断是否可以 Chain</li>
</ul>
<hr>
<h2 id="pipelineexecutor-体系">PipelineExecutor 体系</h2>
<p><strong>核心文件：</strong></p>
<ul>
<li><code>PipelineExecutor.java</code></li>
<li><code>LocalExecutor.java</code></li>
<li><code>RemoteExecutor.java</code></li>
</ul>
<p><strong>学习目标：</strong></p>
<ul>
<li>理解 <code>PipelineExecutor</code> 的作用（执行器抽象）</li>
<li>理解不同执行器的实现（Local、Remote、Embedded）</li>
<li>理解 SPI 机制加载执行器</li>
</ul>
<p><strong>PipelineExecutor 体系：</strong></p>
<pre><code>PipelineExecutor
  ├── LocalExecutor
  ├── RemoteExecutor
  ├── AbstractJobClusterExecutor
  │     ├── YarnJobClusterExecutor
  │     └── KubernetesJobClusterExecutor
  └── AbstractSessionClusterExecutor
        ├── YarnSessionClusterExecutor
        └── KubernetesSessionClusterExecutor
</code></pre>
<p><strong>学习示例：</strong></p>
<pre><code class="language-java">public class PipelineExecutorLearningDemo {
    public static void main(String[] args) throws Exception {
        // 1. 本地执行器
        Configuration localConf = new Configuration();
        localConf.setString(DeploymentOptions.TARGET, &quot;local&quot;);
        
        StreamExecutionEnvironment localEnv = 
            StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(localConf);
        
        // 查看使用的执行器
        PipelineExecutor localExecutor = localEnv.getPipelineExecutor();
        System.out.println(&quot;Local Executor: &quot; + localExecutor.getClass().getName());
        
        // 2. 远程执行器（连接到已有集群）
        Configuration remoteConf = new Configuration();
        remoteConf.setString(DeploymentOptions.TARGET, &quot;remote&quot;);
        remoteConf.setString(JobManagerOptions.ADDRESS, &quot;localhost&quot;);
        remoteConf.setInteger(JobManagerOptions.PORT, 8081);
        
        StreamExecutionEnvironment remoteEnv = 
            StreamExecutionEnvironment.getExecutionEnvironment(remoteConf);
        
        PipelineExecutor remoteExecutor = remoteEnv.getPipelineExecutor();
        System.out.println(&quot;Remote Executor: &quot; + remoteExecutor.getClass().getName());
        
        // 3. 查看所有可用的执行器工厂
        PipelineExecutorServiceLoader serviceLoader = 
            new DefaultExecutorServiceLoader();
        
        System.out.println(&quot;\nAvailable Executor Factories:&quot;);
        // 注意：这需要访问内部 API，仅用于学习
    }
}
</code></pre>
<p><strong>关键类：</strong></p>
<pre><code class="language-java">// 1. 执行器接口
PipelineExecutor.execute(Pipeline, Configuration, ClassLoader)
  └── 返回 CompletableFuture&lt;JobClient&gt;

// 2. LocalExecutor（本地执行）
LocalExecutor.execute()
  └── PerJobMiniClusterFactory.submitJob()
      └── MiniCluster.submitJob()

// 3. RemoteExecutor（远程执行）
RemoteExecutor.execute()
  └── RestClusterClient.submitJob()
      └── 通过 REST API 提交到远程集群
</code></pre>
<hr>
<h2 id="clusterclient-体系">ClusterClient 体系</h2>
<p><strong>核心文件：</strong></p>
<ul>
<li><code>ClusterClient.java</code></li>
<li><code>RestClusterClient.java</code></li>
<li><code>MiniClusterClient.java</code></li>
</ul>
<p><strong>学习目标：</strong></p>
<ul>
<li>理解 <code>ClusterClient</code> 的作用（与集群交互的客户端）</li>
<li>理解作业提交、取消、Savepoint 等操作</li>
<li>理解 REST API 的使用</li>
</ul>
<p><strong>学习示例：</strong></p>
<pre><code class="language-java">public class ClusterClientLearningDemo {
    public static void main(String[] args) throws Exception {
        // 1. 创建 RestClusterClient（连接到运行中的集群）
        Configuration conf = new Configuration();
        conf.setString(RestOptions.ADDRESS, &quot;localhost&quot;);
        conf.setInteger(RestOptions.PORT, 8081);
        
        try (RestClusterClient&lt;StandaloneClusterId&gt; client = 
                new RestClusterClient&lt;&gt;(conf, StandaloneClusterId.getInstance())) {
            
            // 2. 获取集群信息
            System.out.println(&quot;Cluster ID: &quot; + client.getClusterId());
            
            // 3. 列出正在运行的作业
            Collection&lt;JobStatusMessage&gt; jobs = client.listJobs().get();
            System.out.println(&quot;\nRunning Jobs:&quot;);
            for (JobStatusMessage job : jobs) {
                System.out.println(&quot;  - &quot; + job.getJobName() + 
                                 &quot; [&quot; + job.getJobState() + &quot;]&quot;);
            }
            
            // 4. 提交作业（需要 JobGraph）
            // JobGraph jobGraph = ...;
            // JobID jobId = client.submitJob(jobGraph).get();
            
            // 5. 取消作业
            // client.cancel(jobId).get();
            
            // 6. 触发 Savepoint
            // String savepointPath = client.triggerSavepoint(jobId, null).get();
        }
    }
}
</code></pre>
<hr>
<h2 id="cli-命令行工具">CLI 命令行工具</h2>
<p><strong>核心文件：</strong></p>
<ul>
<li><code>CliFrontend.java</code></li>
<li><code>CliFrontendParser.java</code></li>
</ul>
<p><strong>学习目标：</strong></p>
<ul>
<li>理解 <code>flink run</code> 命令的实现</li>
<li>理解命令行参数解析</li>
<li>理解 PackagedProgram 的加载</li>
</ul>
<p><strong>学习方式：</strong></p>
<pre><code class="language-bash"># 1. 查看 flink run 命令的实现
# CliFrontend.run() 方法

# 2. 理解参数解析
# CliFrontendParser.parse()

# 3. 理解 JAR 包加载
# PackagedProgram.newBuilder().build()
</code></pre>
<hr>
<h2 id="packagedprogramjar-包加载">PackagedProgram（JAR 包加载）</h2>
<p><strong>核心文件：</strong></p>
<ul>
<li><code>PackagedProgram.java</code></li>
<li><code>PackagedProgramUtils.java</code></li>
</ul>
<p><strong>学习目标：</strong></p>
<ul>
<li>理解如何加载用户 JAR 包</li>
<li>理解如何调用用户的 main 方法</li>
<li>理解 ClassLoader 隔离</li>
</ul>
<hr>
<h2 id="application-mode">Application Mode</h2>
<p><strong>核心文件：</strong></p>
<ul>
<li><code>ApplicationDispatcherBootstrap.java</code></li>
<li><code>EmbeddedExecutor.java</code></li>
</ul>
<hr>
<h2 id="deployment-抽象">Deployment 抽象</h2>
<p><strong>核心文件：</strong></p>
<ul>
<li><code>ClusterDescriptor.java</code></li>
<li><code>ClusterClientFactory.java</code></li>
</ul>
<hr>
<h2 id="学习路线总结">学习路线总结</h2>
<pre><code>FileSinkDemo.java（起点）
    ↓
StreamExecutionEnvironment.execute()
    ↓
getStreamGraph() ─────────────────&gt; StreamGraphGenerator
    ↓                               (Transformation → StreamGraph)
executeAsync()
    ↓
StreamingJobGraphGenerator ────────&gt; (StreamGraph → JobGraph)
    ↓
PipelineExecutor.execute()
    ├── LocalExecutor  ────────────&gt; MiniCluster.submitJob()
    └── RemoteExecutor ────────────&gt; RestClusterClient.submitJob()
</code></pre>
<hr>
<h2 id="调试实践步骤">调试实践步骤</h2>
<h3 id="修改-filesinkdemo-添加调试代码">修改 FileSinkDemo 添加调试代码</h3>
<pre><code class="language-java">public class FileSinkDemo {
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        StreamExecutionEnvironment env = 
            StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf);
        
        env.setParallelism(1);
        env.enableCheckpointing(5000);
        
        DataStream&lt;String&gt; source = env.fromData(&quot;hello&quot;, &quot;world&quot;, &quot;flink&quot;);
        
        FileSink&lt;String&gt; sink = FileSink
            .forRowFormat(new Path(&quot;file:///tmp/flink-debug-output&quot;), 
                         new SimpleStringEncoder&lt;String&gt;(&quot;UTF-8&quot;))
            .build();
        
        source.sinkTo(sink);
        
        // 1. 查看 Transformations
        System.out.println(&quot;\n=== Transformations ===&quot;);
        List&lt;Transformation&lt;?&gt;&gt; transformations = env.getTransformations();
        for (Transformation&lt;?&gt; t : transformations) {
            System.out.println(&quot;  - &quot; + t.getName() + 
                             &quot; [&quot; + t.getClass().getSimpleName() + &quot;]&quot;);
        }
        
        // 2. 查看 StreamGraph
        System.out.println(&quot;\n=== StreamGraph ===&quot;);
        StreamGraph streamGraph = env.getStreamGraph(false);
        System.out.println(&quot;Job Name: &quot; + streamGraph.getJobName());
        System.out.println(&quot;StreamNodes: &quot; + streamGraph.getStreamNodes().size());
        
        // 3. 查看 JobGraph
        System.out.println(&quot;\n=== JobGraph ===&quot;);
        JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(streamGraph);
        System.out.println(&quot;JobVertices: &quot; + jobGraph.getNumberOfVertices());
        for (JobVertex vertex : jobGraph.getVertices()) {
            System.out.println(&quot;  - &quot; + vertex.getName() + 
                             &quot; (parallelism=&quot; + vertex.getParallelism() + &quot;)&quot;);
        }
        
        // 4. 查看 PipelineExecutor
        System.out.println(&quot;\n=== PipelineExecutor ===&quot;);
        PipelineExecutor executor = env.getPipelineExecutor();
        System.out.println(&quot;Executor: &quot; + executor.getClass().getName());
        
        // ===== 执行作业 =====
        env.execute(&quot;Local FileSystem Debug&quot;);
    }
}
</code></pre>
<h3 id="设置断点开始调试">设置断点开始调试</h3>
<p><strong>重点方法：</strong></p>
<ol>
<li><code>StreamExecutionEnvironment.execute()</code></li>
<li><code>StreamExecutionEnvironment.getStreamGraph()</code></li>
<li><code>StreamGraphGenerator.generate()</code></li>
<li><code>StreamingJobGraphGenerator.createJobGraph()</code></li>
<li><code>LocalExecutor.execute()</code></li>
</ol>
<h3 id="阅读源码理解核心流程">阅读源码理解核心流程</h3>
<p>按照以下顺序阅读：</p>
<ol>
<li><code>StreamExecutionEnvironment</code> → <code>StreamGraph</code> → <code>StreamGraphGenerator</code></li>
<li><code>StreamingJobGraphGenerator</code> → <code>JobGraph</code> → Operator Chain</li>
<li><code>PipelineExecutor</code> → <code>LocalExecutor</code> → <code>MiniCluster</code></li>
<li><code>ClusterClient</code> → <code>RestClusterClient</code> → REST API</li>
<li><code>CliFrontend</code> → <code>PackagedProgram</code> → CLI 工具</li>
</ol>
<hr>
<h2 id="实战手动启动集群并提交作业">实战：手动启动集群并提交作业</h2>
<p>在理解了 Client Layer 的核心流程后，我们可以通过手动启动集群的方式，深入理解 ClusterClient 与集群的交互过程。</p>
<h3 id="启动本地-standalone-集群">启动本地 Standalone 集群</h3>
<p>通过编程方式启动 JobManager 和 TaskManager，完整体验集群启动流程：</p>
<pre><code class="language-java">public class StartClusterDemo {
    public static void main(String[] args) throws Exception {
        Configuration config = createConfiguration();
        
        // 启动 JobManager
        StandaloneSessionClusterEntrypoint jobManager = startJobManager(config);
        Thread.sleep(3000);
        
        // 启动 TaskManager
        TaskManagerRunner taskManager = startTaskManager(config);
        Thread.sleep(3000);
        
        System.out.println(&quot;Access WebUI at: http://localhost:8081&quot;);
        Thread.currentThread().join();
    }
    
    private static Configuration createConfiguration() {
        Configuration config = new Configuration();
        config.set(JobManagerOptions.ADDRESS, &quot;localhost&quot;);
        config.set(JobManagerOptions.PORT, 6123);
        config.set(RestOptions.PORT, 8081);
        config.set(TaskManagerOptions.NUM_TASK_SLOTS, 4);
        
        // 自动调整本地执行配置
        TaskExecutorResourceUtils.adjustForLocalExecution(config);
        return config;
    }
    
    private static StandaloneSessionClusterEntrypoint startJobManager(Configuration config) 
            throws Exception {
        StandaloneSessionClusterEntrypoint entrypoint = 
            new StandaloneSessionClusterEntrypoint(config);
        
        Thread jmThread = new Thread(() -&gt; {
            try {
                ClusterEntrypoint.runClusterEntrypoint(entrypoint);
            } catch (Exception e) {
                e.printStackTrace();
            }
        }, &quot;JobManager-Thread&quot;);
        jmThread.setDaemon(true);
        jmThread.start();
        
        return entrypoint;
    }
    
    private static TaskManagerRunner startTaskManager(Configuration config) throws Exception {
        TaskManagerRunner taskManagerRunner = new TaskManagerRunner(
            config,
            PluginUtils.createPluginManagerFromRootFolder(config),
            TaskManagerRunner::createTaskExecutorService);
        taskManagerRunner.start();
        return taskManagerRunner;
    }
}
</code></pre>
<h3 id="通过-restclusterclient-提交作业">通过 RestClusterClient 提交作业</h3>
<p>使用 <code>RestClusterClient</code> 连接到集群并提交作业，这是理解 Client 与 JobManager 交互的关键：</p>
<pre><code class="language-java">public class SubmitJobToClusterDemo {
    public static void main(String[] args) throws Exception {
        Configuration config = new Configuration();
        config.set(RestOptions.PORT, 8081);
        config.set(RestOptions.ADDRESS, &quot;localhost&quot;);
        
        try (ClusterClient&lt;String&gt; clusterClient = 
                new RestClusterClient&lt;&gt;(config, &quot;standalone-cluster&quot;)) {
            
            System.out.println(&quot;Connected: &quot; + clusterClient.getWebInterfaceURL());
            
            // 构建 JobGraph
            JobGraph jobGraph = createJobGraph(config);
            
            // 提交作业
            JobID jobId = clusterClient.submitJob(jobGraph).get();
            System.out.println(&quot;Submitted job: &quot; + jobId);
            
            // 等待作业完成
            clusterClient.requestJobResult(jobId).get();
        }
    }
    
    private static JobGraph createJobGraph(Configuration config) {
        StreamExecutionEnvironment env = 
            StreamExecutionEnvironment.getExecutionEnvironment(config);
        env.setParallelism(2);
        
        DataStream&lt;String&gt; source = env.fromData(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;);
        source.map(String::toUpperCase).print();
        
        StreamGraph streamGraph = env.getStreamGraph();
        streamGraph.setJobName(&quot;Standalone Cluster Demo&quot;);
        return streamGraph.getJobGraph();
    }
}
</code></pre>
<p>通过这个实战，可以清晰地观察到：</p>
<ol>
<li><strong>集群启动过程</strong>：JobManager 和 TaskManager 的启动顺序和依赖关系</li>
<li><strong>REST API 交互</strong>：ClusterClient 如何通过 REST API 与 JobManager 通信</li>
<li><strong>作业提交流程</strong>：从 StreamGraph → JobGraph → 提交到集群的完整链路</li>
</ol>
<hr>
<h2 id="client-layer-的核心职责">Client Layer 的核心职责</h2>
<p>Client Layer 的三大核心职责：</p>
<ol>
<li>
<p>图转换（Graph Translation）
Transformation → StreamGraph → JobGraph</p>
</li>
<li>
<p>作业提交（Job Submission）
PipelineExecutor → ClusterClient → REST API</p>
</li>
<li>
<p>集群交互（Cluster Interaction）
提交作业、取消作业、触发 Savepoint、查询状态</p>
</li>
</ol>

</main>

  <footer>
  
<script src="https://utteranc.es/client.js"
        repo="qiref/qiref.github.io"
        issue-term="pathname"
        label="Comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>


<script>
(function() {
  const searchInput = document.getElementById('search-input');
  const searchResults = document.getElementById('search-results');
  if (!searchInput || !searchResults) return;
  
  let searchIndex = null;
  let activeIndex = -1;
  let isResultsVisible = false;

  
  async function loadSearchIndex() {
    if (searchIndex) return searchIndex;
    try {
      const response = await fetch('/index.json');
      const data = await response.json();
      searchIndex = data.pages || [];
      return searchIndex;
    } catch (e) {
      console.error('Failed to load search index:', e);
      return [];
    }
  }

  
  function search(query) {
    if (!searchIndex || !query.trim()) return [];
    const q = query.toLowerCase();
    return searchIndex.filter(page => {
      const title = (page.title || '').toLowerCase();
      const content = (page.content || '').toLowerCase();
      return title.includes(q) || content.includes(q);
    }).slice(0, 5);
  }

  
  function highlight(text, query) {
    if (!query) return text;
    const regex = new RegExp(`(${query.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')})`, 'gi');
    return text.replace(regex, '<mark>$1</mark>');
  }

  
  function showResults() {
    searchResults.style.display = 'block';
    isResultsVisible = true;
  }

  
  function hideResults() {
    searchResults.style.display = 'none';
    isResultsVisible = false;
    activeIndex = -1;
  }

  
  function updateActiveItem() {
    const items = searchResults.querySelectorAll('.search-result-item');
    items.forEach((item, index) => {
      if (index === activeIndex) {
        item.classList.add('active');
      } else {
        item.classList.remove('active');
      }
    });
  }

  
  function renderResults(results, query) {
    activeIndex = -1;
    if (!results.length) {
      hideResults();
      return;
    }
    const html = results.map(page => {
      const title = highlight(page.title || 'Untitled', query);
      let summary = page.summary || page.content || '';
      summary = summary.substring(0, 100) + (summary.length > 100 ? '...' : '');
      return `<a class="search-result-item" href="${page.url}">
        <div class="search-result-title">${title}</div>
        <div class="search-result-summary">${summary}</div>
      </a>`;
    }).join('');
    searchResults.innerHTML = html;
    showResults();
  }

  
  let debounceTimer;
  function debounce(fn, delay) {
    return function(...args) {
      clearTimeout(debounceTimer);
      debounceTimer = setTimeout(() => fn.apply(this, args), delay);
    };
  }

  
  searchInput.addEventListener('input', debounce(async function() {
    const query = this.value.trim();
    if (!query) {
      hideResults();
      return;
    }
    await loadSearchIndex();
    const results = search(query);
    renderResults(results, query);
  }, 200));

  
  searchInput.addEventListener('keydown', function(e) {
    if (!isResultsVisible) return;
    
    const items = searchResults.querySelectorAll('.search-result-item');
    const itemCount = items.length;
    if (itemCount === 0) return;

    switch (e.key) {
      case 'ArrowDown':
        e.preventDefault();
        e.stopPropagation();
        activeIndex = (activeIndex + 1) % itemCount;
        updateActiveItem();
        break;
      case 'ArrowUp':
        e.preventDefault();
        e.stopPropagation();
        activeIndex = activeIndex <= 0 ? itemCount - 1 : activeIndex - 1;
        updateActiveItem();
        break;
      case 'Enter':
        if (activeIndex >= 0 && items[activeIndex]) {
          e.preventDefault();
          window.location.href = items[activeIndex].href;
        }
        break;
      case 'Escape':
        e.preventDefault();
        hideResults();
        break;
    }
  });

  
  searchInput.addEventListener('focus', async function() {
    const query = this.value.trim();
    if (query) {
      await loadSearchIndex();
      const results = search(query);
      renderResults(results, query);
    }
  });

  
  document.addEventListener('click', function(e) {
    if (!searchInput.contains(e.target) && !searchResults.contains(e.target)) {
      hideResults();
    }
  });

  
  document.addEventListener('keydown', function(e) {
    if ((e.metaKey || e.ctrlKey) && e.key === 'k') {
      e.preventDefault();
      searchInput.focus();
      searchInput.select();
    }
  });
})();
</script>
  
  <hr/>
  © powered by <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/yihui/hugo-xmin">Xmin</a>  2017 &ndash; 2026 | <a href="https://github.com/qiref">Github</a>
  
  </footer>
  </body>
</html>

