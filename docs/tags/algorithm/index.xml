<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithm on 大道至简</title>
    <link>https://qiref.github.io/tags/algorithm/</link>
    <description>Recent content in Algorithm on 大道至简</description>
    <generator>Hugo</generator>
    <language>cn-zh</language>
    <lastBuildDate>Tue, 05 Dec 2023 11:03:22 +0800</lastBuildDate>
    <atom:link href="https://qiref.github.io/tags/algorithm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>B&#43;树</title>
      <link>https://qiref.github.io/post/2023/12/05/b-%E6%A0%91/</link>
      <pubDate>Tue, 05 Dec 2023 11:03:22 +0800</pubDate>
      <guid>https://qiref.github.io/post/2023/12/05/b-%E6%A0%91/</guid>
      <description>&lt;h2 id=&#34;b树引入&#34;&gt;B树引入&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;B树（英语：B-tree），是一种在计算机科学自平衡的树，能够保持数据有序。这种数据结构能够让查找数据、顺序访问、插入数据及删除的动作，都在对数时间内完成。B树，概括来说是一个一般化的二叉搜索树（binary search tree）一个节点可以拥有2个以上的子节点。与自平衡二叉查找树不同，B树适用于读写相对大的数据块的存储系统，例如磁盘。B树减少定位记录时所经历的中间过程，从而加快访问速度。B树这种数据结构可以用来描述外部存储。这种数据结构常被应用在数据库和文件系统的实现上。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;code&gt;wiki&lt;/code&gt; 上是这么描述 B 树的, 重点在于 B 树被用作存储系统的实现上, 基于二叉搜索树天然的有序性, 实现 logn 级别的查询; 既然是用作存储系统的实现, 那么可以来推导一下, 为什么B 树会用作存储系统的实现?&lt;/p&gt;&#xA;&lt;p&gt;想要实现 logn 级别的查询, &lt;code&gt;binary search tree&lt;/code&gt; &lt;code&gt;skiip list&lt;/code&gt; 都可以实现, Why B 树?&lt;/p&gt;&#xA;&lt;h3 id=&#34;二叉搜索树&#34;&gt;二叉搜索树&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://qiref.github.io/assets/img/b-tree-1.svg&#34; alt=&#34;b+-tree-1&#34;&gt; &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&#xA;&lt;p&gt;二叉搜索树天然有序, 也能达到 logn 级别的查询性能, 但是二叉搜索树, 有个很严重的问题, 如果插入的数据本身是有序的, 那二叉搜索树就会退化为链表, 要解决这个问题, 可以用 &lt;code&gt;AVL Tree (Balanced binary search tree)&lt;/code&gt; 和 &lt;code&gt;Red-Black Tree&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;avl-树&#34;&gt;AVL 树&lt;/h3&gt;&#xA;&lt;p&gt;&lt;code&gt;AVL Tree (Balanced binary search tree)&lt;/code&gt; 在二叉搜索树的基础上, 增加了自平衡的机制, 解决二叉搜索树退化为链表的问题, 但是自平衡也会带来新的问题(平衡条件必须满足所有节点的左右子树高度差不超过1), 插入时可能会触发多次的自平衡, 从而会影响数据插入的效率, 那有没有办法解决频繁的自平衡的问题呢?&lt;/p&gt;&#xA;&lt;h3 id=&#34;红黑树&#34;&gt;红黑树&lt;/h3&gt;&#xA;&lt;p&gt;&lt;code&gt;Red-Black Tree&lt;/code&gt; 就能做到, 红黑树通过制定一系列的规则:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Log Structured Merge Tree</title>
      <link>https://qiref.github.io/post/2023/10/13/log-structured-merge-tree/</link>
      <pubDate>Fri, 13 Oct 2023 11:46:21 +0800</pubDate>
      <guid>https://qiref.github.io/post/2023/10/13/log-structured-merge-tree/</guid>
      <description>&lt;h2 id=&#34;基本概念&#34;&gt;基本概念&lt;/h2&gt;&#xA;&lt;p&gt;&lt;code&gt;Log Structured Merge Tree&lt;/code&gt;, 其本质上是一种存储数据的方式,通常用于各种存储系统的底层数据结构,通过尽可能减少磁盘随机IO来提升写入性能, 适用于写多读少的场景.&lt;/p&gt;&#xA;&lt;h3 id=&#34;随机写和顺序写&#34;&gt;随机写和顺序写&lt;/h3&gt;&#xA;&lt;p&gt;对于一个存储系统而言, 不可避免地需要写入文件到磁盘, 对于常规的写来说, 每来一条数据写一次文件, 数据可能是 &lt;code&gt;add update delete&lt;/code&gt;, 需要频繁操作文件, 每一次写都是一次随机 IO; 为了提高写入速度, &lt;code&gt;LSM Tree&lt;/code&gt; 并不是每一次写操作都把文件写到磁盘, 而是将数据在内存中更新，当内存中的数据达到一定的阈值时，才将这部分数据真正刷新到磁盘文件中. 以这种方式尽可能让每次磁盘 IO 都是顺序写;&lt;/p&gt;&#xA;&lt;h3 id=&#34;思路&#34;&gt;思路&lt;/h3&gt;&#xA;&lt;p&gt;基于减少磁盘的随机 IO 来提升整体存储系统的写入性能这一背景, 很自然可以推导出用批量写入的方式, 要想批量写入, 就需要在内存维护最近写入的数据, 达到阈值之后生成一个文件写入到磁盘, 但是这样又会存在新的问题:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;如果某一条数据已经写入到磁盘文件, 后续又有更新, 怎么处理呢?&lt;/li&gt;&#xA;&lt;li&gt;内存中维护的临时数据, 如果还未来得及写入磁盘, 服务挂了, 重新启动时, 历史写入的数据如何恢复?&lt;/li&gt;&#xA;&lt;li&gt;每次内存中数据达到阈值,写一个整个文件到磁盘,那么最终会生成大量的文件, 如何解决?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;解决问题1, 为了优化这种更新的写入, 可以采用数据版本的做法, 或者给数据增加标志, 然后定期合并, 当然, 这也是以空间换时间, 相同的数据存储了多次, 以提升写入性能; 与此同时, 在数据读取时,由于写入的逻辑改变, 一条数据可能会存在于多个文件中, 因此在读取时, 需要返回最新的数据, 在读取到多条数据时,需要对多条数据进行合取最新;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;解决问题2, 在业界比较标准的做法是 &lt;code&gt;WAL&lt;/code&gt;, &lt;code&gt;WAL&lt;/code&gt; 的基本原理是在执行数据修改操作之前，先将这些操作记录在日志（log）文件中, 以确保在发生故障或崩溃时，可以借助日志进行恢复并保持数据的一致性;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;解决问题3, 为了避免大量文件, 可以对文件进行定期合并, 当数据还在内存中时, 可以借助跳表或者 &lt;code&gt;B+Tree&lt;/code&gt; 等数据结构保证内存中数据的顺序性, 在写文件时, 由于数据是有序的, 在文件合并时,很自然可以借助归并排序保证合并之后的数据的有序性, 而有序性又能天然提高查询效率.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Skip Lists 阅读笔记</title>
      <link>https://qiref.github.io/post/2023/10/01/skip-lists-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Sun, 01 Oct 2023 22:05:38 +0800</pubDate>
      <guid>https://qiref.github.io/post/2023/10/01/skip-lists-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <description>&lt;h2 id=&#34;算法介绍&#34;&gt;算法介绍&lt;/h2&gt;&#xA;&lt;p&gt;《Skip Lists: A Probabilistic Alternative to Balanced Trees》 论文标题翻译就是 跳表: 平衡树的概率性替代方案; 跳表是一种可以用来代替平衡树的数据结构。跳表使用概率平衡而不是严格强制的平衡，因此跳跃列表中的插入和删除算法比平衡树的等效算法要简单得多并且速度明显更快。&lt;/p&gt;&#xA;&lt;p&gt;从论文的标题和介绍, 基本上就能知道跳表是一种怎么样的数据结构, 为了解决平衡树实现的复杂性, 提供一种概率性平衡的数据结构,作为平衡树的平替数据结构, 查询和插入时间复杂度是 &lt;code&gt;O(log n)&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;算法流程&#34;&gt;算法流程&lt;/h2&gt;&#xA;&lt;h3 id=&#34;基本原理&#34;&gt;基本原理&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://qiref.github.io/assets/img/skiplist-1.svg&#34; alt=&#34;skiplist-1&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;节点结构：跳表由多个层级组成，每个层级都是一个有序链表。每个节点包含一个值和多个指向下一层级节点的指针。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;层级索引：跳表的最底层是一个普通的有序链表，每个节点都连接到下一个节点。而在更高的层级，节点以一定的概率连接到更远的节点，形成了一种“跳跃”的效果。这些连接被称为“跳跃指针”，它们允许我们在查找时可以快速地跳过一些节点。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;查找操作：从跳表的顶层开始，我们沿着每个层级向右移动，直到找到目标值或找到一个大于目标值的节点。然后我们进入下一层级继续查找，直到最底层。这种方式可以在平均情况下实现快速的查找，时间复杂度为 O(log n)。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;插入和删除操作：在插入新节点时，我们首先执行查找操作，找到合适的插入位置。然后我们在每个层级上插入新节点，并根据一定的概率决定是否要为该节点添加跳跃指针。删除操作类似，我们首先找到要删除的节点，然后将其从每个层级中移除。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;查询&#34;&gt;查询&lt;/h3&gt;&#xA;&lt;p&gt;&lt;code&gt;level&lt;/code&gt; 表示跳表的层级, 而 &lt;code&gt;forward[i]&lt;/code&gt; 是每一个层级的链表.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;Search(list, searchKey)&#xA;    x := list→header&#xA;    // 从跳表的顶层开始,遍历到第一层&#xA;    for i := list→level downto 1 do&#xA;        while x→forward[i]→key &amp;lt; searchKey do&#xA;            x := x→forward[i]&#xA;    // x→key &amp;lt; searchKey ≤ x→forward[1]→key&#xA;    // 最终的结果从跳表最底层获取&#xA;    x := x→forward[1]&#xA;    if x→key = searchKey then &#xA;        return x→value&#xA;    else &#xA;        return failure&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h3 id=&#34;写入&#34;&gt;写入&lt;/h3&gt;&#xA;&lt;p&gt;由跳表的定义得出, 跳表的上一层级相当于下一层级的索引, 如果需要构建多级的索引, 首先需要解决: &lt;em&gt;当前node是否应该索引到上一层级?&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>DBLog 阅读笔记</title>
      <link>https://qiref.github.io/post/2023/08/09/dblog-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Wed, 09 Aug 2023 10:39:17 +0800</pubDate>
      <guid>https://qiref.github.io/post/2023/08/09/dblog-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <description>&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;&#xA;&lt;p&gt;论文原名: &lt;code&gt;DBLog: A Watermark Based Change-Data-Capture Framework&lt;/code&gt; , 基于 &lt;code&gt;Watermark&lt;/code&gt; 的 &lt;code&gt;Change-Data-Capture&lt;/code&gt;(数据库实时捕获已提交的变更记录) 框架, 本质上是解决数据库同步(全量+增量)的框架, &lt;code&gt;Watermark&lt;/code&gt; 是框架使用的一种手段, 在源表中创建表,生成唯一 uuid 并更新表数据, 在源表中就会生成一条变更记录,记作 &lt;code&gt;Watermark&lt;/code&gt; 的变更记录, 通过 &lt;code&gt;High Watermark&lt;/code&gt; 和 &lt;code&gt;Low Watermark &lt;/code&gt; 将变更记录分割, 保证 select chunk 数据包含了增量的变更记录.&lt;/p&gt;&#xA;&lt;p&gt;框架整体架构如下:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://qiref.github.io/assets/img/DBLog-1.svg&#34; alt=&#34;DBLog-1&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;框架特点:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;按顺序处理捕获到的 &lt;code&gt;changelog&lt;/code&gt;;&lt;/li&gt;&#xA;&lt;li&gt;转储可以随时进行，跨所有表，针对一个特定的表或者针对一个表的具体主键;&lt;/li&gt;&#xA;&lt;li&gt;以块(chunk)的形式获取转储，日志与转储事件交错。通过这种方式，&lt;code&gt;changelog&lt;/code&gt; 可以与转储处理一起进行。如果进程终止，它可以在最后一个完成的块之后恢复，而不需要从头开始。这还允许在需要时对转储进行调整和暂停;&lt;/li&gt;&#xA;&lt;li&gt;不会获取表级锁，这可以防止影响源数据库上的写流量;&lt;/li&gt;&#xA;&lt;li&gt;支持任何类型的输出，因此，输出可以是流、数据存储甚或是 API;&lt;/li&gt;&#xA;&lt;li&gt;设计充分考虑了高可用性。因此，下游的消费者可以放心，只要源端发生变化，它们就可以收到变化事件。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;注意, 本文并非详细介绍 &lt;code&gt;DBLog&lt;/code&gt; 框架本身, 而是分析其框架背后的设计思路.&lt;/p&gt;&#xA;&lt;h2 id=&#34;算法流程&#34;&gt;算法流程&lt;/h2&gt;&#xA;&lt;h3 id=&#34;chunk-划分&#34;&gt;chunk 划分&lt;/h3&gt;&#xA;&lt;p&gt;对于源表数据, 全量数据使用分块读取, 基于 &lt;code&gt;primary key&lt;/code&gt; 顺序排序, 将全量数据划分为 N 个 chunk;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://qiref.github.io/assets/img/DBLog-2.svg&#34; alt=&#34;DBLog-2&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;watermark&#34;&gt;watermark&lt;/h3&gt;&#xA;&lt;p&gt;基于 chunk 划分, 然后 chunk 数据全量写入下游之后, 再将源表的变更记录 &lt;code&gt;changelog&lt;/code&gt; 增量同步到下游, 整体思路就是这样, 但是划分 chunk 有个问题需要解决, 就是先同步到下游的数据不一定的最终的数据, 例如上图 chunk1 中的数据在同步到下游之后可能会删除, 那chunk1 的数据写到下游之后, 下游就会出现脏数据; 如何解决 chunk 和 &lt;code&gt;changelog&lt;/code&gt; 之间不会相互覆盖的问题?&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Dataflow Model 阅读笔记</title>
      <link>https://qiref.github.io/post/2023/05/16/the-dataflow-model-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Tue, 16 May 2023 15:26:10 +0800</pubDate>
      <guid>https://qiref.github.io/post/2023/05/16/the-dataflow-model-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <description>&lt;h2 id=&#34;dataflow-计算模型&#34;&gt;Dataflow 计算模型&lt;/h2&gt;&#xA;&lt;p&gt;Dataflow 的核心计算模型非常简单，它只有两个概念，一个叫做 ParDo，就是并行处理的意思；另一个叫做 GroupByKey，也就是按照 Key 进行分组。&lt;/p&gt;&#xA;&lt;h3 id=&#34;pardo&#34;&gt;ParDo&lt;/h3&gt;&#xA;&lt;p&gt;ParDo 用来进行通用的并行化处理。每个输入元素（这个元素本身有可能是一个有限的集合）都会使用一个 UDF 进行处理（在Dataflow中叫做DoFn），输出是0或多个输出元素。这个例子是把键的前缀进行展开，然后把值复制到展开后的键构成新的键值对并输出。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://qiref.github.io/assets/img/dataflow-pardo.svg&#34; alt=&#34;dataflow-pardo&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;groupbykey&#34;&gt;GroupByKey&lt;/h3&gt;&#xA;&lt;p&gt;GroupByKey 用来按 Key 把元素重新分组。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://qiref.github.io/assets/img/dataflow-group-by-key.svg&#34; alt=&#34;dataflow-group-by-key&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;ParDo 操作因为是对每个输入的元素进行处理，因此很自然地就可以适用于无边界的数据。而 GroupByKey 操作，在把数据发送到下游进行汇总前，需要收集到指定的键对应的所有数据。如果输入源是无边界的，那么我们不知道何时才能收集到所有的数据。所以通常的解决方案是对数据使用窗口操作。&lt;/p&gt;&#xA;&lt;h2 id=&#34;窗口&#34;&gt;窗口&lt;/h2&gt;&#xA;&lt;h3 id=&#34;时间语义&#34;&gt;时间语义&lt;/h3&gt;&#xA;&lt;p&gt;窗口通常基于时间，时间对于窗口来说是必不可少的，在流式计算中，有 processing-time 和 event-time 两种时间语义，具体参考： &lt;a href=&#34;https://archieyao.github.io/posts/2022-02-25-flink-%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89/&#34;&gt;时间语义&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;窗口分类&#34;&gt;窗口分类&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;固定窗口（Fixed Window）固定区间（互不重叠）的窗口，可以基于时间，也可以基于数量；将事件分配到不同区间的窗口中，在通过窗口边界后，窗口内的所有事件会发送给计算函数进行计算；&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;滑动窗口（Sliding Window）固定区间但可以重叠的窗口，需要指定窗口区间以及滑动步长，区间重叠意味着同一个事件会分配到不同窗口参与计算。 窗口区间决定何时触发计算，滑动步长决定何时创建一个新的窗口；&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;会话窗口（Session Window）会话窗口通常基于用户的会话，通过定义会话的超时时间，将事件分割到不同的会话中； 例如，有个客服聊天系统，如果用户超过 30 分钟没有互动，则认为一次会话结束，当客户下次进入，就是一个新的会话了。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;窗口分配与合并&#34;&gt;窗口分配与合并&lt;/h3&gt;&#xA;&lt;p&gt;Dataflow 模型里，需要的不只是 GroupByKey，实际在统计数据的时候，往往需要的是 GroupByKeyAndWindow。统计一个不考虑任何时间窗口的数据，往往是没有意义的；&#xA;Dataflow 模型提出：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;从模型简化的角度上，把所有的窗口策略都当做非对齐窗口，而底层实现来负责把对齐窗口作为一个特例进行优化。&lt;/li&gt;&#xA;&lt;li&gt;窗口操作可以被分隔为两个互相相关的操作：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;set&amp;lt;Window&amp;gt; AssignWindows(T datum)&lt;/code&gt; 即窗口分配操作。这个操作把元素分配到 0 或多个窗口中去。&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;set&amp;lt;window&amp;gt; MergeWindows(Set&amp;lt;Window&amp;gt; windows)&lt;/code&gt; 即窗口合并操作，这个操作在汇总时合并窗口。&#xA;而在实际的逻辑实现层面，Dataflow 最重要的两个函数，也就是 AssignWindows 函数和 MergeWindows 函数。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;窗口分配&#34;&gt;窗口分配&lt;/h4&gt;&#xA;&lt;p&gt;每一个原始的事件，在业务处理函数之前，其实都是（key, value, event_time）这样一个三元组。而 AssignWindows 要做的，就是把这个三元组，根据我们的处理逻辑，变成（key, value, event_time, window）这样的四元组。&lt;/p&gt;</description>
    </item>
    <item>
      <title>堆和堆排序</title>
      <link>https://qiref.github.io/post/2023/05/12/%E5%A0%86%E5%92%8C%E5%A0%86%E6%8E%92%E5%BA%8F/</link>
      <pubDate>Fri, 12 May 2023 17:01:04 +0800</pubDate>
      <guid>https://qiref.github.io/post/2023/05/12/%E5%A0%86%E5%92%8C%E5%A0%86%E6%8E%92%E5%BA%8F/</guid>
      <description>&lt;h2 id=&#34;堆&#34;&gt;堆&lt;/h2&gt;&#xA;&lt;p&gt;堆的本质是树，用数组表示的完全二叉树；&lt;/p&gt;&#xA;&lt;h3 id=&#34;定义&#34;&gt;定义&lt;/h3&gt;&#xA;&lt;p&gt;一棵深度为k且有 &lt;code&gt;2^k - 1&lt;/code&gt; 个结点的二叉树称为满二叉树。&lt;/p&gt;&#xA;&lt;p&gt;根据二叉树的性质2, 满二叉树每一层的结点个数都达到了最大值, 即满二叉树的第i层上有 &lt;code&gt;2^(i-1)&lt;/code&gt; 个结点 (i≥1) 。&lt;/p&gt;&#xA;&lt;p&gt;如果对满二叉树的结点进行编号, 约定编号从根结点起, 自上而下, 自左而右。则深度为k的, 有n个结点的二叉树, 当且仅当其每一个结点都与深度为k的满二叉树中编号从1至n的结点一一对应时, 称之为完全二叉树。&lt;/p&gt;&#xA;&lt;p&gt;从满二叉树和完全二叉树的定义可以看出, 满二叉树是完全二叉树的特殊形态, 即如果一棵二叉树是满二叉树, 则它必定是完全二叉树。&lt;/p&gt;&#xA;&lt;p&gt;参考： &lt;a href=&#34;https://baike.baidu.com/item/%E5%AE%8C%E5%85%A8%E4%BA%8C%E5%8F%89%E6%A0%91/7773232&#34;&gt;https://baike.baidu.com/item/%E5%AE%8C%E5%85%A8%E4%BA%8C%E5%8F%89%E6%A0%91/7773232&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;性质&#34;&gt;性质&lt;/h3&gt;&#xA;&lt;pre&gt;&lt;code&gt;arr：[2 3 4 52 2 2 1]&#xA;idx： 0 1 2 3  4 5 6&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;i 下标和元素之间的映射关系：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;左子节点：&lt;code&gt;2*i+1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;右子节点：&lt;code&gt;2*i+2&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;父节点：&lt;code&gt;(i-1)/2&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;大根堆&#34;&gt;大根堆&lt;/h3&gt;&#xA;&lt;p&gt;完全二叉树里，每一个子树的最大值是根节点；&lt;/p&gt;&#xA;&lt;h3 id=&#34;小根堆&#34;&gt;小根堆&lt;/h3&gt;&#xA;&lt;p&gt;完全二叉树里，每一个子树的最小值是根节点；&lt;/p&gt;&#xA;&lt;h2 id=&#34;堆排序&#34;&gt;堆排序&lt;/h2&gt;&#xA;&lt;h3 id=&#34;定义堆&#34;&gt;定义堆&lt;/h3&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// maxHeap 定义一个大根堆&#xA;type maxHeap struct {&#xA;    Data  []int&#xA;    Count int&#xA;}&#xA;&#xA;func NewMaxHeap(size int) *maxHeap {&#xA;    return &amp;amp;maxHeap{&#xA;        Data:  make([]int, size),&#xA;        Count: 0,&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h3 id=&#34;插入数据&#34;&gt;插入数据&lt;/h3&gt;&#xA;&lt;p&gt;插入数据时，是往数组最后增加元素，由于需要保证大根堆的性质，如果新加入的元素比父节点大，则跟父节点交换位置，以此类推，一直到根节点，这个交换流程完成后，新元素插入就完成了。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chandy-Lamport 算法笔记</title>
      <link>https://qiref.github.io/post/2023/05/08/chandy-lamport-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Mon, 08 May 2023 22:38:42 +0800</pubDate>
      <guid>https://qiref.github.io/post/2023/05/08/chandy-lamport-%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/</guid>
      <description>&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;&#xA;&lt;p&gt;Global Snapshot（Global State）：全局快照，分布式系统在 Failure Recovery 的时候非常有用，也是广泛应用在分布式系统，更多是分布式计算系统中的一种容错处理理论基础。&lt;/p&gt;&#xA;&lt;p&gt;在 Chandy-Lamport 算法中，为了定义分布式系统的 Global Snapshot，先将分布式系统简化成有限个进程和进程之间的 channel 组成，也就是一个有向图 （GAG）：节点是进程，边是 channel。因为是分布式系统，也就是说，这些进程是运行在不同的物理机器上的。那么一个分布式系统的  Global Snapshot 就是有进程的状态和 channel 中的 message 组成，这个也是分布式快照算法需要记录的。因此，Chandy-Lamport 算法解决了分布式系统在 Failure Recovery 时，可以从  Global Snapshot 中恢复的问题；&lt;/p&gt;&#xA;&lt;h2 id=&#34;算法过程&#34;&gt;算法过程&lt;/h2&gt;&#xA;&lt;h3 id=&#34;前提条件及定义&#34;&gt;前提条件及定义&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;process（Pn）：分布式系统中的进程，用 P1，P2，P3 表示；&lt;/li&gt;&#xA;&lt;li&gt;channel：分布式系统中，Pn 与 Pm 通信的管道，C12 表示从 P1 到 P2 的 channel，反之，C32 表示从 P3 到 P2的 channel；&lt;/li&gt;&#xA;&lt;li&gt;message：分布式系统中，Pn 与 Pm 之间发送的业务消息；M23 表示从 P2 到 P3 的 message；&lt;/li&gt;&#xA;&lt;li&gt;marker：在 Chandy-Lamport 算法中，Pn 与 Pm 之间发送的标记消息，不同于业务的 message，marker 是由 Chandy-Lamport 算法定义，用于帮助实现快照算法；&lt;/li&gt;&#xA;&lt;li&gt;snapshot/state：都表示快照，同时包括进程本身的状态和 message；下文中统一全局快照叫 snapshot，process 本地快照叫 state；&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Chandy-Lamport 算法有一些前提条件：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Go语言实现 bitmap</title>
      <link>https://qiref.github.io/post/2023/04/28/go%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0-bitmap/</link>
      <pubDate>Fri, 28 Apr 2023 11:24:38 +0800</pubDate>
      <guid>https://qiref.github.io/post/2023/04/28/go%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0-bitmap/</guid>
      <description>&lt;h2 id=&#34;算法说明&#34;&gt;算法说明&lt;/h2&gt;&#xA;&lt;p&gt;Bitmap算法是一种基于位运算的数据结构，用于解决大规模数据的快速查找和统计问题。其基本原理是将一个大数据集合映射到一个二进制向量中，其中每个元素对应于数据集合中的一个元素，向量中的每一位表示该元素是否存在于集合中。&lt;/p&gt;&#xA;&lt;p&gt;具体来说，Bitmap算法通过使用一个位图（bitmap）来表示一个数据集合，其中每个元素对应一个位。如果某个元素在数据集合中出现，则将其对应的位设置为1，否则将其对应的位设置为0。通过这种方式，可以快速地进行集合操作，如并集、交集和差集等。&lt;/p&gt;&#xA;&lt;p&gt;Bitmap算法的主要优点在于其空间效率高，可以用较小的空间存储大规模数据集合。另外，Bitmap算法的时间复杂度也非常低，可以快速地进行集合操作。&lt;/p&gt;&#xA;&lt;h2 id=&#34;如何用数组表示一个-bitmap&#34;&gt;如何用数组表示一个 bitmap&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://qiref.github.io/assets/img/bitmap-index-cal.svg&#34; alt=&#34;bitmap-index-cal&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;以 1byte 为例：8位能表示8个元素， 0-7 号对应了 b[0] 下标， 8-15 号对应了 b[1] 下标，以此类推。&lt;/p&gt;&#xA;&lt;p&gt;因此，数组下标 n 跟bitmap元素序号 bitmapIdx 的关系为：&lt;code&gt;n = bitmapIdx &amp;gt;&amp;gt; 3&lt;/code&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;值如何映射到-bitmap-数组&#34;&gt;值如何映射到 bitmap 数组&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://qiref.github.io/assets/img/bitmap-index-map.svg&#34; alt=&#34;bitmap-index-cal&#34;&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;当找到了 元素序号 n 在数组中的下标之后，如何给 b[n] 赋值呢？&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;1 &amp;lt;&amp;lt; (bitmapIdx &amp;amp; 7)&lt;/code&gt; 等同于 &lt;code&gt;1 &amp;lt;&amp;lt; (bitmapIdx % 8)&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;(bitmapIdx % 8)&lt;/code&gt; 找到在了在数组 b[n] 中的第 &lt;code&gt;m&lt;/code&gt; 位，然后 &lt;code&gt;1 &amp;lt;&amp;lt; m&lt;/code&gt; 之后，就相当于给数组赋值，把第 &lt;code&gt;m&lt;/code&gt; 位 置为1。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;验证&#34;&gt;验证&lt;/h3&gt;&#xA;&lt;p&gt;同样以 1byte 为例：借用上述结论，第 24 号元素，对应的数组下标 n 为：&lt;code&gt;n = 24 &amp;gt;&amp;gt; 3 &lt;/code&gt; 结果为3, b[3]；&lt;/p&gt;</description>
    </item>
    <item>
      <title>Go语言实现 LRU</title>
      <link>https://qiref.github.io/post/2023/04/27/go%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0-lru/</link>
      <pubDate>Thu, 27 Apr 2023 21:43:12 +0800</pubDate>
      <guid>https://qiref.github.io/post/2023/04/27/go%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0-lru/</guid>
      <description>&lt;p&gt;LRU（Least Recently Used）算法，即最近最少使用算法;其基本思想是，如果一个数据最近被访问过，那么它在未来被访问的概率也会很高；反之，如果一个数据很久都没有被访问过，那么它在未来被访问的概率就相对较低。因此，LRU算法选择淘汰最近最少使用的数据，即选择最长时间没有被访问过的数据进行淘汰。&lt;/p&gt;&#xA;&lt;p&gt;具体来说，LRU算法通常使用一个双向链表和一个哈希表来实现。双向链表中的节点按照最近访问时间的顺序排列，最近访问的节点排在链表头部，最久未访问的节点排在链表尾部。哈希表中存储每个节点的地址，以便快速查找和删除。&lt;/p&gt;&#xA;&lt;p&gt;当需要访问一个数据时，LRU算法首先在哈希表中查找该数据，如果存在，则将对应的节点移动到链表头部；如果不存在，则将该数据添加到链表头部，并在哈希表中创建对应的节点。&lt;/p&gt;&#xA;&lt;p&gt;当需要淘汰数据时，LRU算法选择链表尾部的节点进行淘汰，并在哈希表中删除对应的节点。&lt;/p&gt;&#xA;&lt;p&gt;golang 实现 LRU 算法：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package lru&#xA;&#xA;import (&#xA;    &amp;quot;container/list&amp;quot;&#xA;    &amp;quot;errors&amp;quot;&#xA;    &amp;quot;sync&amp;quot;&#xA;)&#xA;&#xA;// LRU implements a non-thread safe fixed size LRU cache&#xA;type LRU struct {&#xA;    size      int&#xA;    evictList *list.List&#xA;    items     map[interface{}]*list.Element&#xA;}&#xA;&#xA;// entry is used to hold a value in the evictList&#xA;type entry struct {&#xA;    key   interface{}&#xA;    value interface{}&#xA;}&#xA;&#xA;// NewLRU constructs an LRU of the given size&#xA;func NewLRU(size int) (*LRU, error) {&#xA;    if size &amp;lt;= 0 {&#xA;        return nil, errors.New(&amp;quot;must provide a positive size&amp;quot;)&#xA;    }&#xA;    c := &amp;amp;LRU{&#xA;        size:      size,&#xA;        evictList: list.New(),&#xA;        items:     make(map[interface{}]*list.Element),&#xA;    }&#xA;    return c, nil&#xA;}&#xA;&#xA;// Add adds a value to the cache.  Returns true if an eviction occured.&#xA;func (c *LRU) Add(key, value interface{}) bool {&#xA;    // Check for existing item&#xA;    if ent, ok := c.items[key]; ok {&#xA;        c.evictList.MoveToFront(ent)&#xA;        ent.Value.(*entry).value = value&#xA;        return false&#xA;    }&#xA;&#xA;    // Add new item&#xA;    ent := &amp;amp;entry{key, value}&#xA;    entry := c.evictList.PushFront(ent)&#xA;    c.items[key] = entry&#xA;&#xA;    evict := c.evictList.Len() &amp;gt; c.size&#xA;    // Verify size not exceeded&#xA;    if evict {&#xA;        c.removeOldest()&#xA;    }&#xA;    return evict&#xA;}&#xA;&#xA;// Get looks up a key&#39;s value from the cache.&#xA;func (c *LRU) Get(key interface{}) (value interface{}, ok bool) {&#xA;    if ent, ok := c.items[key]; ok {&#xA;        c.evictList.MoveToFront(ent)&#xA;        return ent.Value.(*entry).value, true&#xA;    }&#xA;    return&#xA;}&#xA;&#xA;// Remove removes the provided key from the cache, returning if the&#xA;// key was contained.&#xA;func (c *LRU) Remove(key interface{}) bool {&#xA;    if ent, ok := c.items[key]; ok {&#xA;        c.removeElement(ent)&#xA;        return true&#xA;    }&#xA;    return false&#xA;}&#xA;&#xA;// removeOldest removes the oldest item from the cache.&#xA;func (c *LRU) removeOldest() {&#xA;    ent := c.evictList.Back()&#xA;    if ent != nil {&#xA;        c.removeElement(ent)&#xA;    }&#xA;}&#xA;&#xA;// removeElement is used to remove a given list element from the cache&#xA;func (c *LRU) removeElement(e *list.Element) {&#xA;    c.evictList.Remove(e)&#xA;    kv := e.Value.(*entry)&#xA;    delete(c.items, kv.key)&#xA;    if c.onEvict != nil {&#xA;        c.onEvict(kv.key, kv.value)&#xA;    }&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;需要注意的是这个 LRU 实现并不是线程安全的，如果需要线程安全，需要在外层方法加锁，同时，由于 golang 的 lock 并不是可重入的，需要注意避免死锁问题。 以下实现中基于 LRU 做了一次封装，实现了线程安全的内存级 LRU-Cache：&lt;/p&gt;</description>
    </item>
    <item>
      <title>ProtoBuf数据协议</title>
      <link>https://qiref.github.io/post/2022/01/11/protobuf%E6%95%B0%E6%8D%AE%E5%8D%8F%E8%AE%AE/</link>
      <pubDate>Tue, 11 Jan 2022 00:18:23 +0000</pubDate>
      <guid>https://qiref.github.io/post/2022/01/11/protobuf%E6%95%B0%E6%8D%AE%E5%8D%8F%E8%AE%AE/</guid>
      <description>&lt;p&gt;摘要：ProtoBuf(Protocol Buffers)是一种跨平台、语言无关、可扩展的序列化结构数据的方法，可用于网络数据交换及数据存储。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;protocol-buffers介绍&#34;&gt;Protocol Buffers介绍&lt;/h2&gt;&#xA;&lt;p&gt;不同于 &lt;code&gt;XML&lt;/code&gt; 、&lt;code&gt;JSON&lt;/code&gt; 这种文本格式数据，Protocol Buffers 是一种二进制格式数据。在Protocol Buffers 诞生之初，就被赋予两个特点：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;向前兼容，很容易引入新字段，应对字段的频繁变更&lt;/li&gt;&#xA;&lt;li&gt;数据格式具备描述性，并且支持多语言处理&lt;/li&gt;&#xA;&lt;li&gt;传输效率高&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;基于以上这些特性，Protocol Buffers 被广泛应用于各种 RPC 框架中，并且是 Google 的数据通用语言。&lt;/p&gt;&#xA;&lt;h3 id=&#34;protocol-buffers协议文件&#34;&gt;Protocol Buffers协议文件&lt;/h3&gt;&#xA;&lt;p&gt;Protocol Buffers 在使用前需要先定义好协议文件，以 &lt;code&gt;.proto&lt;/code&gt; 为后缀的文件就是Protocol Buffers 的协议文件。&lt;/p&gt;&#xA;&lt;p&gt;Example:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-protobuf&#34;&gt;// 指定protobuf的版本，proto3是最新的语法版本&#xA;syntax = &amp;quot;proto3&amp;quot;;&#xA;&#xA;// 定义数据结构，message 你可以想象成java的class，c语言中的struct&#xA;message Response {&#xA;  string data = 1;   // 定义一个string类型的字段，字段名字为data, 序号为1&#xA;  int32 status = 2;   // 定义一个int32类型的字段，字段名字为status, 序号为2&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;如果 A 和 B 要基于 Protocol Buffers 协议进行通信，那么在通信前，A 和 B 都需要有同一份协议文件，所以在 Protocol Buffers 数据传输过程中，不需要数据的 &lt;code&gt;Schema&lt;/code&gt; 信息；&lt;/p&gt;</description>
    </item>
    <item>
      <title>逆波兰表达式算法</title>
      <link>https://qiref.github.io/post/2019/09/04/%E9%80%86%E6%B3%A2%E5%85%B0%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%AE%97%E6%B3%95/</link>
      <pubDate>Wed, 04 Sep 2019 00:18:23 +0000</pubDate>
      <guid>https://qiref.github.io/post/2019/09/04/%E9%80%86%E6%B3%A2%E5%85%B0%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%AE%97%E6%B3%95/</guid>
      <description>&lt;p&gt;摘要：将中缀表达式转化为后缀表达式，以及计算后缀表达式的算法。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;&#xA;import org.slf4j.Logger;&#xA;import org.slf4j.LoggerFactory;&#xA;&#xA;import java.util.HashSet;&#xA;import java.util.Scanner;&#xA;import java.util.Stack;&#xA;&#xA;/**&#xA; * @author YaoQi&#xA; * Date: 2019/1/5 15:45&#xA; * Modified:&#xA; * Description: 中缀表达式转后缀表达式&#xA; */&#xA;public class InfixToSuffixHandler {&#xA;&#xA;    private static final Logger logger = LoggerFactory.getLogger(InfixToSuffixHandler.class);&#xA;&#xA;    private static HashSet&amp;lt;Character&amp;gt; opStr = new HashSet&amp;lt;&amp;gt;();&#xA;&#xA;    static {&#xA;        logger.info(&amp;quot;Initialization operator&amp;quot;);&#xA;        opStr.add(&#39;+&#39;);&#xA;        opStr.add(&#39;-&#39;);&#xA;        opStr.add(&#39;*&#39;);&#xA;        opStr.add(&#39;/&#39;);&#xA;        logger.info(&amp;quot;Initialization finished&amp;quot;);&#xA;    }&#xA;&#xA;    /**&#xA;     * 判断字符是否为操作符&#xA;     *&#xA;     * @param c 字符&#xA;     * @return&#xA;     */&#xA;    private static boolean isOpStr(char c) {&#xA;        return opStr.contains(c);&#xA;    }&#xA;&#xA;    /**&#xA;     * 判断字符是否为操作数&#xA;     *&#xA;     * @param c 字符&#xA;     * @return&#xA;     */&#xA;    private static boolean isOperand(char c) {&#xA;        return c &amp;gt;= &#39;0&#39; &amp;amp;&amp;amp; c &amp;lt;= &#39;9&#39;;&#xA;    }&#xA;&#xA;    /**&#xA;     * 得到当前操作符的优先级&#xA;     *&#xA;     * @param c 操作符&#xA;     * @return&#xA;     */&#xA;    private static int priority(char c) {&#xA;        switch (c) {&#xA;            case &#39;*&#39;:&#xA;            case &#39;/&#39;:&#xA;                return 3;&#xA;            case &#39;+&#39;:&#xA;            case &#39;-&#39;:&#xA;                return 2;&#xA;            case &#39;(&#39;:&#xA;                return 1;&#xA;            default: {&#xA;                logger.error(&amp;quot;c: {} is not operator marks&amp;quot;, c);&#xA;                return 0;&#xA;            }&#xA;        }&#xA;    }&#xA;&#xA;    /**&#xA;     * 用后缀表达式求值&#xA;     *&#xA;     * @param suffixExpr 后缀表达式&#xA;     * @return&#xA;     */&#xA;    public static int numberCalculate(String suffixExpr) {&#xA;        Stack&amp;lt;Integer&amp;gt; count = new Stack&amp;lt;&amp;gt;();&#xA;        char c;&#xA;        int number1, number2;&#xA;        for (int i = 0; i &amp;lt; suffixExpr.length(); i++) {&#xA;            if (isOperand(c = suffixExpr.charAt(i))) {&#xA;                count.push(c - &#39;0&#39;);&#xA;            } else {&#xA;                number2 = count.pop();&#xA;                number1 = count.pop();&#xA;                switch (c) {&#xA;                    case &#39;+&#39;:&#xA;                        count.push(number1 + number2);&#xA;                        break;&#xA;                    case &#39;-&#39;:&#xA;                        count.push(number1 - number2);&#xA;                        break;&#xA;                    case &#39;*&#39;:&#xA;                        count.push(number1 * number2);&#xA;                        break;&#xA;                    case &#39;/&#39;:&#xA;                        count.push(number1 / number2);&#xA;                        break;&#xA;                    default:&#xA;                        break;&#xA;                }&#xA;            }&#xA;        }&#xA;        return count.pop();&#xA;    }&#xA;&#xA;&#xA;    /**&#xA;     * 将中缀表达式转化为后缀表达式&#xA;     *&#xA;     * @param expression 中缀表达式&#xA;     * @return 返回后缀表达式&#xA;     */&#xA;    public static StringBuilder getSuffixExpression(String expression) {&#xA;        //保存已将建立的后缀表达式&#xA;        StringBuilder suffixExpr = new StringBuilder();&#xA;        //操作符栈&#xA;        Stack&amp;lt;Character&amp;gt; opStr = new Stack&amp;lt;&amp;gt;();&#xA;        //输入表达式某个位置的字符&#xA;        char c;&#xA;        //运算符栈中弹出的字符&#xA;        char pop;&#xA;        for (int i = 0; i &amp;lt; expression.length(); i++) {&#xA;            c = expression.charAt(i);&#xA;            //如果当前字符为操作数&#xA;            if (isOperand(c)) {&#xA;                suffixExpr.append(c);&#xA;            } else if (isOpStr(c)) {&#xA;                //如果当前字符为操作符&#xA;                if (opStr.isEmpty()) {&#xA;                    opStr.push(c);&#xA;                } else {&#xA;                    while (!opStr.isEmpty() &amp;amp;&amp;amp; priority(opStr.peek()) &amp;gt;= priority(c)) {&#xA;                        pop = opStr.pop();&#xA;                        suffixExpr.append(pop);&#xA;                    }&#xA;                    opStr.push(c);&#xA;                }&#xA;            } else if (&#39;(&#39; == c) {&#xA;                //如果当前字符为‘(’&#xA;                opStr.push(c);&#xA;            } else if (&#39;)&#39; == c) {&#xA;                //如果当前字符为‘)’&#xA;                while ((pop = opStr.pop()) != &#39;(&#39;) {&#xA;                    suffixExpr.append(pop);&#xA;                }&#xA;            } else {&#xA;                logger.error(&amp;quot;c: {} is not valid in {}&amp;quot;, c, expression);&#xA;            }&#xA;        }&#xA;        while (!opStr.isEmpty()) {&#xA;            suffixExpr.append(opStr.pop());&#xA;        }&#xA;        System.out.println(&amp;quot;转换后的后缀表达式为：&amp;quot; + suffixExpr);&#xA;        return suffixExpr;&#xA;    }&#xA;&#xA;    public static void main(String[] args) {&#xA;        Scanner scan = new Scanner(System.in, &amp;quot;UTF-8&amp;quot;);&#xA;        //输入的表达式&#xA;        String expression = scan.nextLine();&#xA;        // 生成后缀表达式&#xA;        StringBuilder suffixExpr = getSuffixExpression(expression);&#xA;        System.out.println(&amp;quot;后缀表达式计算的结果为：&amp;quot; + numberCalculate(suffixExpr.toString()));&#xA;&#xA;    }&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h3 id=&#34;tips&#34;&gt;tips&lt;/h3&gt;&#xA;&lt;p&gt;一般的代数表达式都是中缀表达式，也就是操作数在操作符两边，后缀表达式（逆波兰表达式）就是操作符在操作数后面。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
